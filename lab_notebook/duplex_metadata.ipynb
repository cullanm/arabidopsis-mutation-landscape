{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPRECATED\n",
    "\n",
    "use the script duplex_metadata.py now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for analyzing the distribution of PCR duplicates in duplex sequencing runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + '/../python_scripts') # this lets us import files in python_scripts (like gtools)\n",
    "import gtools\n",
    "if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "    os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "\n",
    "import pysam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "import multiprocessing\n",
    "import pickle\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = ''\n",
    "sample_names += 'Col-0-1 Col-0-2 Col-0-3 Col-0-4 Col-0-5 Col-0-6 Col-0-7 Col-0-8 '\n",
    "# sample_names += 'chandler '\n",
    "# sample_names += 'Col-0-swapped-1 Col-0-swapped-2 Col-0-swapped-3 Col-0-swapped-4 Col-0-swapped-5 Col-0-swapped-6 '\n",
    "# sample_names += 'Ler-0-germline '\n",
    "# sample_names += 'UVB-1 UVB-2 UVC-002 UVC-005 UVC-01 UVC-02 '\n",
    "# sample_names += 'UVCnotime-0h UVCnotime-1h UVCnotreat-C1 UVCnotreat-C2 '\n",
    "# sample_names += 'Col-0-bonus1 Col-0-bonus2 Col-0-bonus3 '\n",
    "# sample_names += 'arp-1 arp-2 '\n",
    "# sample_names += 'atx12r7-1 atx12r7-2 atx12r7-3 '\n",
    "# sample_names += 'chr8-1 chr8-2 chr8-3 '\n",
    "# sample_names += 'clf-1 clf-2 clf-3 '\n",
    "# sample_names += 'ddm1-1 ddm1-2 ddm1-3 '\n",
    "# sample_names += 'h2aw7-1 h2aw7-2 h2aw7-3 '\n",
    "# sample_names += 'h2ax35-1 h2ax35-2 h2ax35-3 '\n",
    "# sample_names += 'ku80-1 ku80-2 ku80-3 '\n",
    "# sample_names += 'met1-epiRIL12-1 met1-epiRIL12-2 met1-epiRIL12-3 '\n",
    "# sample_names += 'msh2-1 msh2-2 msh2-3 '\n",
    "sample_names += 'msh6-1 msh6-2 msh6-3 '\n",
    "# sample_names += 'nrpd1b-1 nrpd1b-2 nrpd1b-3 '\n",
    "# sample_names += 'parp2-1 parp2-2 parp2-3 '\n",
    "sample_names += 'poly-1 poly-2 poly-3 '\n",
    "# sample_names += 'rad5a-1 rad5a-2 rad5a-3 '\n",
    "# sample_names += 'rad7a-1 rad7a-2 rad7a-3 '\n",
    "# sample_names += 'sdg8-1 sdg8-2 sdg8-3 '\n",
    "# sample_names += 'suvh456-2 suvh456-3 '\n",
    "# sample_names += 'xpg-1 xpg-2 xpg-3'\n",
    "sample_names = sample_names.split()\n",
    "# files = [f'data/align/big_{s}_merged.bam' for s in sample_names]\n",
    "                \n",
    "# sample_names = 'Col-0-1 Col-0-2 Col-0-3 Col-0-4 Col-0-5 Col-0-6 Col-0-7 Col-0-8 \\\n",
    "#                 Col-0-1_merged_01 Col-0-1_merged_02 Col-0-1_merged_04 Col-0-1_merged_06 Col-0-1_merged_08 \\\n",
    "#                 Col-0-2_merged_01 Col-0-2_merged_02 Col-0-2_merged_04 Col-0-2_merged_06 Col-0-2_merged_08 \\\n",
    "#                 Col-0-3_merged_01 Col-0-3_merged_02 Col-0-3_merged_04 Col-0-3_merged_06 Col-0-3_merged_08 \\\n",
    "#                 Col-0-4_merged_01 Col-0-4_merged_02 Col-0-4_merged_04 Col-0-4_merged_06 Col-0-4_merged_08 \\\n",
    "#                 Col-0-5_merged_01 Col-0-5_merged_02 Col-0-5_merged_04 Col-0-5_merged_06 Col-0-5_merged_08 \\\n",
    "#                 Col-0-6_merged_01 Col-0-6_merged_02 Col-0-6_merged_04 Col-0-6_merged_06 Col-0-6_merged_08 \\\n",
    "#                 Col-0-7_merged_01 Col-0-7_merged_02 Col-0-7_merged_04 Col-0-7_merged_06 Col-0-7_merged_08 \\\n",
    "#                 Col-0-8_merged_01 Col-0-8_merged_02 Col-0-8_merged_04 Col-0-8_merged_06 Col-0-8_merged_08 \\\n",
    "#                 Col-0-1_01frags Col-0-1_02frags Col-0-1_04frags Col-0-1_06frags Col-0-1_08frags \\\n",
    "#                 Col-0-2_01frags Col-0-2_02frags Col-0-2_04frags Col-0-2_06frags Col-0-2_08frags \\\n",
    "#                 Col-0-3_01frags Col-0-3_02frags Col-0-3_04frags Col-0-3_06frags Col-0-3_08frags \\\n",
    "#                 Col-0-4_01frags Col-0-4_02frags Col-0-4_04frags Col-0-4_06frags Col-0-4_08frags \\\n",
    "#                 Col-0-5_01frags Col-0-5_02frags Col-0-5_04frags Col-0-5_06frags Col-0-5_08frags \\\n",
    "#                 Col-0-6_01frags Col-0-6_02frags Col-0-6_04frags Col-0-6_06frags Col-0-6_08frags \\\n",
    "#                 Col-0-7_01frags Col-0-7_02frags Col-0-7_04frags Col-0-7_06frags Col-0-7_08frags \\\n",
    "#                 Col-0-8_01frags Col-0-8_02frags Col-0-8_04frags Col-0-8_06frags Col-0-8_08frags'.split()\n",
    "files = []\n",
    "for s in sample_names:\n",
    "    if 'merged' in s:\n",
    "        files.append(f'data/align/read_subsample/big_{s}.bam')\n",
    "    elif 'frags' in s:\n",
    "        files.append(f'data/align/fragment_subsample/big_{s}.bam')\n",
    "    elif 'chandler' in s:\n",
    "        files.append(f'data/align/chandler_KVKCS001D_0_filtered.bam')\n",
    "    else:\n",
    "        files.append(f'data/align/big_{s}_merged.bam')\n",
    "\n",
    "genome_file = 'data/ref/arabidopsis_ref/ref.fa'\n",
    "size_to_process = 1000000 # will only process reads on first chromosome up to this position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get coverage data of each fragment in the bam files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a pickled dictionary with key=source replicate (RG) value=[(chrom, start, tlen, umi, + strand pairs, - strand pairs)]\n",
    "def get_frags(file, out, sub=2):\n",
    "    fragments = defaultdict(lambda: [0, 0])\n",
    "    aln = pysam.AlignmentFile(file, 'rb')\n",
    "    total_reads = sum([x.total for x in aln.get_index_statistics()])\n",
    "    rep_reads = defaultdict(lambda: 0)\n",
    "    for read in tqdm(aln.fetch(contig='Chr1'), position=0, miniters=1000000, maxinterval=999):\n",
    "        frag_start = read.reference_start\n",
    "        tlen = abs(read.template_length) # this is non-inclusive\n",
    "        if read.has_tag('RX'):\n",
    "            umi = read.get_tag('RX')\n",
    "            rep = umi.split('_')[-1]\n",
    "        else:\n",
    "            umi = read.get_tag('RG')\n",
    "            rep = umi\n",
    "        rep_reads[rep] += 1\n",
    "        \n",
    "        # thow out read pairs that don't align as expected, ignore read 2 so we don't double count pairs\n",
    "        if read.is_unmapped or read.mate_is_unmapped or not (read.flag & 2) or read.is_reverse or read.mapping_quality < 10:\n",
    "            continue\n",
    "        \n",
    "        if random.random() > sub:\n",
    "            continue\n",
    "        \n",
    "        frag_strand = '+' if read.is_read1 else '-'\n",
    "        \n",
    "        if frag_strand == '+':\n",
    "            fragments[(read.reference_name, frag_start, tlen, umi, rep)][0] += 1\n",
    "        else:\n",
    "            fragments[(read.reference_name, frag_start, tlen, umi, rep)][1] += 1\n",
    "                \n",
    "        if read.reference_start > size_to_process:\n",
    "            break\n",
    "    \n",
    "    fragments = dict(fragments)\n",
    "    frags = {rep:[] for rep in rep_reads}\n",
    "    for frag in fragments:\n",
    "        frags[frag[4]].append((frag[0], frag[1], frag[2], frag[3], fragments[frag][0], fragments[frag][1]))\n",
    "    \n",
    "    for rep in frags:\n",
    "        frags[rep].sort() # the sort matters for the scrambled null section\n",
    "    with open(out, 'wb') as fout:\n",
    "        pickle.dump(frags, fout)\n",
    "    \n",
    "    # print(f'{file}: {total_reads} reads')\n",
    "    # for rep in rep_reads:\n",
    "    #     print(f'{int((rep_reads[rep] / sum(rep_reads.values())) * total_reads)} reads in replicate {rep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=8)\n",
    "    processes = []\n",
    "    for i in range(len(files)):\n",
    "        processes.append(pool.apply_async(get_frags, (files[i], f'tmp/metadata_{sample_names[i]}_frags.pkl')))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frags = dict() # key: sample; value: [(chrom, frag_start, tlen, umi, + reads, - reads)]\n",
    "for sample in tqdm(sample_names, total=len(files), position=0):\n",
    "    with open(f'tmp/metadata_{sample}_frags.pkl', 'rb') as f:\n",
    "        l = pickle.load(f)\n",
    "        frags[sample] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample_frags = frags[sample_names[0]][list(frags[sample_names[0]])[0]]\n",
    "strand_1 = np.array([x[4] for x in first_sample_frags])\n",
    "strand_2 = np.array([x[5] for x in first_sample_frags])\n",
    "\n",
    "avgs = []\n",
    "for i in range(10):\n",
    "    a = strand_1 * (strand_2 == i)\n",
    "    avgs.append(sum(a) / np.count_nonzero(a))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), avgs)\n",
    "ax.set_xlabel('bottom strand coverage')\n",
    "ax.set_ylabel('average top strand coverage')\n",
    "ax.set_title('Duplex coverage correlates between strands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate fragment conflict rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frags['Col-0-1']['big_Col-0_1a_filtered'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_rates = []\n",
    "for i, sample in tqdm(enumerate(sample_names), total=len(sample_names)):\n",
    "    \n",
    "    # for each replicate, randomly sample half as many fragments from other replicates to compute a \"half\" conflict rate \n",
    "    # (the true conflict rate is calculated as 1 - (1-half conflict rate)^2)\n",
    "    # the reason I calculate a \"half\" rate is that some samples only have 2 replicates, and if one has more fragments than the other, its \n",
    "    # \"full\" conflict rate can't be calculated, so I use the half as an extremely close approximation\n",
    "    rates = []\n",
    "    rep_frag_counts = []\n",
    "    if len(frags[sample]) == 1 or 'swapped' in sample:\n",
    "        conflict_rates.append(0)\n",
    "        print(f'{sample} conflict rate of {conflict_rates[-1]}')\n",
    "        continue\n",
    "    \n",
    "    for rep in frags[sample]:\n",
    "        rep_frags = frags[sample][rep] # fragments from this rep\n",
    "        other_frags = list(itertools.chain.from_iterable([frags[sample][r] for r in frags[sample] if r != rep])) # fragments from other reps\n",
    "        \n",
    "        # randomly select half as many fragments from other_frags as are in rep_frags\n",
    "        ratio = len(rep_frags) / len(other_frags)\n",
    "        if ratio > 2:\n",
    "            continue\n",
    "        other_subsample = {(x[0], x[1], x[2]):(x[4], x[5]) for x in other_frags if random.random() < 0.5 * ratio}\n",
    "        \n",
    "        # count the number of conflicts which make a fragment uncallable\n",
    "        uncallable_conflicts = 0\n",
    "        for frag in rep_frags:\n",
    "            if (frag[0], frag[1], frag[2]) in other_subsample: # if there's a conflict\n",
    "                # if the fragment doesn't contribute at least 76% of the top and bottom strand pairs, the conflict will be uncallable\n",
    "                if frag[4] / max(1, (frag[4] + other_subsample[(frag[0], frag[1], frag[2])][0])) < 0.76 or \\\n",
    "                   frag[5] / max(1, (frag[5] + other_subsample[(frag[0], frag[1], frag[2])][1])) < 0.76:\n",
    "                    uncallable_conflicts += 1\n",
    "        conflict_rate = 1 - (1 - (uncallable_conflicts / len(rep_frags))) ** 2\n",
    "        rates.append(conflict_rate)\n",
    "        \n",
    "        rep_frag_counts.append(sum([x[4] > 0 and x[5] > 0 for x in rep_frags])) # estimated number of callable fragments\n",
    "    \n",
    "    # calculate the sample conflict rate as the average across replicates, weighted by the number of fragments in the replicate\n",
    "    conflict_rates.append(sum([rates[i] * rep_frag_counts[i] / sum(rep_frag_counts) for i in range(len(rates))]))\n",
    "    print(f'{sample} conflict rate of {conflict_rates[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a best fit correlation between fragment number and conflict rate (only use samples where conflict rate could be estimated)\n",
    "counts = []\n",
    "rates = []\n",
    "for i, sample in enumerate(sample_names):\n",
    "    if conflict_rates[i] == 0:\n",
    "        continue\n",
    "    counts.append(sum([len(frags[sample][r]) for r in frags[sample]]))\n",
    "    rates.append(conflict_rates[i])\n",
    "counts += [0] * 10000 # my tiny brain can't figure out how to fix the intercept at 0, so I'm just adding a bunch of points at (0, 0)\n",
    "rates += [0] * 10000\n",
    "slope, intercept = np.polyfit(counts, rates, 1)\n",
    "line_x = np.linspace(0, 5000000, 1000)\n",
    "line_y = [x * slope + intercept for x in line_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_rates = []\n",
    "for i, sample in enumerate(sample_names):\n",
    "    if 'swapped' in sample:\n",
    "        imputed_rates.append(0)\n",
    "    elif conflict_rates[i] != 0:\n",
    "        imputed_rates.append(conflict_rates[i])\n",
    "    else:\n",
    "        imputed_rates.append(sum([len(frags[sample][r]) for r in frags[sample]]) * slope + intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter([sum([len(frags[s][r]) for r in frags[s]]) for s in sample_names], imputed_rates, color=['tab:blue' if x == 0 else 'tab:orange' for x in conflict_rates])\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "ax.plot(line_x, line_y)\n",
    "ax.set_xlim(xlim), ax.set_ylim(ylim)\n",
    "ax.set_xlabel('Fragment number (in first Mb)')\n",
    "ax.set_ylabel('Conflict rate')\n",
    "ax.set_title('Conflict rate and fragment number')\n",
    "s_blue = plt.scatter([], [], c='tab:blue')\n",
    "s_orange = plt.scatter([], [], c='tab:orange')\n",
    "ax.legend([s_orange, s_blue], ['calculated', 'imputed'])\n",
    "fig.savefig('figs/conflict_rate_and_fragment_number.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('conflict rates where rates for samples with no replicates are imputed:')\n",
    "print([round(x, 5) for x in imputed_rates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([round(x, 5) for x in imputed_rates[:8]])\n",
    "print([round(x, 5) for x in imputed_rates[8:8 + (len(imputed_rates) - 8) // 2]])\n",
    "print([round(x, 5) for x in imputed_rates[8 + (len(imputed_rates) - 8) // 2:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate genome coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = 300\n",
    "metadata = []\n",
    "for i, sample in enumerate(sample_names):\n",
    "    total_reads = sum([x[4] + x[5] for x in frags[sample]])\n",
    "    total_frags = len(frags[sample])\n",
    "    callable_frags = len([frag for frag in frags[sample] if (frag[4] > 0 and frag[5] > 1) or (frag[4] > 1 and frag[5] > 0)])\n",
    "    seq_eff = callable_frags / total_reads\n",
    "    \n",
    "    # metrics related to coverage, need to double check\n",
    "    cov = total_reads * cycles / size_to_process\n",
    "    opt_cov = cov * 0.107\n",
    "    unique_cov = total_frags * cycles / size_to_process\n",
    "    callable_cov = callable_frags * cycles / size_to_process\n",
    "    \n",
    "    metadata.append([file.split('/')[-1], total_frags, callable_frags, cov, opt_cov, unique_cov, callable_cov, seq_eff])\n",
    "pd.DataFrame(metadata, columns='file total_frags callable_frags total_coverage optimal_coverage unique_coverage callable_coverage sequencing_efficiency'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsample to determine the optimal sequencing efficiency at perfect dilution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_levels = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=8)\n",
    "    processes = []\n",
    "    for i, sample in enumerate(sample_names):\n",
    "        for sub_level in sub_levels:\n",
    "            processes.append(pool.apply_async(get_frags, (files[i], f'tmp/{sample}_sub_{sub_level}.pkl', sub_level)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_frags = defaultdict(lambda: dict())\n",
    "for i, sample in tqdm(enumerate(sample_names)):\n",
    "    for sub_level in sub_levels:\n",
    "        with open(f'tmp/{sample}_sub_{sub_level}.pkl', 'rb') as f:\n",
    "            frags = pickle.load(f)\n",
    "            for replicate in sorted(list(frags.keys())):\n",
    "                sub_frags[sample + '_' + replicate][sub_level] = frags[replicate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_eff_files = [defaultdict(lambda: ('', 0, 0, 0)) for x in range(3)] # indexed as [# reads needed to call - 1][index of original file]\n",
    "\n",
    "subset_frags = {x:sub_frags[x] for x in list(sub_frags)}\n",
    "\n",
    "# ############## Duplex-seq\n",
    "cmap = plt.cm.tab20\n",
    "fig, axs = plt.subplots(3, figsize=(len(subset_frags), 6), sharex=True, sharey=True)\n",
    "for i, req_strand, req_total in zip(range(3), [1, 1, 2], [2, 3, 4]):\n",
    "    seq_eff_overlap = []\n",
    "    seq_eff_all = []\n",
    "    for sample in subset_frags:\n",
    "        for sub_level in sub_levels:\n",
    "            total_reads = sum([x[4] + x[5] for x in subset_frags[sample][sub_level]])\n",
    "            callable_frags_overlap = len([x for x in subset_frags[sample][sub_level] if x[4] >= req_strand and x[5] >= req_strand and x[4] + x[5] >= req_total])\n",
    "            callable_frags_all = len([x for x in subset_frags[sample][sub_level] if x[4] >= 2 * req_strand and x[5] >= 2 * req_strand and x[4] + x[5] >= 2 * req_total])\n",
    "            seq_eff_overlap.append(callable_frags_overlap / total_reads)\n",
    "            seq_eff_all.append(callable_frags_all / total_reads)\n",
    "    axs[i].bar(range(len(seq_eff_overlap)), seq_eff_overlap, color=[cmap((x // len(sub_levels)) % 2) for x in range(len(seq_eff_overlap))])\n",
    "    axs[i].bar(range(len(seq_eff_all)), seq_eff_all, color=[cmap((x // len(sub_levels)) % 2 + 2) for x in range(len(seq_eff_all))])\n",
    "    axs[i].set_title(f'Need >={req_strand * 2} reads per strand and >={req_total * 2} total to call', y=0.7)\n",
    "axs[1].set_ylabel('Callable fragmens per aligned read pair')\n",
    "axs[-1].set_xticks([len(sub_levels) // 2 + x * len(sub_levels) for x in range(len(subset_frags))])\n",
    "axs[-1].set_xticklabels(subset_frags.keys(), rotation=45, ha='right')\n",
    "axs[0].legend(['only read1/2 overlap is callable', 'all of fragment is callable'], bbox_to_anchor=(1, 1), loc='upper left')\n",
    "fig.suptitle('Sequencing efficiency for subsampled libraries', y=0.95)\n",
    "fig.savefig('figs/seq_efficiency_for_subsampled_libraries.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ################# depreciated CODEC-seq\n",
    "# fig, axs = plt.subplots(3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "# for i, req_strand, req_total in zip(range(3), [0, 0, 0], [1, 2, 3]):\n",
    "#     seq_eff = []\n",
    "#     for j, file in enumerate(sub_files):\n",
    "#         total_reads = sum([x[4] + x[5] for x in sub_frags[file]])\n",
    "#         callable_frags = len([x for x in sub_frags[file] if x[4] >= req_strand and x[5] >= req_strand and x[4] + x[5] >= req_total])\n",
    "# #         print(file, total_reads, callable_frags)\n",
    "#         seq_eff.append(callable_frags / total_reads)\n",
    "#         if callable_frags / total_reads > peak_eff_files[i][j // len(sub_levels)][1]:\n",
    "#             peak_eff_files[i][j // len(sub_levels)] = (file, callable_frags / total_reads, callable_frags, total_reads)\n",
    "#     axs[i].bar(range(len(sub_files)), seq_eff, color=[cmap((x // len(sub_levels)) % 2) for x in range(len(sub_files))])\n",
    "#     axs[i].set_title(f'Need >={req_strand} reads per strand and >={req_total} total to call', y=0.7)\n",
    "# axs[1].set_ylabel('Callable fragmens per aligned read pair')\n",
    "# axs[-1].set_xticks([len(sub_levels) // 2 + x * len(sub_levels) for x in range(len(files))])\n",
    "# axs[-1].set_xticklabels(sample_names)\n",
    "# fig.suptitle('Sequencing efficiency for subsampled libraries', y=0.95)\n",
    "# fig.savefig('figs/seq_efficiency_for_subsampled_libraries.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check fragment lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_lens = []\n",
    "for i, sample in enumerate(sample_names):\n",
    "    values = [x[2] for x in frags[sample][list(frags[sample])[0]]]\n",
    "    median_lens.append(stat.median(values))\n",
    "    print(int(median_lens[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(files), sharex=True)\n",
    "fig.set_size_inches(12, 20)\n",
    "for i, sample in tqdm(enumerate(sample_names)):\n",
    "    axs[i].hist([x[2] for x in frags[sample][list(frags[sample])[0]]], bins=range(50, 2000, 10))\n",
    "    axs[i].hist([x[2] for x in frags[sample][list(frags[sample])[0]] if x[4] > 0 and x[5] > 0], bins=range(50, 2000, 10))\n",
    "    axs[i].set_title(f'{sample_names[i]} (median {int(median_lens[i])})', y=0.5, ha='left')\n",
    "    # break\n",
    "axs[-1].set_xlabel('Fragment size (bp)')\n",
    "axs[len(files) // 2].set_ylabel('# fragments')\n",
    "fig.suptitle('Duplex seq fragment lengths', y=0.91)\n",
    "fig.savefig('figs/duplex_fragment_length.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize duplicate coverage per fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total coverage of each fragment to estimate the number of viable fragments in the library\n",
    "fig, axs = plt.subplots(len(sample_names), sharex=True)\n",
    "fig.set_size_inches(6, 8)\n",
    "for i, sample in tqdm(enumerate(sample_names)):\n",
    "    axs[i].hist([x[4] + x[5] for x in frags[sample]], bins=range(30))\n",
    "    axs[i].hist([x[4] + x[5] for x in frags[sample] if x[4] > 0 and x[5] > 0], bins=range(30))\n",
    "    axs[i].set_title(sample, y=0.5)\n",
    "axs[-1].set_xlabel('# read pairs covering fragment')\n",
    "axs[-1].set_xlabel('# read pairs covering fragment')\n",
    "axs[len(files) // 2].set_ylabel('# fragments')\n",
    "axs[0].legend(['Reads in 1 strand', 'Reads in both strands'])\n",
    "fig.suptitle('Duplex seq fragment coverage', y=0.91)\n",
    "fig.savefig('figs/duplex_fragment_total_coverage.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if there's evidence that one strand of DNA is being lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fraction of two stranded support\n",
    "sample_reps = []\n",
    "for sample in sample_names:\n",
    "    sample_reps += [(sample, rep) for rep in frags[sample]]\n",
    "sample_reps = sample_reps[0:25:3]\n",
    "\n",
    "fig, axs = plt.subplots(len(sample_reps), sharex=True)\n",
    "fig.set_size_inches(6, 8)\n",
    "for i, (sample, rep) in tqdm(enumerate(sample_reps)):\n",
    "    fractions = []\n",
    "    x_values = range(1, 20)\n",
    "    for j in x_values:\n",
    "        j_frags = [x for x in frags[sample][rep] if x[4] + x[5] == j]\n",
    "        try:\n",
    "            fractions.append(len([x for x in j_frags if x[4] != 0 and x[5] != 0]) / len(j_frags))\n",
    "        except ZeroDivisionError:\n",
    "            fractions.append(1)\n",
    "    axs[i].plot(x_values, fractions)\n",
    "    axs[i].plot(x_values, [(2**x - 2)/(2**x) for x in x_values])\n",
    "    axs[i].set_title(sample + ' ' + rep, y=0.5)\n",
    "axs[-1].legend(['Observed', 'Random sampling expectation'], loc='upper center', bbox_to_anchor=(0, -1, 0.5, 0))\n",
    "axs[-1].set_xlabel('# read pairs covering fragment')\n",
    "axs[len(axs) // 2].set_ylabel('Fraction of fragments with 2-stranded coverage')\n",
    "fig.suptitle('Duplex seq fragment 2-stranded coverage', y=0.91)\n",
    "fig.savefig('figs/duplex_fragment_2-stranded_coverage.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check if the \"scrambled\" null is feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the fragments of Col-0-1 and count how many reads are in Col-0-{2-8} with the same start and tlen\n",
    "idxs = [0] * 8\n",
    "self_reads = [] # number of reads in Col-0-1 for each fragment\n",
    "other_reads = [] # number of reads in Col-0-{2-8} for each fragment\n",
    "num_sources = [] # number other samples with a matching fragment\n",
    "last_span = ('', 0, 0)\n",
    "for i, frag in tqdm(enumerate(frags[sample_names[0]])): # for each fragment in Col-0-1\n",
    "    span = (frag[0], frag[1], frag[2]) # chrom, start, tlen (don't consider umi)\n",
    "    if span == last_span: # don't double count if there's more than one fragment in Col-0-1 with the same start and tlen\n",
    "        continue\n",
    "    last_span = span\n",
    "    \n",
    "    others = 0\n",
    "    sources = 0 \n",
    "    for j in range(1, 8): # for Col-0-{2-8}\n",
    "        # iterate through fragments until past the current Col-0-1 fragment\n",
    "        while idxs[j] < len(frags[sample_names[j]]): # don't go past the list end\n",
    "            comp_frag = frags[sample_names[j]][idxs[j]]\n",
    "            comp_span = (comp_frag[0], comp_frag[1], comp_frag[2])\n",
    "            if comp_span > span:\n",
    "                break\n",
    "            if comp_span == span: # if the fragment matches\n",
    "                others += comp_frag[4] + comp_frag[5]\n",
    "                sources += 1\n",
    "            idxs[j] += 1\n",
    "\n",
    "    self_reads.append(frag[4] + frag[5])\n",
    "    other_reads.append(others)\n",
    "    num_sources.append(sources)\n",
    "    \n",
    "read_sources = np.zeros((3, len(self_reads))) # indexed as read_sources[0 (self)/1 (other)][fragment index] = number of reads\n",
    "read_sources[0] = self_reads\n",
    "read_sources[1] = other_reads\n",
    "read_sources[2] = num_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Col-0-1 fragments with at least one matching fragment in another sample\n",
    "np.count_nonzero(read_sources[1]), len(read_sources[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sources = 10\n",
    "bins = [x / 10 for x in range(12)]\n",
    "bars = np.zeros((max_sources, len(bins)))\n",
    "frac_in_self = read_sources[0] / (read_sources[0] + read_sources[1])\n",
    "for i, b in enumerate(bins[:-1]):\n",
    "    in_bin = np.logical_and(frac_in_self >= b, frac_in_self < bins[i + 1])\n",
    "    column = np.zeros(max_sources)\n",
    "    for s in range(max_sources):\n",
    "        column[s] = np.count_nonzero(np.logical_and(in_bin, read_sources[2] <= s))\n",
    "    bars[:, i] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(max_sources - 1, -1, -1):\n",
    "    ax.bar([x + (bins[1] - bins[0]) / 2 for x in bins], bars[i], width=bins[1] - bins[0])\n",
    "ax.legend(range(max_sources - 1, -1, -1), title='Found in X other samples', ncol=2, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.set_xlabel('reads in Col-0-1 / reads in all Col-0 samples')\n",
    "ax.set_ylabel('# fragments in Col-0-1')\n",
    "ax.set_title('Fragments with copies in other samples')\n",
    "fig.savefig('figs/fragments_with_copies_in_other_samples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot factors of PCR bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(50, 300, 10))\n",
    "bin_start = 40\n",
    "dups_by_size = defaultdict(lambda: [])\n",
    "sterrs_by_size = defaultdict(lambda: [])\n",
    "counts_by_size = defaultdict(lambda: [])\n",
    "for sample in sample_names:\n",
    "    last_bin = bin_start\n",
    "    for b in tqdm(bins, position=0):\n",
    "        dup_counts = [x[4] + x[5] for x in frags[sample] if x[2] < b and x[2] >= last_bin]\n",
    "        dups_by_size[sample].append(sum(dup_counts) / len(dup_counts) if len(dup_counts) > 0 else 0)\n",
    "        if len(dup_counts) > 1:\n",
    "            sterrs_by_size[sample].append(stat.stdev(dup_counts) / (len(dup_counts) ** (1/2)))\n",
    "        else:\n",
    "            sterrs_by_size[sample].append(0)\n",
    "        counts_by_size[sample].append(len(dup_counts))\n",
    "        last_bin = b\n",
    "    max_count = max(counts_by_size[sample])\n",
    "    counts_by_size[sample] = [x / max_count for x in counts_by_size[sample]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(files), sharex=True, figsize=(12, 8))\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "for i, sample in enumerate(sample_names):\n",
    "    axs[i].bar(range(len(bins)), dups_by_size[sample], yerr=sterrs_by_size[sample], color=[cmap(x) for x in counts_by_size[sample]])\n",
    "    axs[i].set_title(sample_names[i], y=0.7, x=0.1)\n",
    "axs[-1].set_xticks(range(0, len(bins), 5))\n",
    "axs[-1].set_xticklabels(bins[0::5])\n",
    "axs[-1].set_xlabel('Fragment size')\n",
    "axs[len(files) // 2].set_ylabel('Average PCR duplicates')\n",
    "fig.suptitle('PCR bias by fragment size', y=0.92)\n",
    "fig.savefig('figs/PCR_bias_by_fragment_size.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = dict()\n",
    "with open(genome_file, 'r') as f:\n",
    "    lines = f.read().split('>')[1:]\n",
    "    for l in lines:\n",
    "        genome[l.split(None, 1)[0]] = l.split('\\n', 1)[1].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [x / 100 for x in range(5, 105, 5)]\n",
    "bin_start = 0\n",
    "dups_by_gc = defaultdict(lambda: [])\n",
    "callable_by_gc = defaultdict(lambda: [])\n",
    "sterrs_by_gc = defaultdict(lambda: [])\n",
    "counts_by_gc = defaultdict(lambda: [])\n",
    "for sample in sample_names:\n",
    "    last_bin = bin_start\n",
    "    for b in tqdm(bins, position=0):\n",
    "        dup_counts = []\n",
    "        callable_counts = [0, 0]\n",
    "        for x in frags[sample]:\n",
    "            sequence = genome[x[0]][x[1]:x[1] + x[2]]\n",
    "            gc = (sequence.count('C') + sequence.count('G')) / len(sequence)\n",
    "            if gc < b and gc >= last_bin:\n",
    "                dup_counts.append(x[4] + x[5])\n",
    "                if x[4] > 0 and x[5] > 0:\n",
    "                    callable_counts[0] += 1\n",
    "                callable_counts[1] += 1\n",
    "        \n",
    "        if len(dup_counts) > 1:\n",
    "            dups_by_gc[sample].append(sum(dup_counts) / len(dup_counts))\n",
    "            callable_by_gc[sample].append(callable_counts[0] / callable_counts[1])\n",
    "            sterrs_by_gc[sample].append(stat.stdev(dup_counts) / (len(dup_counts) ** (1/2)))\n",
    "        else:\n",
    "            dups_by_gc[sample].append(0)\n",
    "            callable_by_gc[sample].append(0)\n",
    "            sterrs_by_gc[sample].append(0)\n",
    "        counts_by_gc[sample].append(len(dup_counts))\n",
    "        last_bin = b\n",
    "    \n",
    "    max_count = max(counts_by_gc[sample])\n",
    "    counts_by_gc[sample] = [x / max_count for x in counts_by_gc[sample]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(files), sharex=True, figsize=(12, 8))\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "for i, sample in enumerate(sample_names):\n",
    "    axs[i].bar(range(len(bins)), dups_by_gc[sample], yerr=sterrs_by_gc[sample], color=[cmap(x) for x in counts_by_gc[sample]])\n",
    "    axs[i].set_title(sample_names[i], y=0.7, x=0.9)\n",
    "axs[-1].set_xticks(range(0, len(bins), 2))\n",
    "bin_size = bins[1] - bins[0]\n",
    "axs[-1].set_xticklabels([round(b - bin_size / 2, 3) for b in bins[::2]])\n",
    "axs[-1].set_xlabel('GC content')\n",
    "axs[len(files) // 2].set_ylabel('Average PCR duplicates')\n",
    "fig.suptitle('PCR bias by GC content', y=0.92)\n",
    "fig.savefig('figs/PCR_bias_by_GC_content.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = sum([genome[chrom].count('C') + genome[chrom].count('G') for chrom in genome])\n",
    "print(f'genomic gc content: {gc / sum([len(genome[chrom]) for chrom in genome])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
