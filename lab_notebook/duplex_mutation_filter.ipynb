{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPRECATED\n",
    "\n",
    "use the scripts add_duplex_filter_columns.py and filter_duplex_variants.py now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook filters the output of duplex_caller to only true mutations. It also contains the code for determining the minimum number of supporting reads to be confident in mutation calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + '/../python_scripts') # this lets us import files in python_scripts (like gtools)\n",
    "import gtools\n",
    "if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "    os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import pandas as pd\n",
    "import csv\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "import pysam\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_names = 'Col-0-1 Col-0-2 Col-0-3 Col-0-4 Col-0-5 Col-0-6 Col-0-7 Col-0-8 ' \\\n",
    "               # 'Col-0-swapped-1 Col-0-swapped-2 Col-0-swapped-3 Col-0-swapped-4 Col-0-swapped-5 Col-0-swapped-6 ' \\\n",
    "               #  'Ler-0-germline ' \\\n",
    "               #  'UVB-1 UVB-2 UVC-002 UVC-005 UVC-01 UVC-02 ' \\\n",
    "                # 'UVCnotime-0h UVCnotime-1h UVCnotreat-C1 UVCnotreat-C2'\n",
    "                # Col-0-bonus1 Col-0-bonus2 Col-0-bonus3\\\n",
    "                # arp-1 arp-2 \\\n",
    "                # atx12r7-1 atx12r7-2 atx12r7-3 \\\n",
    "                # chr8-1 chr8-2 chr8-3 \\\n",
    "                # clf-1 clf-2 clf-3 \\\n",
    "                # ddm1-1 ddm1-2 ddm1-3 \\\n",
    "                # h2aw7-1 h2aw7-2 h2aw7-3 \\\n",
    "                # h2ax35-1 h2ax35-2 h2ax35-3 \\\n",
    "                # ku80-1 ku80-2 ku80-3 \\\n",
    "                # met1-epiRIL12-1 met1-epiRIL12-2 met1-epiRIL12-3 \\\n",
    "                # msh2-1 msh2-2 msh2-3 \\\n",
    "                # msh6-1 msh6-2 msh6-3 \\\n",
    "                # nrpd1b-1 nrpd1b-2 nrpd1b-3 \\\n",
    "                # parp2-1 parp2-2 parp2-3 \\\n",
    "                # poly-1 poly-2 poly-3 \\\n",
    "                # rad5a-1 rad5a-2 rad5a-3 \\\n",
    "                # rad7a-1 rad7a-2 rad7a-3 \\\n",
    "                # sdg8-1 sdg8-2 sdg8-3 \\\n",
    "                # suvh456-2 suvh456-3 \\\n",
    "                # xpg-1 xpg-2 xpg-3'\n",
    "# sample_names = sample_names.split()\n",
    "\n",
    "# sample_names = 'Col-0-1 Col-0-2 Col-0-3 Col-0-4 Col-0-5 Col-0-6 Col-0-7 Col-0-8 \\\n",
    "#                 Col-0-1_merged_01 Col-0-1_merged_02 Col-0-1_merged_04 Col-0-1_merged_06 Col-0-1_merged_08 \\\n",
    "#                 Col-0-2_merged_01 Col-0-2_merged_02 Col-0-2_merged_04 Col-0-2_merged_06 Col-0-2_merged_08 \\\n",
    "#                 Col-0-3_merged_01 Col-0-3_merged_02 Col-0-3_merged_04 Col-0-3_merged_06 Col-0-3_merged_08 \\\n",
    "#                 Col-0-4_merged_01 Col-0-4_merged_02 Col-0-4_merged_04 Col-0-4_merged_06 Col-0-4_merged_08 \\\n",
    "#                 Col-0-5_merged_01 Col-0-5_merged_02 Col-0-5_merged_04 Col-0-5_merged_06 Col-0-5_merged_08 \\\n",
    "#                 Col-0-6_merged_01 Col-0-6_merged_02 Col-0-6_merged_04 Col-0-6_merged_06 Col-0-6_merged_08 \\\n",
    "#                 Col-0-7_merged_01 Col-0-7_merged_02 Col-0-7_merged_04 Col-0-7_merged_06 Col-0-7_merged_08 \\\n",
    "#                 Col-0-8_merged_01 Col-0-8_merged_02 Col-0-8_merged_04 Col-0-8_merged_06 Col-0-8_merged_08 \\\n",
    "#                 Col-0-1_01frags Col-0-1_02frags Col-0-1_04frags Col-0-1_06frags Col-0-1_08frags \\\n",
    "#                 Col-0-2_01frags Col-0-2_02frags Col-0-2_04frags Col-0-2_06frags Col-0-2_08frags \\\n",
    "#                 Col-0-3_01frags Col-0-3_02frags Col-0-3_04frags Col-0-3_06frags Col-0-3_08frags \\\n",
    "#                 Col-0-4_01frags Col-0-4_02frags Col-0-4_04frags Col-0-4_06frags Col-0-4_08frags \\\n",
    "#                 Col-0-5_01frags Col-0-5_02frags Col-0-5_04frags Col-0-5_06frags Col-0-5_08frags \\\n",
    "#                 Col-0-6_01frags Col-0-6_02frags Col-0-6_04frags Col-0-6_06frags Col-0-6_08frags \\\n",
    "#                 Col-0-7_01frags Col-0-7_02frags Col-0-7_04frags Col-0-7_06frags Col-0-7_08frags \\\n",
    "#                 Col-0-8_01frags Col-0-8_02frags Col-0-8_04frags Col-0-8_06frags Col-0-8_08frags'.split()\n",
    "# duplex_files = [f'data/variant/big_{s}_duplex_strandsup.tsv' for s in sample_names] # output of duplex_caller prefiltered to require coverage in both strands and 24% support in both strands\n",
    "# informed_freq_files = [f'data/variant/big_{s}_informed_freq.tsv' for s in sample_names] # output of dup_informed_caller --min_support 2. Used to find the frequency of each mutations in the sample \n",
    "# informed_cont_files = [f'data/variant/big_{s}_informed_control.tsv' for s in sample_names[:8]] # output of dup_informed_caller --min_support 2 --duplicate_support 2. Used to filter out cryptic dups and common PCR errors\n",
    "\n",
    "sample_names = ['chandler']\n",
    "duplex_files = ['data/variant/chandler_KVKCS001D_0_filtered_duplex_strandsup.tsv']\n",
    "informed_freq_files = ['data/variant/chandler_KVKCS001D_0_filtered_informed_freq.tsv']\n",
    "informed_cont_files = [f'data/variant/big_{s}_informed_control.tsv' for s in 'Col-0-1 Col-0-2 Col-0-3 Col-0-4 Col-0-5 Col-0-6 Col-0-7 Col-0-8'.split()]\n",
    "\n",
    "# coverage_files = [f'../data/coverage/total/big_{name}.bg' for name in sample_names[:8]]\n",
    "ref_index = 'data/ref/ref.fa.fai'\n",
    "blacklist_prefix = 'data/blacklist/arabidopsis/duplex_blacklist_'\n",
    "\n",
    "output_files = [f'data/variant/big_{s}_mutations.tsv' for s in sample_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(informed_freq_files[0], quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pos == 8582750]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom_sizes = gtools.load_chrom_sizes(ref_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all are indexed as bl[chrom][pos] = value\n",
    "bl_cov_per = dict() # coverage percentile\n",
    "bl_dup_dist = dict() # mismatches required to have a duplicate 120mer in the genome\n",
    "bl_poly_at = dict() # length of poly-A or poly-T starting or ending at position\n",
    "bl_poly_rep = dict() # length of di or trinucleotide repeat starting or ending at position\n",
    "for chrom in tqdm(chrom_sizes):\n",
    "    bl_cov_per[chrom] = np.load(f'{blacklist_prefix}{chrom}_coverage_percentile.npy')\n",
    "    bl_dup_dist[chrom] = np.load(f'{blacklist_prefix}{chrom}_duplicate_distance.npy')\n",
    "    bl_poly_at[chrom] = np.load(f'{blacklist_prefix}{chrom}_poly_at_length.npy')\n",
    "    bl_poly_rep[chrom] = np.load(f'{blacklist_prefix}{chrom}_poly_repeat_length.npy')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero((bl_cov_per['ChrC'] > 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Add extra info needed for mutation calling to duplex_caller output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_var(line):\n",
    "    var = line[:-1].split('\\t')\n",
    "    var[1] = int(var[1]) # change position to an int\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds information used for filtering mutations\n",
    "def add_mut_info(call_file, freq_file, control_files, output_file):\n",
    "    # open all files\n",
    "    f_call = open(call_file, 'r')\n",
    "    f_freq = open(freq_file, 'r')\n",
    "    f_conts = [open(f, 'r') for f in control_files]\n",
    "    f_out = open(output_file, 'w')\n",
    "    \n",
    "    # extract the header from all files\n",
    "    try:\n",
    "        dcs = {x:i for i, x in enumerate(next(f_call)[:-1].split('\\t'))} # duplex caller columns, key=column, value=column number\n",
    "        ics = {x:i for i, x in enumerate(next(f_freq)[:-1].split('\\t'))} # informed caller columns\n",
    "        [next(f) for f in f_conts]\n",
    "    except StopIteration:\n",
    "        print(f'empty file in {call_file} or {freq_file}')\n",
    "        return\n",
    "    \n",
    "    # write the output header with extra columns\n",
    "    f_out.write('\\t'.join(list(dcs.keys()) + 'high_sup avg_mq cov_per dup_dist poly_at poly_rep sample_sup sample_cov control_sup'.split()) + '\\n')\n",
    "    \n",
    "    # load the first freq and control variants\n",
    "    cur_freq = line_to_var(next(f_freq))\n",
    "    cur_conts = [line_to_var(next(f)) for f in f_conts]\n",
    "    \n",
    "    # print(ics)\n",
    "    # print(cur_freq)\n",
    "    \n",
    "    # for each variant in the call file, add extra information and output\n",
    "    bar = tqdm(miniters=10000, maxinterval=100)\n",
    "    while True:\n",
    "        # get the next variant in the call file\n",
    "        try:\n",
    "            cur_var = line_to_var(next(f_call))\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        # calculate the number of supporting reads with a high BQ value (>=30)\n",
    "        if cur_var[dcs['ref']] == '*' or cur_var[dcs['alt']] == '*':\n",
    "            high_sup = int(cur_var[dcs['f_sup']]) + int(cur_var[dcs['r_sup']])\n",
    "        else:\n",
    "            high_sup = sum([(ord(bq) - 33) >= 30 for bq in cur_var[dcs['bq']]])\n",
    "        \n",
    "        # calculate the average MQ of supporting reads\n",
    "        avg_mq = sum([ord(mq) - 33 for mq in cur_var[dcs['mq']]]) / len(cur_var[dcs['mq']])\n",
    "        \n",
    "        # find the blacklist values\n",
    "        cov_per = bl_cov_per[cur_var[dcs['chrom']]][int(cur_var[dcs['pos']])]\n",
    "        dup_dist = bl_dup_dist[cur_var[dcs['chrom']]][int(cur_var[dcs['pos']])]\n",
    "        poly_at = bl_poly_at[cur_var[dcs['chrom']]][int(cur_var[dcs['pos']])]\n",
    "        poly_rep = bl_poly_rep[cur_var[dcs['chrom']]][int(cur_var[dcs['pos']])]\n",
    "        \n",
    "        # read in variants from the frequency and control files until at or past the current variant\n",
    "        while cur_freq[:4] < cur_var[:4]:\n",
    "            try:\n",
    "                cur_freq = line_to_var(next(f_freq))\n",
    "            except StopIteration:\n",
    "                cur_freq = [chr(1000)]\n",
    "            \n",
    "        for i in range(len(f_conts)):\n",
    "            while cur_conts[i][:4] < cur_var[:4]:\n",
    "                try:\n",
    "                    cur_conts[i] = line_to_var(next(f_conts[i]))\n",
    "                except StopIteration:\n",
    "                    cur_conts[i] = [chr(1000)]\n",
    "        \n",
    "        # add the support and coverage of the variant within the sample\n",
    "        if cur_var[0] == 'Chr4' and cur_var[1] == 8582750:\n",
    "            print(f'at var {cur_var}, cur_freq is {cur_freq}')\n",
    "        if cur_freq[:4] == cur_var[:4]:\n",
    "            sample_sup = int(cur_freq[ics['f_sup']]) + int(cur_freq[ics['r_sup']])\n",
    "            sample_cov = int(cur_freq[ics['f_cov']]) + int(cur_freq[ics['r_cov']])\n",
    "        else:\n",
    "            sample_sup = 0\n",
    "            sample_cov = 0\n",
    "        \n",
    "        # add the support of the variant across all control samples\n",
    "        control_sup = 0\n",
    "        for i in range(len(f_conts)):\n",
    "            if cur_conts[i][:4] == cur_var[:4]:\n",
    "                control_sup += int(cur_conts[i][ics['f_sup']]) + int(cur_conts[i][ics['r_sup']])\n",
    "        \n",
    "        cur_var += [high_sup, avg_mq, cov_per, dup_dist, poly_at, poly_rep, sample_sup, sample_cov, control_sup]\n",
    "        \n",
    "        if True:\n",
    "            f_out.write('\\t'.join(map(str, cur_var)) + '\\n')\n",
    "        \n",
    "        bar.update()\n",
    "            \n",
    "    f_call.close()\n",
    "    f_freq.close()\n",
    "    for f in f_conts:\n",
    "        f.close()\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # start worker processes from pool\n",
    "    pool = multiprocessing.Pool(processes=8)\n",
    "    processes = []\n",
    "    for i, sample in enumerate(sample_names):\n",
    "        f_call = duplex_files[i]\n",
    "        f_freq = informed_freq_files[i]\n",
    "        f_conts = [f for f in informed_cont_files if sample[:7] not in f] # don't use a sample as its own control\n",
    "        \n",
    "        if len([x for x in informed_cont_files if x not in f_conts]) > 0:\n",
    "            print(f'not using control(s) {[x for x in informed_cont_files if x not in f_conts]} for sample {sample}')\n",
    "        # print(f_call, f_freq, f_conts)\n",
    "        processes.append(pool.apply_async(add_mut_info, (f_call, f_freq, f_conts, f'tmp/added_info_{sample_names[i]}.tsv')))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Call mutations using each modified output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returs a boolean list of whether each row of the df passes the contaminant filter\n",
    "# Variants fail this filter if their fragment has 2 variants >max_dist bp apart\n",
    "# max_dist=0 never allows more than one variant in a fragment, max_dist=1 allows a pair of back-to-back variants\n",
    "# input df must be sorted by chrom, frag_start, frag_len, frag_umi, pos\n",
    "def not_contaminant(df, max_dist):\n",
    "    df.frag_umi = df.frag_umi.fillna('') # need to set NAs to something else, as NA != NA\n",
    "    rows = df.itertuples()\n",
    "    prev_frag = ('')\n",
    "    keep = []\n",
    "    poss = []\n",
    "    while True:\n",
    "        r = next(rows, None)\n",
    "        if r is None or (r.chrom, r.frag_start, r.frag_len, r.frag_umi) != prev_frag:\n",
    "            keep += [poss[-1] - poss[0] <= max_dist] * len(poss) if len(poss) > 0 else []\n",
    "            if r is None:\n",
    "                return keep\n",
    "            prev_frag = (r.chrom, r.frag_start, r.frag_len, r.frag_umi)\n",
    "            poss = []\n",
    "        poss.append(r.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a df with only variants which pass all these filter cutoffs\n",
    "def filter_vars(df, min_strand_frac=0.76, min_strand_cov=2, min_total_cov=6, min_high_sup=4, min_avg_mq=10, max_emm=0.21, \\\n",
    "                min_cov_per=0.007, min_dup_dist=-1, max_poly_at=7, max_poly_rep=4, max_cont_dist=4, min_indel_end_dist=6, \\\n",
    "                max_control_sup=6, require_overlap=False, max_frag_len=2000): # FIXME return max_frag_len to 300\n",
    "    \n",
    "    # do most filters, but not require_overlap nor the full strand frac (just require 0.24 for now)\n",
    "    df = df[(df.f_read1_sup / df.f_read1_cov >= 0.24) & (df.r_read1_sup / df.r_read1_cov >= 0.24) & \\\n",
    "            (df.f_read1_cov >= min_strand_cov) & (df.r_read1_cov >= min_strand_cov) & \\\n",
    "            (df.f_read1_cov + df.r_read1_cov >= min_total_cov) & \\\n",
    "            (df.high_sup >= min_high_sup) & \\\n",
    "            (df.avg_mq >= min_avg_mq) & \\\n",
    "            (df.end_mismatch_rate <= max_emm) & \\\n",
    "            (df.cov_per >= min_cov_per) & \\\n",
    "            # (df.cov_per <= 1 - min_cov_per) & \\\n",
    "            (df.dup_dist >= min_dup_dist) & \\\n",
    "            (df.poly_at <= max_poly_at) & (df.poly_rep <= max_poly_rep) & \\\n",
    "            (df.control_sup <= max_control_sup) & \\\n",
    "            (df.frag_len <= max_frag_len)]\n",
    "    \n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    keep = []\n",
    "    for r in df.itertuples():\n",
    "        if r.ref != '*' and r.alt != '*':\n",
    "            k = True\n",
    "        elif r.alt == '*' and r.pos > r.frag_start + r.frag_len // 2: # if a deletion nearer the end of the fragment\n",
    "            k = r.frag_start + r.frag_len - r.pos - len(r.alt) >= min_indel_end_dist # count distance from the last base of the deletion\n",
    "        else:\n",
    "            k = r.pos - r.frag_start >= min_indel_end_dist and r.frag_start + r.frag_len - r.pos - 1 >= min_indel_end_dist\n",
    "        keep.append(k)\n",
    "    df = df[keep]\n",
    "    \n",
    "    df = df.sort_values('chrom frag_start frag_len frag_umi pos ref alt'.split())\n",
    "    df = df[not_contaminant(df, max_cont_dist)]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    # now that contaminants are removed, require overlap and full read support (contaminants are less likely to be filtered if these filters are run first)\n",
    "    if require_overlap:\n",
    "        df = df[(df.f_sup > 0) & (df.r_sup > 0)]\n",
    "    df = df[(df.f_read1_sup / df.f_read1_cov >= min_strand_frac) & (df.r_read1_sup / df.r_read1_cov >= min_strand_frac)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import EmptyDataError \n",
    "\n",
    "# load each set of variants with the added info, then filter to only mutations\n",
    "dfs_mut = []\n",
    "for i in range(len(sample_names)):\n",
    "    try:\n",
    "        df = pd.read_table(f'tmp/added_info_{sample_names[i]}.tsv', quoting=csv.QUOTE_NONE)\n",
    "    except EmptyDataError:\n",
    "        print(f'no mutations for sample {sample_names[i]}')\n",
    "        dfs_mut.append(None)\n",
    "        continue\n",
    "    print(f'{sample_names[i]} loaded {len(df)} raw mutations')\n",
    "\n",
    "    if 'swapped' in sample_names[i]: # for the swapped samples, it wouldn't be fair to include the sample's own reads in the control support\n",
    "        df.control_sup -= 2\n",
    "    \n",
    "    if 'Ler-0-germline' in sample_names[i]:\n",
    "        df_fil = filter_vars(df, max_cont_dist=1000)\n",
    "    else:\n",
    "        df_fil = filter_vars(df)\n",
    "        \n",
    "    print(f'{len(df_fil)} mutations remain after filters')\n",
    "    \n",
    "    df_fil = df_fil.drop_duplicates('chrom pos ref alt'.split())\n",
    "    print(f'{len(df_fil)} unique mutations remain after filters')\n",
    "    \n",
    "    dfs_mut.append(df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label all the mutations which are present in a sibling plant\n",
    "for i in tqdm(range(len(sample_names))):\n",
    "    if dfs_mut[i] is None:\n",
    "        continue\n",
    "    sibling_muts = set()\n",
    "    for j in range(len(sample_names)):\n",
    "        if i == j or sample_names[i].rsplit('-', 1)[0] != sample_names[j].rsplit('-', 1)[0] or dfs_mut[j] is None:\n",
    "            continue\n",
    "        df = dfs_mut[j]\n",
    "        sibling_muts.update(zip(df.chrom, df.pos, df.ref, df.alt))\n",
    "    \n",
    "    dfs_mut[i]['in_sibling'] = dfs_mut[i].apply(lambda r: (r.chrom, r.pos, r.ref, r.alt) in sibling_muts, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = dfs_mut[0].copy()\n",
    "df_tmp = df_tmp[(df_tmp.chrom == 'Chr4') & (df_tmp.pos > 8500000) & (df_tmp.pos < 10000000)]\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove contaminants (mutations shared between non-sibling samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pair of samples, get all the \"contaminant\" mutations found in both\n",
    "df_contam = pd.DataFrame(columns=dfs_mut[0].columns)\n",
    "for i in range(len(sample_names)):\n",
    "    for j in range(len(sample_names)):\n",
    "        if sample_names[i].rsplit('-', 1)[0] == sample_names[j].rsplit('-', 1)[0]: # ignore siblings\n",
    "            continue\n",
    "        if len(dfs_mut[i]) > 10000 or len(dfs_mut[j]) > 10000: # ignore samples with large numbers of mutations (the Ler-0 germline mutations)\n",
    "            continue\n",
    "        if 'swapped' in sample_names[i] or 'swapped' in sample_names[j]: # ignore swapped samples\n",
    "            continue\n",
    "        \n",
    "        df = dfs_mut[i].merge(dfs_mut[j]['chrom pos alt ref'.split()], how='inner')\n",
    "        if len(df) > 0:\n",
    "            print(f'{sum(df.in_sibling)} germline and {sum(~df.in_sibling)} somatic mutations in {sample_names[i]} are in {sample_names[j]}')\n",
    "\n",
    "        df_contam = df_contam.append(df)\n",
    "df_contam = df_contam.sort_values('chrom pos ref alt'.split())\n",
    "contaminants = set(zip(df_contam.chrom, df_contam.pos, df_contam.ref, df_contam.alt))\n",
    "print(f'{len(contaminants)} unique and {len(df_contam)} total contaminant mutations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all contaminant mutations\n",
    "for i in range(len(dfs_mut)):\n",
    "    if 'swapped' in sample_names[i]: # don't discard \"contaminants\" for the swapped samples\n",
    "        continue\n",
    "    \n",
    "    dfs_mut[i] = dfs_mut[i][dfs_mut[i].apply(lambda r: (r.chrom, r.pos, r.ref, r.alt) not in contaminants, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final mutation counts (unique and in sibling):')\n",
    "for i in range(len(dfs_mut)):\n",
    "    print(f'{sample_names[i]}\\t{sum(~dfs_mut[i].in_sibling)}\\t{sum(dfs_mut[i].in_sibling)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final mutation set\n",
    "for i in range(len(dfs_mut)):\n",
    "    dfs_mut[i].to_csv(output_files[i], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfs_mut[0].frag_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_examples = dfs_mut[1][(dfs_mut[1].chrom == 'Chr5') & (np.abs(dfs_mut[1].pos - 24022141) < 300)]\n",
    "df_examples = df_examples.append(dfs_mut[1][(dfs_mut[1].chrom == 'Chr1') & (np.abs(dfs_mut[1].pos - 10497750) < 300)])\n",
    "df_examples = df_examples.append(dfs_mut[1][(dfs_mut[1].chrom == 'Chr5') & (np.abs(dfs_mut[1].pos - 17567566) < 300)])\n",
    "df_examples = df_examples.append(dfs_mut[1][(dfs_mut[1].chrom == 'Chr1') & (np.abs(dfs_mut[1].pos - 16015120) < 300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtools.sam_view_df(df_examples, ['data/align/big_Col-0-2_merged.bam', 'data/align/big_Col-0-3_merged.bam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing filter sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check BQ and MQ histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln = pysam.AlignmentFile('data/align/chandler_KVKCS001D_0_filtered.bam')\n",
    "bqs = np.zeros(50, dtype='i')\n",
    "mqs = np.zeros(50, dtype='i')\n",
    "i = 0\n",
    "mismatches = 0\n",
    "total = 0\n",
    "for read in tqdm(aln.fetch()):\n",
    "    i += 1\n",
    "    if i > 50000:\n",
    "        break\n",
    "    for bq in read.get_forward_qualities():\n",
    "        bqs[bq] += 1\n",
    "    mqs[read.mapping_quality] += 1\n",
    "    for x in read.get_aligned_pairs(with_seq=True):\n",
    "        if x[2] is None or x[2].islower():\n",
    "            mismatches += 1\n",
    "        total += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, sharex=True)\n",
    "axs[0].bar(range(len(bqs)), bqs)\n",
    "axs[1].bar(range(len(mqs)), mqs)\n",
    "axs[0].set_title('BQs')\n",
    "axs[1].set_title('MQs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Find optimal cutoff for each filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_samples = list(range(1, 5))\n",
    "# fake_samples = list(range(1, 4))\n",
    "\n",
    "real_samples = [0]\n",
    "fake_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Col-0-{1-4} as a training dataset. Also load the swapped samples for estimating FP rate\n",
    "df_real = None\n",
    "for i in tqdm(real_samples):\n",
    "    df = pd.read_table(f'tmp/added_info_{sample_names[i]}.tsv', quoting=csv.QUOTE_NONE)\n",
    "    print(f'Loaded {len(df)} raw mutations from {sample_names[i]}')\n",
    "    \n",
    "    df = df[df.control_sup < 100]\n",
    "    print(f'{len(df)} after pre-filter')\n",
    "    \n",
    "    if df_real is None:\n",
    "        df_real = df\n",
    "    else:\n",
    "        df_real = df_real.append(df)\n",
    "df_real = df_real.sort_values('chrom frag_start frag_len frag_umi pos ref alt'.split()) # first by fragment (filter_vars will do this, but doing it here keeps the order stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.DataFrame(columns=df_real.columns)\n",
    "for i in tqdm(fake_samples):\n",
    "    df = pd.read_table(f'tmp/added_info_{sample_names[i]}.tsv', quoting=csv.QUOTE_NONE)\n",
    "    print(f'Loaded {len(df)} raw mutations')\n",
    "    \n",
    "    df.control_sup -= 2 # for the swapped samples, it wouldn't be fair to include the sample's own reads in the control support\n",
    "    \n",
    "    df = df[df.control_sup < 100]\n",
    "    print(f'{len(df)} after pre-filter')\n",
    "    \n",
    "    df_fake = df_fake.append(df)\n",
    "df_fake = df_fake.sort_values('chrom frag_start frag_len frag_umi pos ref alt'.split()) # first by fragment (filter_vars will do this, but doing it here keeps the order stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cov, fake_cov = 1, 1 # this can be changed back to zero, not that it makes a difference\n",
    "for i in real_samples:\n",
    "    for chrom in chrom_sizes:\n",
    "        real_cov += np.sum(np.load(f'data/coverage/2f1r/big_{sample_names[i]}_{chrom}.npy'))\n",
    "for i in fake_samples:\n",
    "    for chrom in chrom_sizes:\n",
    "        fake_cov += np.sum(np.load(f'data/coverage/2f1r/big_{sample_names[i]}_{chrom}.npy'))\n",
    "real_cov, fake_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the fraction of variants which are C->T\n",
    "def ct_rate(df):\n",
    "    # df = df[(df.ref != '*') & (df.alt != '*')]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return np.nan\n",
    "    return sum(((df.ref == \"C\") & (df.alt == \"T\")) | ((df.ref == \"G\") & (df.alt == \"A\"))) / len(df)\n",
    "\n",
    "# returns the fraction of variants which are indels\n",
    "def indel_rate(df):\n",
    "    if len(df) == 0:\n",
    "        return np.nan\n",
    "    return sum((df.ref == \"*\") | (df.alt == \"*\")) / len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_ct = 0.49 # ct_rate(filter_vars(df_real, min_strand_cov=4, min_strand_frac=0.99, max_emm=0.3)) # 0.494 is the C>T rate of the weng et al unique mutations\n",
    "# min_ct = ct_rate(df_real)\n",
    "# min_indel = indel_rate(filter_vars(df_real, min_strand_cov=4, max_emm=0.3))\n",
    "# min_ct, max_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_rates(df_real_fil, df_fake_fil):\n",
    "    swapped_fp_rate_est = (len(df_fake_fil) / fake_cov) / (len(df_real_fil) / real_cov) if len(df_real_fil) > 0 else np.nan\n",
    "    swapped_tp_count_est = max(0, len(df_real_fil) * (1 - swapped_fp_rate_est))\n",
    "    \n",
    "    return swapped_fp_rate_est, swapped_tp_count_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the output of two filter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use one set of filters here\n",
    "df_real_fil_1 = filter_vars(df_real, min_high_sup=1)\n",
    "df_fake_fil_1 = filter_vars(df_fake, min_high_sup=1)\n",
    "len(df_real_fil_1), len(df_fake_fil_1), fp_rates(df_real_fil_1, df_fake_fil_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a different set here (the output will be more useful if this one calls more mutations)\n",
    "df_real_fil_2 = filter_vars(df_real, min_high_sup=0)\n",
    "df_fake_fil_2 = filter_vars(df_fake, min_high_sup=0)\n",
    "len(df_real_fil_2), len(df_fake_fil_2), fp_rates(df_real_fil_2, df_fake_fil_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the variants called with one filter set but not the other\n",
    "df_unique = df_real_fil_2.merge(df_real_fil_1['chrom pos alt ref'.split()], on='chrom pos alt ref'.split(), how='outer', indicator=True)\n",
    "df_unique = df_unique[df_unique._merge != 'both']\n",
    "\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = df_unique.sample(10)\n",
    "# gtools.sam_view_df(df_tmp, [f'data/align/big_Col-0-{i}_merged.bam' for i in range(1, 5)], max_reads=50000, window_size=1)\n",
    "# df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See what happens as filters are adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 - np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_to_desc = {\n",
    "    'min_strand_cov': 'min read coverage in each strand',\n",
    "    'min_total_cov': 'min total read coverage',\n",
    "    'min_high_sup': 'min support from high BQ (>30) reads',\n",
    "    'min_avg_mq': 'min average MQ of supporting reads',\n",
    "    'max_emm': 'max mismatch rate before an end is trimmed',\n",
    "    'min_indel_end_dist': 'min distance from an indel to fragment end',\n",
    "    'min_cov_per': 'min coverage percentile of genomic site',\n",
    "    'min_dup_dist': \"min hamming distance to another fragment in the genome\",\n",
    "    'max_poly_at': 'max length of poly-A/T repeat',\n",
    "    'max_poly_rep': 'max length of di/trinucleotide repeat',\n",
    "    'max_control_sup': 'max (non-sequencing error) occurences in control samples',\n",
    "    'max_frag_len': 'max fragment length',\n",
    "    'max_cont_dist': 'max distance between any 2 variants in a fragment',\n",
    "    'min_strand_frac': 'min fraction of reads in support from each strand',\n",
    "    'require_overlap': 'require (1) or don\\'t require (0) read1-read2 overlap',\n",
    "}\n",
    "\n",
    "cur_cutoffs = {\n",
    "    'min_strand_cov':2,\n",
    "    'min_total_cov':6,\n",
    "    'min_high_sup':4,\n",
    "    'min_avg_mq':10,\n",
    "    'max_emm':0.21,\n",
    "    'min_indel_end_dist':6,\n",
    "    'min_cov_per':0.007,\n",
    "    'min_dup_dist':0,\n",
    "    'max_poly_at':7,\n",
    "    'max_poly_rep':4,\n",
    "    'max_control_sup':6,\n",
    "    'max_frag_len':300,\n",
    "    'max_cont_dist':4,\n",
    "    'min_strand_frac':0.76,\n",
    "    'require_overlap':0,\n",
    "}\n",
    "\n",
    "sliders = [\n",
    "    [{'min_strand_cov':x} for x in range(10)],\n",
    "    [{'min_total_cov':x} for x in range(20)],\n",
    "    [{'min_high_sup':x} for x in range(20)],\n",
    "    [{'min_avg_mq':x} for x in range(44)],\n",
    "    [{'max_emm':x / 40} for x in range(42)],\n",
    "    [{'min_indel_end_dist':x} for x in range(20)],\n",
    "    [{'min_cov_per':x / 1000} for x in range(52)],\n",
    "    [{'min_dup_dist':x} for x in range(5)],\n",
    "    [{'max_poly_at':x} for x in range(10)],\n",
    "    [{'max_poly_rep':x} for x in range(10)],\n",
    "    [{'max_control_sup':x} for x in range(50)],\n",
    "    [{'max_frag_len':x} for x in range(250, 350, 10)],\n",
    "    [{'max_cont_dist':x} for x in range(10)],\n",
    "    [{'min_strand_frac':x / 20} for x in range(22)],\n",
    "    [{'require_overlap':x} for x in [False, True]],\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(math.ceil(len(sliders) / 3), 3, figsize=(20, 12), gridspec_kw={'hspace':0.33}, sharey=True)\n",
    "for i, slider in tqdm(enumerate(sliders)):\n",
    "    # if i != 4:\n",
    "    #     continue\n",
    "    swap_fpr = []\n",
    "    swap_tpc = []\n",
    "    ct_rates = []\n",
    "    for filter_set in slider:\n",
    "        df_r = filter_vars(df_real, **filter_set) # .drop_duplicates('chrom pos ref alt'.split())\n",
    "        df_f = filter_vars(df_fake, **filter_set)\n",
    "        rates = fp_rates(df_r, df_f)\n",
    "        swap_fpr.append(min(0.2, rates[0]) if not np.isnan(rates[0]) else np.nan) # output a max FP rate of 0.2, keep nans (min(x, nan) evalues to x)\n",
    "        swap_tpc.append(rates[1])\n",
    "        ct_rates.append(ct_rate(df_r))\n",
    "    ct_max = max([x for x in ct_rates if not np.isnan(x)]) # calculate the max ct within the subplot\n",
    "    ct_depletion = [min(0.2, ct_max - x) if not np.isnan(x) else np.nan for x in ct_rates] # calculate the max - threshold ct rate, limit to 0.2 and propagate nans\n",
    "    \n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    x_axis = list(slider[0].keys())[0]\n",
    "    x_values = [filter_set[x_axis] for filter_set in slider]\n",
    "    ax_count = axs[row, col].twinx()\n",
    "    a, = axs[row, col].plot(x_values, swap_fpr, c=plt.get_cmap('tab20')(6))\n",
    "    b, = axs[row, col].plot(x_values, ct_depletion, c=plt.get_cmap('tab20')(7))\n",
    "    c, = ax_count.plot(x_values, swap_tpc, c=plt.get_cmap('tab20')(0))\n",
    "    axs[row, col].set_xlabel(param_to_desc[x_axis])\n",
    "    d = axs[row, col].vlines(cur_cutoffs[x_axis], ymin=-1, ymax=1, color='black')\n",
    "    \n",
    "    \n",
    "axs[0, 0].set_ylim(0, 0.205)\n",
    "fig.legend([a, b, c, d], ['FP rate (est swapped samples)', 'C>T rate below max (max rate within plot - rate at threshold)', 'TPs detected (est swapped samples)', 'selected cutoff'], loc='upper center')\n",
    "fig.savefig('figs/varying_nanoseq_mutation_filtering_cutoffs.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check for increased mutation rate near fragment ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 3\n",
    "max_end_dist = 75\n",
    "\n",
    "df_seq_err = None\n",
    "df_pcr_err = None\n",
    "frag_end_cov = np.zeros(150)\n",
    "i = 0\n",
    "for df_chunk in tqdm(pd.read_table('data/variant/big_Col-0-1_duplex.tsv', quoting=csv.QUOTE_NONE, chunksize=100000)):\n",
    "    df_s = df_chunk[((df_chunk.f_sup == 1) & (df_chunk.r_sup == 0) & (df_chunk.r_cov > 0)) | ((df_chunk.r_sup == 1) & (df_chunk.f_sup == 0) & (df_chunk.f_cov > 0))]\n",
    "    df_p = df_chunk[((df_chunk.f_read1_sup == df_chunk.f_read1_cov) & (df_chunk.r_read1_sup == 0) & (df_chunk.r_read1_cov > 0)) | \n",
    "                    ((df_chunk.r_read1_sup == df_chunk.r_read1_cov) & (df_chunk.f_read1_sup == 0) & (df_chunk.f_read1_cov > 0))]\n",
    "    \n",
    "    df_seq_err = df_s if df_seq_err is None else df_seq_err.append(df_s)\n",
    "    df_pcr_err = df_p if df_pcr_err is None else df_pcr_err.append(df_p)\n",
    "    \n",
    "    for frag_len in df_chunk.frag_len:\n",
    "        frag_end_cov[:min(150, frag_len // 2)] += 2\n",
    "    if i > 100:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "# bin the fragment coverage\n",
    "frag_cov_binned = np.ones(max_end_dist // bin_size)\n",
    "for i in range(len(frag_end_cov)):\n",
    "    if i // bin_size < len(frag_cov_binned):\n",
    "        frag_cov_binned[i // bin_size] += frag_end_cov[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snps_and_indels_over_frag(df):\n",
    "    frag_indels = np.zeros(max_end_dist // bin_size)\n",
    "    frag_snvs = np.zeros(max_end_dist // bin_size)\n",
    "    for r in df.itertuples():\n",
    "        p = min(r.pos - r.frag_start, r.frag_start + r.frag_len - r.pos) // bin_size\n",
    "        if p >= len(frag_snvs):\n",
    "            continue\n",
    "        if r.ref == '*' or r.alt == '*':\n",
    "            frag_indels[p] += 1\n",
    "        else:\n",
    "            frag_snvs[p] += 1\n",
    "\n",
    "    return frag_snvs, frag_indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds(arr):\n",
    "    intervals = np.zeros((2, len(arr)))\n",
    "    for i in range(len(arr)):\n",
    "        intervals[:, i] = stats.binom.interval(0.95, sum(arr), arr[i] / sum(arr))\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# called_snp, called_indel = snps_and_indels_over_frag(filter_vars(df_real))\n",
    "# unfiltered_snp, unfiltered_indel = snps_and_indels_over_frag(df_real.sample(100000))\n",
    "# seq_err_snp, seq_err_indel = snps_and_indels_over_frag(df_seq_err)\n",
    "# pcr_err_snp, pcr_err_indel = snps_and_indels_over_frag(df_pcr_err)\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(6, 3), sharex=True)\n",
    "x = [x * bin_size for x in range(len(called_snp))]\n",
    "\n",
    "axs[0].set_title('SNVs', y=0.7)\n",
    "axs[0].plot(x, called_snp / sum(called_snp) / frag_cov_binned)\n",
    "axs[0].plot(x, unfiltered_snp / sum(unfiltered_snp) / frag_cov_binned)\n",
    "axs[0].plot(x, seq_err_snp / sum(seq_err_snp) / frag_cov_binned)\n",
    "axs[0].plot(x, pcr_err_snp / sum(pcr_err_snp) / frag_cov_binned)\n",
    "axs[0].legend(['called mutations', 'unfiltered variants', 'sequencing errors', 'PCR errors'], bbox_to_anchor=(1,1), loc='upper left')\n",
    "\n",
    "axs[0].fill_between(x, bounds(called_snp)[0] / sum(called_snp) / frag_cov_binned, bounds(called_snp)[1] / sum(called_snp) / frag_cov_binned, alpha=0.2)\n",
    "axs[0].fill_between(x, bounds(unfiltered_snp)[0] / sum(unfiltered_snp) / frag_cov_binned, bounds(unfiltered_snp)[1] / sum(unfiltered_snp) / frag_cov_binned, alpha=0.2)\n",
    "axs[0].fill_between(x, bounds(seq_err_snp)[0] / sum(seq_err_snp) / frag_cov_binned, bounds(seq_err_snp)[1] / sum(seq_err_snp) / frag_cov_binned, alpha=0.2)\n",
    "axs[0].fill_between(x, bounds(pcr_err_snp)[0] / sum(pcr_err_snp) / frag_cov_binned, bounds(pcr_err_snp)[1] / sum(pcr_err_snp) / frag_cov_binned, alpha=0.2)\n",
    "\n",
    "axs[1].set_title('indels', y=0.7)\n",
    "axs[1].plot(x, called_indel / sum(called_indel) / frag_cov_binned)\n",
    "axs[1].plot(x, unfiltered_indel / sum(unfiltered_indel) / frag_cov_binned)\n",
    "axs[1].plot(x, seq_err_indel / sum(seq_err_indel) / frag_cov_binned)\n",
    "axs[1].plot(x, pcr_err_indel / sum(pcr_err_indel) / frag_cov_binned)\n",
    "\n",
    "axs[1].fill_between(x, bounds(called_indel)[0] / sum(called_indel) / frag_cov_binned, bounds(called_indel)[1] / sum(called_indel) / frag_cov_binned, alpha=0.2)\n",
    "axs[1].fill_between(x, bounds(unfiltered_indel)[0] / sum(unfiltered_indel) / frag_cov_binned, bounds(unfiltered_indel)[1] / sum(unfiltered_indel) / frag_cov_binned, alpha=0.2)\n",
    "axs[1].fill_between(x, bounds(seq_err_indel)[0] / sum(seq_err_indel) / frag_cov_binned, bounds(seq_err_indel)[1] / sum(seq_err_indel) / frag_cov_binned, alpha=0.2)\n",
    "axs[1].fill_between(x, bounds(pcr_err_indel)[0] / sum(pcr_err_indel) / frag_cov_binned, bounds(pcr_err_indel)[1] / sum(pcr_err_indel) / frag_cov_binned, alpha=0.2)\n",
    "\n",
    "axs[1].set_xlabel('Distance from fragment end (bp)')\n",
    "axs[0].set_ylabel('Mutation/error rate', ha='right')\n",
    "fig.savefig('figs/mutation_rate_across_fragment_length.svg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use subsampled files to determine min required coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_levels = [(1, 1, 3), (1, 2, 3), (2, 2, 3), (3, 3, 3)] # (f coverage, r coverage, replicates)\n",
    "sub_files = [] # subsample files generated by duplex_subsampler\n",
    "for x in subsample_levels:\n",
    "    for i in range(x[2]):\n",
    "        sub_files.append(f'../data/variant/subsample/big_Col-0-1_{x[0]}f{x[1]}rpairs_{i}_duplex_strandsup.tsv')\n",
    "high_file = '../data/variant/subsample/big_Col-0-1_ge4pairs_duplex_strandsup.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # start worker processes from pool\n",
    "    with multiprocessing.Pool(processes=8) as pool:\n",
    "        processes = []\n",
    "        naive_controls = naive_var_files[1:8]\n",
    "        informed_controls = informed_var_files[1:8]\n",
    "        for i in range(len(sub_files)):\n",
    "            processes.append(pool.apply_async(add_mut_info, (sub_files[i], naive_controls, informed_controls, f'tmp/added_info_{i}.tsv')))\n",
    "        processes.append(pool.apply_async(add_mut_info, (high_file, naive_controls, informed_controls, f'tmp/added_info_{len(sub_files)}.tsv')))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each set of variants with the added info, then filter to only mutations\n",
    "dfs_sub = []\n",
    "for i in tqdm(range(len(sub_files) + 1)):\n",
    "    df = pd.read_table(f'tmp/added_info_{i}.tsv', quoting=csv.QUOTE_NONE)\n",
    "    print(f'Loaded {len(df)} raw mutations')\n",
    "    \n",
    "    df['f_read1_frac'] = df.f_read1_sup / df.f_read1_cov\n",
    "    df['r_read1_frac'] = df.r_read1_sup / df.r_read1_cov\n",
    "    df['f_frac'] = df.f_sup / df.f_cov\n",
    "    df['r_frac'] = df.r_sup / df.r_cov\n",
    "    df.worst_frac = df.worst_frac.fillna(0)\n",
    "    df.total_frac = df.total_frac.fillna(0)\n",
    "    \n",
    "    df = filter_vars(df, 0.76, -1, 2, -1, 4, 10, 0.21, 0.01, -1, 5, 3, 1, 1, 1, 8, 10)\n",
    "    df = df.drop_duplicates('chrom pos ref alt'.split()) # keep only one copy of a mutation\n",
    "    print(f'{len(df)} unique mutations remain after filters')\n",
    "    \n",
    "    dfs_sub.append(df)\n",
    "\n",
    "df_high = dfs_sub[-1]\n",
    "dfs_sub = dfs_sub[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check how many mutations don't have enough support in the high coverage file, but do in the subsampled files (FP)\n",
    "for i in range(len(dfs_sub)):\n",
    "    dfs_sub[i] = dfs_sub[i].merge(df_high['chrom pos ref alt'.split()], how='outer', indicator='found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for sub in subsample_levels:\n",
    "    s += [f'{sub[0]}f{sub[1]}r'] * sub[2]\n",
    "for i in range(len(dfs_sub)):\n",
    "    tp = sum(dfs_sub[i].found == 'both')\n",
    "    fp = sum(dfs_sub[i].found == 'left_only')\n",
    "    fn = sum(dfs_sub[i].found == 'right_only')\n",
    "    print(f'{s[i]}\\tTP: {tp}\\tFP: {fp}\\tFN: {fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = dfs_sub[0].append(dfs_sub[1]).append(dfs_sub[2])#[dfs_sub[0].found == 'left_only']\n",
    "gtools.sam_view_df(df_tmp, ['../data/align/big_Col-0-1_merged.bam', '../data/align/subsample/big_Col-0-1_1f1rpairs_0.bam', '../data/align/subsample/big_Col-0-1_1f1rpairs_1.bam', '../data/align/subsample/big_Col-0-1_1f1rpairs_2.bam', '../data/align/subsample/big_Col-0-1_ge4pairs.bam'], run=False)\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
