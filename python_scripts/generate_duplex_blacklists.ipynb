{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/../python_scripts')\n",
    "import gtools\n",
    "import argparse\n",
    "import subprocess\n",
    "import pysam\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Creates blacklist files for filtering variants of duplex-seq libraries. Takes the reference genome and a set of BAM files for coverage calculations as input. \n",
    "Requires GenMap (https://github.com/cpockrandt/genmap) to be installed. Produces four .npy arrays for each chromosome: |n\n",
    "    - {prefix}{chrom}_coverage_percentile.npy: the coverage percentile of each site in the genome, e.g. 0.01 means 1% of the genome has a lower or equal coverage|n\n",
    "    - {prefix}{chrom}_duplicate_distance.npy: the minimum hamming distance between a kmer at the genomic position of length --fragment_length and any other position in the genome.\n",
    "        e.g. 2 means there's another kmer in the genome where only 2 bases differ. If no kmer is discovered, a value of 10 is assigned.|n\n",
    "    - {prefix}{chrom}_poly_at_length.npy: length of the longest poly-A/T repeat starting at the position or 1bp away. Only considers lengths >=4. e.g.|n\n",
    "|t        sequence:  ACTGAAAAAAAAGTCCCA|n\n",
    "|t        blacklist: 000880000008800000|n\n",
    "    - {prefix}{chrom}_poly_repeat_length.npy: length of the longest poly-di/trinucleotide repeat starting at the position of 1bp away. Only considers length >=3. e.g.|n\n",
    "|t        sequence:  CCCGAGAGAGAGACCTC|n\n",
    "|t        blacklist: 00554000000455000\n",
    "    ''', formatter_class=gtools.EscapeFormatter)\n",
    "parser.add_argument('-f', '--fasta', required=True, dest='fasta', metavar='FASTA', type=str,\n",
    "                   help='genome fasta')\n",
    "parser.add_argument('-b', '--bedgraphs', required=True, dest='bedgraphs', metavar='BAMs', type=str, nargs='+',\n",
    "                   help='one or more coverage bedgraph files used to calculate coverage percentile per genomic site. These can \\\n",
    "                   be generated with \"bedtools genomecov -bg -ibam {BAM}\"')\n",
    "parser.add_argument('-o', '--output', dest='prefix', metavar='PREFIX', type=str, default='./',\n",
    "                   help='prefix to prepend to blacklist outputs (default: ./)')\n",
    "parser.add_argument('-l', '--fragment_length', dest='fragment_length', metavar='INT', type=int, default=150,\n",
    "                   help='length of DNA fragments. Used for calculating the duplicate distance blacklist and passed to genmap -k argument (default: 150)')\n",
    "parser.add_argument('-m', '--max_hamming', dest='max_hamming', metavar='INT', type=int, default=3,\n",
    "                    help='maximum hamming distance to check with genmap for making the duplicate distance blacklist. Increasing this increases runtime. Setting \\\n",
    "                    to a negative value will skip running genmap and the blacklist will be all 10s (default: 3)')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='INT', type=int, default=1,\n",
    "                    help='number of threads to use when calculating genome coverage (default: 1)')\n",
    "parser.add_argument('-t', '--tmp', dest='tmp_prefix', metavar='PREFIX', type=str, default='tmp/',\n",
    "                   help='prefix to add to temporary files. Will make any directories which do not exist (default: tmp/)')\n",
    "# parser.add_argument('-@', '--threads', dest='threads', metavar='INT', type=int, default=1,\n",
    "#                    help='number of threads. Each chromosome can be processed by a separate thread. (default: 1)')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    print('Determined code is running in Jupyter')\n",
    "    if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "        os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "    args = parser.parse_args(' -@ 8 --fasta data/ref/ref.fa --bams data/align/big_Col-0-1_merged.bam data/align/big_Col-0-1_merged.bam data/align/big_Col-0-1_merged.bam data/align/big_Col-0-1_merged.bam data/align/big_Col-0-1_merged.bam data/align/big_Col-0-1_merged.bam data/align/big_Col-0-7_merged.bam data/align/big_Col-0-8_merged.bam --output data/blacklist/new_arabidopsis/'.split()) # used for testing\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "sys.stderr.write('Running generate_duplex_blacklists.py with arguments:\\n' + '\\n'.join([f'{key}={val}' for key, val in vars(args).items()]) + '\\n')\n",
    "\n",
    "no_match_dist = 10\n",
    "if args.max_hamming >= 10:\n",
    "    no_match_dist = args.max_hamming + 1\n",
    "    print(f'WARNING: --max_hamming is >=10, changing the value reported for no kmer detected to {args.max_hamming + 1}')\n",
    "\n",
    "if args.prefix and '/' in args.prefix:\n",
    "    os.makedirs(os.path.dirname(args.prefix), exist_ok=True)\n",
    "if args.tmp_prefix and '/' in args.tmp_prefix:\n",
    "    os.makedirs(os.path.dirname(args.tmp_prefix), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shell(command):\n",
    "    proc = subprocess.run(command, shell=True, capture_output=True)\n",
    "    if proc.returncode != 0:\n",
    "        sys.stderr.write(f'ERROR: failed to run \"{command}\"\\n')\n",
    "        sys.stderr.write(str(proc.stderr))\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = gtools.load_genome(args.fasta)\n",
    "chrom_sizes = {chrom:len(genome[chrom]) for chrom in genome}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Duplicate distance blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.max_hamming >= 0: # only run this if necessary\n",
    "    sys.stderr.write('calculating duplicate distance blacklist\\n')\n",
    "    \n",
    "    sys.stderr.write('making a genmap index of the reference fasta\\n')\n",
    "    index_dir = f'{args.tmp_prefix}generate_duplex_blacklists_genmap_index_{args.fasta.replace(\"/\", \"_\")}'\n",
    "    run_shell(f'genmap index -F {args.fasta} -I {index_dir}')\n",
    "    \n",
    "    # for each hamming distance (allowable mismatches), run genmap map\n",
    "    duplicate_dist = {chrom:np.full(chrom_sizes[chrom], no_match_dist, dtype=np.byte) for chrom in chrom_sizes}\n",
    "    for i in range(args.max_hamming, -1, -1): \n",
    "        sys.stderr.write(f'searching for kmers with max of {i} errors\\n')\n",
    "        bedgraph_prefix = f'{args.tmp_prefix}generate_duplex_blacklists_{args.fasta.replace(\"/\", \"_\")}_{args.fragment_length}_{i}'\n",
    "        run_shell(f'genmap map -K {args.fragment_length} -bg -E {i} -I {index_dir} -O {bedgraph_prefix}')\n",
    "        \n",
    "        # open the genmap bedgraph and add its info to duplicate_dist\n",
    "        with open(f'{bedgraph_prefix}.bedgraph', 'r') as f:\n",
    "            for l in f:\n",
    "                fields = l.split()\n",
    "                # set values within the bedgraph interval. If occurences is 1 (unique) keep value the same, else set value to i\n",
    "                duplicate_dist[fields[0]][int(fields[1]):int(fields[2])] = duplicate_dist[fields[0]][int(fields[1]):int(fields[2])] * (fields[3] == '1') + i * (fields[3]  != '1')\n",
    "        run_shell(f'rm {bedgraph_prefix}.bedgraph')\n",
    "    run_shell(f'rm -r {index_dir}')\n",
    "    \n",
    "    for i in range(args.max_hamming + 1):\n",
    "        sys.stderr.write(f'{sum([np.count_nonzero(duplicate_dist[chrom] == i) for chrom in chrom_sizes]) / sum(chrom_sizes.values()) * 100}% of the genome has a (120, {i}) duplicate\\n')\n",
    "    \n",
    "    sys.stderr.write(f'writing duplicate distance blacklists to {args.prefix}{{chrom}}_duplicate_distance.npy\\n')\n",
    "    for chrom in duplicate_dist:\n",
    "        np.save(f'{args.prefix}{chrom}_duplicate_distance.npy', duplicate_dist[chrom])\n",
    "    del duplicate_dist\n",
    "else:\n",
    "    sys.stderr.write(f'--max_hamming is <0, creating duplicate distance blacklists with a {no_match_dist} for every site\\n')\n",
    "    for chrom in chrom_sizes:\n",
    "        np.save(f'{args.prefix}{chrom}_duplicate_distance.npy', np.full(chrom_sizes[chrom], no_match_dist, dtype=np.byte))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Coverage blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def genomecov(fin, fout):\n",
    "#     run_shell(f'bedtools genomecov -bg -ibam {fin} > {fout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert BAM files to bedgraphs\n",
    "# if __name__ == '__main__':\n",
    "#     # start worker processes from pool\n",
    "#     sys.stderr.write('converting BAM files to coverage bedgraphs using bedtools genomecov\\n')\n",
    "#     tmp_files = [f'{args.tmp_prefix}generate_duplex_blacklists_coverage_{args.fasta.replace(\"/\", \"_\")}_{i}.bg' for i in range(len(args.bedgraphs))]\n",
    "#     with multiprocessing.Pool(processes=max(1, args.threads)) as pool:\n",
    "#         processes = []\n",
    "#         for i, f in enumerate(args.bedgraphs): # for each chromosome, add a process\n",
    "#             processes.append(pool.apply_async(genomecov, (f, tmp_files[i])))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "#     sys.stderr.flush()\n",
    "#     time.sleep(2)\n",
    "#     # check for errors\n",
    "#     for p in processes:\n",
    "#         p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('loading coverage bedgraphs\\n')\n",
    "coverage = {chrom:np.zeros(chrom_sizes[chrom], dtype=int) for chrom in chrom_sizes}\n",
    "for bg in args.bedgraphs:\n",
    "    with open(bg, 'r') as f:\n",
    "        for l in f:\n",
    "            fields = l.split()\n",
    "            if 'e' in fields[3]: # if number is in scientific notation, python needs to convert to int using a float intermediate\n",
    "                c = int(float(fields[3]))\n",
    "            else:\n",
    "                c = int(fields[3])\n",
    "            coverage[fields[0]][int(fields[1]):int(fields[2])] += c\n",
    "    # run_shell(f'rm {tmp_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 10M positions of the genome and get their coverage\n",
    "sys.stderr.write('calculating conversion of coverage to percentile\\n')\n",
    "sample_size = 1000000\n",
    "sampled_covs = []\n",
    "for chrom in chrom_sizes:\n",
    "    k = int(sample_size * (chrom_sizes[chrom] / sum(chrom_sizes.values())))\n",
    "    for i in range(k):\n",
    "        sampled_covs.append(coverage[chrom][random.randrange(chrom_sizes[chrom])])\n",
    "\n",
    "# count the number of positions with each coverage value\n",
    "maxcov = int(np.percentile(sampled_covs, 99.9))\n",
    "cov_counts = np.zeros(maxcov + 1)\n",
    "for cov in sampled_covs:\n",
    "    cov_counts[min(maxcov, cov)] += 1\n",
    "\n",
    "# # count the number of positions with coverage less than n for n=(0, max coverage)\n",
    "# cov_less_than = np.zeros(maxcov)\n",
    "# for i in range(len(cov_less_than)):\n",
    "#     cov_less_than[i] = sum(cov_less_thha)\n",
    "    \n",
    "# make an array that takes a coverage value and outputs the percentile\n",
    "percentile = np.zeros(maxcov)\n",
    "for i in range(len(percentile)):\n",
    "    percentile[i] = sum(cov_counts[:i]) / len(sampled_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write(f'coverage\\tpercentile\\n')\n",
    "for i in [0] + list(range(1, len(percentile), len(percentile) // 10)):\n",
    "    sys.stderr.write(f'{i}\\t\\t{percentile[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each coverage value in the genome into a percentile\n",
    "sys.stderr.write('converting coverage values to percentiles\\n')\n",
    "cov_percentile = dict() # cov_percentile[chrom][pos] = percent of genome with lesser coverage\n",
    "for chrom in tqdm(chrom_sizes):\n",
    "    # apply function to coverage array to convert coverage to percentile\n",
    "    cov_percentile[chrom] = np.vectorize(lambda x: percentile[x] if x < len(percentile) else percentile[-1])(coverage[chrom])\n",
    "    del coverage[chrom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write(f'writing coverage percentile blacklists to {args.prefix}{{chrom}}_coverage_percentile.npy\\n')\n",
    "for chrom in duplicate_dist:\n",
    "    np.save(f'{args.prefix}{chrom}_coverage_percentile.npy', cov_percentile[chrom])\n",
    "del cov_percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Poly A/T and poly di/tri-nucleotide repeat blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('calculating poly A/T and poly di/tri-nucleotide repeat blacklists\\n')\n",
    "\n",
    "\n",
    "poly_at_len = {chrom:np.zeros(chrom_sizes[chrom], dtype='b') for chrom in chrom_sizes} \n",
    "    # poly_at_len[chrom][pos] = length of longest poly-A or poly-T which starts or ends at the position (or the position is 1bp outside of)\n",
    "poly_rep_len = {chrom:np.zeros(chrom_sizes[chrom], dtype='b') for chrom in chrom_sizes}\n",
    "    # poly_rep_len[chrom][pos] = length (in number of repeats) of longest di/tri-nuc repeat which starts or ends at the position (or the position is 1bp outside of)\n",
    "\n",
    "# get regex iterables for each possible nucleotide repeat\n",
    "match_iters = defaultdict(lambda: [])\n",
    "for chrom in chrom_sizes:\n",
    "    match_iters[chrom].append((1, re.finditer('AAAA+', genome[chrom]))) # poly A, the \"1\" here is the length of repeat, will be \"2\" for di and \"3\" for tri\n",
    "    match_iters[chrom].append((1, re.finditer('TTTT+', genome[chrom]))) # poly T\n",
    "    for x in 'ACGT':\n",
    "        for y in 'ACGT':\n",
    "            if x != y: # two nucleotides must be different to be a dinucleotide repeat\n",
    "                match_iters[chrom].append((2, re.finditer('(' + x + y + '){3,}', genome[chrom]))) # dinucleotide repeats\n",
    "            for z in 'ACGT':\n",
    "                if x != y or x != z or y != z: # at least one pair of nucleotides must be different to be a trinucleotide repeat\n",
    "                    match_iters[chrom].append((3, re.finditer('(' + x + y + z + '){3,}', genome[chrom]))) # trinucleotide repeats\n",
    "    \n",
    "# iterate through each regex iterable and add the length of the repeat to the array\n",
    "for chrom in match_iters:\n",
    "    for replen, it in tqdm(match_iters[chrom], desc=chrom): # for each regex search (e.g. CT repeats)\n",
    "        for match in it:\n",
    "            start = match.span()[0]\n",
    "            end = match.span()[1]\n",
    "            poly_len = (end - start) // replen\n",
    "            if replen == 1:\n",
    "                poly_at_len[chrom][start - 1] = max(poly_at_len[chrom][start - 1], poly_len)\n",
    "                poly_at_len[chrom][start] = max(poly_at_len[chrom][start], poly_len)\n",
    "                poly_at_len[chrom][end - 1] = max(poly_at_len[chrom][end - 1], poly_len)\n",
    "                if end != chrom_sizes[chrom]: # this happens if the chrom ends with a poly repeat\n",
    "                    poly_at_len[chrom][end] = max(poly_at_len[chrom][end], poly_len)\n",
    "            else:\n",
    "                poly_rep_len[chrom][start - 1] = max(poly_rep_len[chrom][start - 1], poly_len)\n",
    "                poly_rep_len[chrom][start] = max(poly_rep_len[chrom][start], poly_len)\n",
    "                poly_rep_len[chrom][end - 1] = max(poly_rep_len[chrom][end - 1], poly_len)\n",
    "                if end != chrom_sizes[chrom]:\n",
    "                    poly_rep_len[chrom][end] = max(poly_rep_len[chrom][end], poly_len)\n",
    "\n",
    "for i in range(4, 10):\n",
    "    sys.stderr.write(f'blacklisted {sum(np.count_nonzero(poly_at_len[chrom] == i) for chrom in chrom_sizes) / sum(chrom_sizes.values()) * 100}% for length {i} A/T nucleotide repeats\\n')\n",
    "for i in range(3, 10):\n",
    "    sys.stderr.write(f'blacklisted {sum(np.count_nonzero(poly_rep_len[chrom] == i) for chrom in chrom_sizes) / sum(chrom_sizes.values()) * 100}% for length {i} di/tri nucleotide repeats\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write(f'writing poly-A/T blacklists to {args.prefix}{{chrom}}_poly_at_length.npy\\n')\n",
    "sys.stderr.write(f'writing poly-di/trinucleotide repeat blacklists to {args.prefix}{{chrom}}_poly_repeat_length.npy\\n')\n",
    "for chrom in chrom_sizes:\n",
    "    np.save(f'{args.prefix}{chrom}_poly_at_length.npy', poly_at_len[chrom])\n",
    "    np.save(f'{args.prefix}{chrom}_poly_repeat_length.npy', poly_rep_len[chrom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write(f'completed generate_duplex_blacklists.py on fasta {args.fasta}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
