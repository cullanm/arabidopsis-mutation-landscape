{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtools\n",
    "import pysam\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import multiprocessing\n",
    "from os.path import exists\n",
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Calculates the callable fragment coverage for duplex-seq. Takes a BAM file of duplex-seq reads as input and determines the number of callable fragments \n",
    "overlapping each position in the genome, in other words, the number of opportunities a mutation could have been detected and pass all duplex-seq filters \n",
    "at each position. This script is not 100% accurate, as it does not consider indels in reads, and indels alter the reference bases a read spans.\n",
    "    ''')\n",
    "parser.add_argument(dest='input', metavar='INPUT_FILE', type=str, \n",
    "                   help='BAM file to call from. Must be coordinate sorted and have an index file')\n",
    "parser.add_argument(dest='output', metavar='OUTPUT_PREFIX', type=str,\n",
    "                   help='prefix to append to output numpy arrays, final outputs will be {prefix}{chrom}.npy for each chromosome')\n",
    "# parser.add_argument('-m', '--min_mapped_cov', dest='min_mapped_cov', metavar='MIN_MAPPED_COV', type=int, default=1,\n",
    "#                    help='number of reads from each mapped strand required to call variants (default: 1)')\n",
    "parser.add_argument('-t', '--min_total_cov', dest='min_total_cov', metavar='INT', type=int, default=6,\n",
    "                   help='number of covering reads required to call variants within a fragment (default: 6)')\n",
    "parser.add_argument('-s', '--min_orig_cov', dest='min_orig_cov', metavar='INT', type=int, default=2,\n",
    "                   help='number of reads required from each original strand to call variants within a fragment (default: 2)')\n",
    "parser.add_argument('-q', '--min_mq', dest='min_mq', metavar='FLOAT', type=int, default=10,\n",
    "                   help='minimum average mq of pcr duplicate reads required to call variants within a fragment (default: 10)')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='INT', type=int, default=1,\n",
    "                   help='chromosomes are divided between threads. Cannot use more threads than num chromosomes + 1 (default: 1)')\n",
    "parser.add_argument('-b', '--blacklist', dest='blacklist', metavar='PREFIX', type=str, default=None,\n",
    "                   help='''prefix of the blacklist files generated by duplex_blacklist_regions. Files should be \n",
    "                   \"{prefix}{contig}_{type}.npy\" where type is coverage_percentile, duplicate_distance, poly_at_length, \n",
    "                   and poly_repeat_length (default: do not use blacklist)''')\n",
    "parser.add_argument('-c', '--blacklist_cutoffs', dest='blacklist_cutoffs', metavar='FLOAT,INT,INT,INT', type=str, default='0.007,1,-1,7,4',\n",
    "                   help='''cutoffs to use for the blacklist. Same order as listed in \"-b\" help section. Values equal to the cutoff are \n",
    "                    counted as callable (default: 0.007,1,-1,7,4)''')\n",
    "parser.add_argument('-e', '--end_dist', dest='end_dist', metavar='INT', type=int, default=5,\n",
    "                   help='minimum distance from the fragment end required to call a mutation. The end mismatch rate filter makes \\\n",
    "                   fragment ends uncallable (default: 5)')\n",
    "parser.add_argument('-l', '--max_frag_len', dest='max_frag_len', metavar='INT', type=int, default=300,\n",
    "                   help='maximum fragment length before a fragment is uncallable (default: 300)')\n",
    "parser.add_argument('-r', '--require_overlap', dest='require_overlap', action='store_true',\n",
    "                   help='whether read1-read2 overlap is required to call a variant. WARNING: setting this only gives the correct \\\n",
    "                   output when reads are 150bp (default: not required)')\n",
    "parser.add_argument('-u', '--umi', dest='umi', metavar='UMI', type=str, default=None,\n",
    "                   help='Consider the UMI in the provided BAM tag when grouping pre-PCR fragments (default: no UMI)')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    print('Determined code is running in Jupyter')\n",
    "    if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "        os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "    args = parser.parse_args('data/align/chandler_freq_problem.bam tmp/coverage_test_ --max_frag_len 2000 -@ 1 -b data/blacklist/arabidopsis/duplex_blacklist_ --umi RX'.split()) # used for testing\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "sys.stderr.write('Running duplex_coverage.py with arguments:\\n' + '\\n'.join([f'{key}={val}' for key, val in vars(args).items()]) + '\\n')\n",
    "\n",
    "if not exists(args.input):\n",
    "    sys.stderr.write('ERROR: input file {} does not exist\\n'.format(args.input))\n",
    "    exit()\n",
    "\n",
    "aln = pysam.AlignmentFile(args.input, 'rb') # load BAM for doing checks\n",
    "\n",
    "# assert index can be opened\n",
    "try:\n",
    "    aln.check_index()\n",
    "except:\n",
    "    sys.stderr.write('ERROR: failed to open index file {}\\n'.format(args.input + '.bai'))\n",
    "    exit()\n",
    "\n",
    "if args.output and '/' in args.output:\n",
    "    os.makedirs(os.path.dirname(args.output), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that file is coordinate sorted\n",
    "last = ('', 0)\n",
    "read_length = -1\n",
    "for i, read in enumerate(aln.fetch()):\n",
    "    # check if current read aligned before previous read (out of order)\n",
    "    nex = (read.reference_name, read.reference_start)\n",
    "    if nex < last:\n",
    "        sys.stderr.write('ERROR: BAM file is not coordinate sorted\\n')\n",
    "        exit()\n",
    "    last = nex\n",
    "    read_length = max(read_length, read.query_length)\n",
    "    if i > 10000:\n",
    "        break\n",
    "sys.stderr.write(f'inferred a read length of {read_length}bp\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = [x.contig for x in aln.get_index_statistics()]\n",
    "chroms.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.blacklist is not None:\n",
    "    # all are indexed as bl[chrom][pos] = value\n",
    "    bl_cov_per = dict() # coverage percentile\n",
    "    bl_dup_dist = dict() # mismatches required to have a duplicate 120mer in the genome\n",
    "    bl_poly_at = dict() # length of poly-A or poly-T starting or ending at position\n",
    "    bl_poly_rep = dict() # length of di or trinucleotide repeat starting or ending at position\n",
    "    sys.stderr.write('Loading blacklist files\\n')\n",
    "    for chrom in tqdm(chroms):\n",
    "        bl_cov_per[chrom] = np.load(f'{args.blacklist}{chrom}_coverage_percentile.npy')\n",
    "        bl_dup_dist[chrom] = np.load(f'{args.blacklist}{chrom}_duplicate_distance.npy')\n",
    "        bl_poly_at[chrom] = np.load(f'{args.blacklist}{chrom}_poly_at_length.npy')\n",
    "        bl_poly_rep[chrom] = np.load(f'{args.blacklist}{chrom}_poly_repeat_length.npy')\n",
    "    \n",
    "    # extract the blacklist cutoffs from the command line parameter\n",
    "    s = args.blacklist_cutoffs.split(',')\n",
    "    assert len(s) == 5\n",
    "    cutoff_cov_min = float(s[0])\n",
    "    cutoff_cov_max = float(s[1])\n",
    "    cutoff_dup_dist = int(s[2])\n",
    "    cutoff_poly_at = int(s[3])\n",
    "    cutoff_poly_rep = int(s[4])\n",
    "    \n",
    "    # combine the cov_per, poly_at, and poly_rep blacklists to a single boolean blacklist where true means callable\n",
    "    bl_bool = dict()\n",
    "    for chrom in chroms:\n",
    "        bl_bool[chrom] = ~((bl_cov_per[chrom] < cutoff_cov_min) | (bl_cov_per[chrom] > cutoff_cov_max)) & \\\n",
    "                         (bl_poly_at[chrom] <= cutoff_poly_at) & (bl_poly_rep[chrom] <= cutoff_poly_rep)\n",
    "    del bl_cov_per, bl_poly_at, bl_poly_rep\n",
    "else:\n",
    "    sys.stderr.write('No blacklist files provided, defaulting to whitelisting all of genome\\n')\n",
    "    cutoff_dup_dist = -1\n",
    "    \n",
    "    bl_bool = dict()\n",
    "    bl_dup_dist = dict()\n",
    "    for chrom in chroms:\n",
    "        bl_bool[chrom] = np.full(aln.get_reference_length(chrom), True)\n",
    "        bl_dup_dist[chrom] = np.zeros(aln.get_reference_length(chrom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_counts = {x.contig:x.total for x in aln.get_index_statistics()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrom_coverage(file, chrom):\n",
    "    frags = defaultdict(lambda: [0, 0, 0]) # key (frag_start, tlen, umi), value [+ strand read pairs, - strand read pairs, total MQ of forward reads]\n",
    "    aln = pysam.AlignmentFile(args.input, 'rb')\n",
    "    output = np.zeros(aln.get_reference_length(chrom), dtype=np.intc)\n",
    "    i = 0\n",
    "    for read in tqdm(aln.fetch(contig=chrom), position=0, mininterval=10, total=read_counts[chrom], desc=chrom):\n",
    "        i += 1\n",
    "        # periodically flush\n",
    "        if i % 100000 == 0 or i == read_counts[chrom]:\n",
    "            to_flush = []\n",
    "            for frag in frags:\n",
    "                if i != read_counts[chrom] and read.reference_start <= frag[0]: # if we haven't passed all the reads for the current fragment\n",
    "                    break\n",
    "                to_flush.append(frag)\n",
    "            for frag in to_flush:\n",
    "                v = frags[frag]\n",
    "                del frags[frag]\n",
    "                \n",
    "                if v[2] / (v[0] + v[1]) < args.min_mq: # insufficient MQ\n",
    "                    continue\n",
    "                if bl_dup_dist[chrom][frag[0]] < cutoff_dup_dist: # fragment fails the dup_dist blacklist\n",
    "                    continue\n",
    "                if frag[1] > args.max_frag_len: # fragment is too large\n",
    "                    continue\n",
    "                \n",
    "                overlap_callable = (v[0] * 2 >= args.min_orig_cov) and (v[1] * 2 >= args.min_orig_cov) and ((v[0] + v[1]) * 2 >= args.min_total_cov)\n",
    "                all_callable = (v[0] >= args.min_orig_cov) and (v[1] >= args.min_orig_cov) and ((v[0] + v[1]) >= args.min_total_cov) and not args.require_overlap\n",
    "\n",
    "                if all_callable:\n",
    "                    # note: this is safe even if the slice end is less than the start, arr[5:3] += 1 does nothing\n",
    "                    output[frag[0] + args.end_dist:min(frag[0] + read_length, frag[0] + frag[1] - args.end_dist)] += 1 # add coverage for the forward read\n",
    "                    output[max(frag[0] + args.end_dist, frag[0] + frag[1] - read_length):frag[0] + frag[1] - args.end_dist] += 1 # add coverage for the reverse read\n",
    "                    output[max(frag[0] + args.end_dist, frag[0] + frag[1] - read_length):min(frag[0] + read_length, frag[0] + frag[1] - args.end_dist)] -= 1 # subtract double-counted coverage in the overlap\n",
    "                elif overlap_callable:\n",
    "                    output[max(frag[0] + args.end_dist, frag[0] + frag[1] - read_length):min(frag[0] + read_length, frag[0] + frag[1] - args.end_dist)] += 1\n",
    "                    \n",
    "        # thow out read pairs that don't align as expected, ignore reverse reads so we don't double count each fragment\n",
    "        if read.is_unmapped or read.mate_is_unmapped or not (read.flag & 2) or read.is_reverse:\n",
    "            continue\n",
    "        \n",
    "        # find the fragment start and end. Used to find PCR duplicates\n",
    "        frag_start = min(read.reference_start, read.next_reference_start)\n",
    "        tlen = abs(read.template_length)\n",
    "        frag_strand = '+' if read.is_read1 else '-'\n",
    "        umi = '' if args.umi is None else read.get_tag(args.umi)\n",
    "            \n",
    "        if frag_strand == '+':\n",
    "            frags[(frag_start, tlen, umi)][0] += 1\n",
    "        else:\n",
    "            frags[(frag_start, tlen, umi)][1] += 1\n",
    "        frags[(frag_start, tlen, umi)][2] += read.mapping_quality\n",
    "    \n",
    "    output *= bl_bool[chrom] # only count positions that pass the blacklists\n",
    "    \n",
    "    sys.stderr.write(f'made coverage array for {chrom}\\n')\n",
    "    \n",
    "    np.save(f'{args.output}{chrom}.npy', output)\n",
    "\n",
    "    sys.stderr.write(f'wrote coverage array for {chrom}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # start worker processes from pool\n",
    "    pool = multiprocessing.Pool(processes=max(1, args.threads - 1))\n",
    "    processes = []\n",
    "    for chrom in chroms: # for each chromosome, add a process\n",
    "        processes.append(pool.apply_async(chrom_coverage, (args.input, chrom)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_frag_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write(f'completed duplex_coverage.py on {args.input}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverages = dict()\n",
    "# for chrom in chroms:\n",
    "#     with open(f'{args.output}{chrom}.pkl', 'rb') as f:\n",
    "#         coverages[chrom] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     print(i, coverages['Chr1'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
