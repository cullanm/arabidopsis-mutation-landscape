{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import gtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Calls variants from a coordinate sorted BAM file of duplex sequencing reads. Ignores reads that didn't map as a concordant pair. Output will treat back-to-back SNVs and \n",
    "indels longer than 1bp as multiple 1bp variants, these will need to be grouped later. For insertions, this means a CG insertion will have two entries in the output,\n",
    "a *->C and *->CG, with the second entry listing the support for the inserted G only. Note, the vast majority of variants output by this script will not be true variants,\n",
    "further filtering steps will be necessary. Output is a tsv with the following columns:|n\n",
    "    - chrom: chromosome/contig|n\n",
    "    - pos: 0-based position of the variant. For insertions, this is the site in the reference which the insertion occurs before|n\n",
    "    - ref: reference sequence, \"*\" for insertion|n\n",
    "    - alt: alt sequence, \"*\" for deletion|n\n",
    "    - frag_start: start position of the original fragment which contains the variant (two variants at the same position may be on different fragments)|n\n",
    "    - frag_length: length of the original fragment which contains the variant|n\n",
    "    - frag_umi: UMI of the original fragment (will be empty if umi argument isn't specified)|n\n",
    "    - f_read1_sup: number of reads generated from the forward strand of the original fragment which support the variant|n\n",
    "    - f_read1_cov: number of reads generated from the forward strand of the original fragment which overlap the variant|n\n",
    "    - r_read1_sup: number of reads generated from the reverse strand of the original fragment which support the variant|n\n",
    "    - r_read1_cov: number of reads generated from the reverse strand of the original fragment which overlap the variant|n\n",
    "    - f_sup: number of reads mapping to the forward strand which support the variant|n\n",
    "    - f_cov: number of reads mapping to the forward strand which overlap the variant|n\n",
    "    - r_sup: number of reads mapping to the reverse strand which support the variant|n\n",
    "    - r_cov: number of reads mapping to the reverse strand which overlap the variant|n\n",
    "    - f_read1_conc: number of read pairs from the forward strand of the original fragment where both read1 and read2 support the variant|n\n",
    "    - r_read1_conc: number of read pairs from the reverse strand of the original fragment where both read1 and read2 support the variant|n\n",
    "    - mq: mapping quality of the reads supporting the variant. ascii converted (qual + 33)|n\n",
    "    - base quality: base call quality of the variant bases. ascii converted (qual + 33)|n\n",
    "    - end_mismatch_rate: maximum mismatch rate of any window starting from a fragment end and containing the variant. e.g. Removing all variants with \n",
    "    end_mismatch_rate > 0.5 would mean ignoring the ends of each fragment until there are more matches than mismatches from the position to the fragment end.\n",
    "    ''', formatter_class=gtools.EscapeFormatter)\n",
    "parser.add_argument(dest='input', metavar='INPUT_FILE', type=str, \n",
    "                   help='BAM file to call from. Must be coordinate sorted and have an index file. Reads must be paired end with no unpaired reads present.')\n",
    "parser.add_argument(dest='output', metavar='OUTPUT_FILE', type=str,\n",
    "                   help='output variant file')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='THREADS_INT', type=int, default=1,\n",
    "                   help='chromosomes are divided between threads. Cannot use more threads than num chromosomes + 1 (default: 1)')\n",
    "parser.add_argument('-t', '--tmp', dest='tmp', metavar='TMP_STRING', type=str, default='./',\n",
    "                   help='prefix to append temporary output files (default: ./)')\n",
    "parser.add_argument('-u', '--umi', dest='umi', metavar='TAG', type=str, default=None,\n",
    "                   help='Consider the UMI in the provided BAM tag when grouping pre-PCR fragments (default: no UMI)')\n",
    "parser.add_argument('-a', '--all', dest='all', action='store_true',\n",
    "                   help='when set, outputs all variants rather than only those with support in both original strands')\n",
    "parser.add_argument('-C', '--chunk_size', dest='chunk_size', metavar='INT', type=int, default=10000,\n",
    "                   help='''No effect on output. Number of reads to process before flushing buffered information. Increasing\n",
    "                   may speed up runtime, especially for high-coverage samples. Decreasing lowers memory requirement (default: 10000)''')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    # args = parser.parse_args('tests/test_duplex_caller/duplex_caller_test_case.bam tests/test_duplex_caller/out.tsv -@ 1 --all -t tmp/'.split()) # used for testing\n",
    "    if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "        os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "    args = parser.parse_args('-@ 6 --umi RG --all data/align/big_parp2-1_merged.bam tmp/test_duplex_caller.tsv'.split()) # used for testing\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "if not os.path.isfile(args.input):\n",
    "    sys.stderr.write('ERROR: input file {} does not exist\\n'.format(args.input))\n",
    "    exit()\n",
    "\n",
    "aln = pysam.AlignmentFile(args.input, 'rb') # load BAM for doing checks\n",
    "\n",
    "# assert index can be opened\n",
    "try:\n",
    "    aln.check_index()\n",
    "except:\n",
    "    sys.stderr.write('ERROR: failed to open index file {}\\n'.format(args.input + '.bai'))\n",
    "    exit()\n",
    "\n",
    "if args.output and '/' in args.output:\n",
    "    os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "\n",
    "if args.tmp and '/' in args.tmp:\n",
    "    os.makedirs(os.path.dirname(args.tmp), exist_ok=True)\n",
    "    \n",
    "# set threads to num chromosomes + 1 if more were allocated\n",
    "if args.threads > len(aln.get_index_statistics()) + 1:\n",
    "    args.threads = len(aln.get_index_statistics()) + 1\n",
    "    sys.stderr.write('WARNING: specified more threads than number of chromosomes + 1. Reducing threads to {}\\n'.format(len(aln.get_index_statistics()) + 1))\n",
    "\n",
    "sys.stderr.write('Running duplex_caller.py with arguments:\\n' + '\\n'.join([f'{key}={val}' for key, val in vars(args).items()]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that file is coordinate sorted and contains the MD tag and specified UMI\n",
    "last = ('', 0)\n",
    "for i, read in enumerate(aln.fetch()):\n",
    "    # check if UMI tag is present\n",
    "    if args.umi is not None:\n",
    "        try:\n",
    "            read.get_tag(args.umi)\n",
    "        except KeyError:\n",
    "            sys.stderr.write(f'ERROR: BAM file does not contain the specified UMI tag \"{args.umi}\"\\n')\n",
    "            exit()\n",
    "    \n",
    "    # check if MD tag is present\n",
    "    if not read.is_unmapped and (read.flag & 2): # if properly mapped\n",
    "        try:\n",
    "            read.get_tag('MD')\n",
    "        except KeyError:\n",
    "            sys.stderr.write('ERROR: BAM file does not contain the \"MD\" tag\\n')\n",
    "            exit()\n",
    "    \n",
    "    # check if current read aligned before previous read (out of order)\n",
    "    nex = (read.reference_name, read.reference_start)\n",
    "    if nex < last:\n",
    "        sys.stderr.write('ERROR: BAM file is not coordinate sorted\\n')\n",
    "        exit()\n",
    "    last = nex\n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- code requires coordinate sorted BAM\n",
    "- code requires the BAM to contain the MD tag to function (see https://samtools.github.io/hts-specs/SAMtags.pdf). Tag follows the regex `[0-9]+(([A-Z]|\\^[A-Z]+)[0-9]+)*`, where a number indicates a match with the reference, a letter indicates the reference base at a mismatch, and a deletion is specified as a carat followed by one or more letters indicating the aligned reference sequence. This is how Bowtie2 produces the MD field. Insertions are not represented in the MD tag.\n",
    "- all position variables and output should be zero-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pbars = True # whether to draw tqdm progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be the column names ofthe final output, the temporary outputs of call_variants must match this order\n",
    "output_columns = 'chrom pos ref alt frag_start frag_len frag_umi f_read1_sup f_read1_cov r_read1_sup r_read1_cov f_sup f_cov r_sup r_cov f_read1_conc r_read1_conc mq bq end_mismatch_rate'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls variants for a single contig\n",
    "def call_variants(input_filepath, contig, total_reads, contig_len, output_filepath, process_num, lock):\n",
    "    aln = pysam.AlignmentFile(input_filepath, 'rb')\n",
    "    args.chunk_size = 10000\n",
    "\n",
    "    # key is (pos (0 based), ref, alt, frag_start, frag_length, umi)\n",
    "    variants = defaultdict(lambda: {'f_read1_sup': 0, # supporting reads in original + strand\n",
    "                                    'r_read1_sup': 0, # supporting reads in original - strand\n",
    "                                    'f_sup': 0, # supporting reads on + strand\n",
    "                                    'r_sup': 0, # supporting reads on - strand\n",
    "                                    'f_read1_conc': 0, # number of read pairs on original + strand where both support\n",
    "                                    'r_read1_conc': 0, # number of read pairs on original - strand where both support\n",
    "                                    'mq': '', # mapping qualities of supporting reads\n",
    "                                    'bq': '', # base qualities of supporting reads\n",
    "                                    'f_read1_names': set(), # names of supporting reads on original + strand\n",
    "                                    'r_read1_names': set(), # names of supporting reads on original - strand\n",
    "                                    'start_mismatch_rates': [], # maximal mismatch rates from variant toward the fragment start\n",
    "                                    'end_mismatch_rates': [], # maximal mismatch rates from variant toward the fragment end\n",
    "                                    'start_coverage': 0, # total bases from variant to read start\n",
    "                                    'end_coverage': 0}) # total bases from variant to read end\n",
    "    frag_coverage = dict() # key is (frag_start, frag_length, umi, category) where category is f_read1, r_read1, f, or r; value is array of read coverage over the fragment\n",
    "    # chrom_coverage = np.zeros(contig_len, dtype=np.short) # read coverage of chromosome (all fragments)\n",
    "    num_read = 0\n",
    "    cur_pos = -1\n",
    "    ready_to_flush = False\n",
    "    output_file = open(output_filepath, 'w') # temporary output file\n",
    "    \n",
    "    # initialize progress bar\n",
    "    if draw_pbars:\n",
    "        with lock:\n",
    "            pbar = tqdm(total=total_reads, desc=contig, position=process_num, leave=True, mininterval=10)\n",
    "    \n",
    "    for read in aln.fetch(contig=contig):\n",
    "        num_read += 1\n",
    "        \n",
    "        # skip pairs that don't align as expected\n",
    "        if read.is_unmapped or read.mate_is_unmapped or not (read.flag & 2):\n",
    "            continue\n",
    "\n",
    "        # determine the strand of the fragment based on the alignment of read1\n",
    "        if read.is_read1:\n",
    "            frag_strand = 'r_read1' if read.is_reverse else 'f_read1'\n",
    "        else:\n",
    "            frag_strand = 'f_read1' if read.is_reverse else 'r_read1'\n",
    "            \n",
    "        # determine the fragment start and length\n",
    "        frag_start = min(read.reference_start, read.next_reference_start)\n",
    "        frag_len = abs(read.template_length)\n",
    "        \n",
    "        # split md tag into matches (numbers), mismatches (letters), and deletions (carat + letter(s)). non-matches are always separated by a number\n",
    "        mds = re.findall('[0-9]+|[A-Z]|\\^[A-Z]+', read.get_tag('MD'))\n",
    "        \n",
    "        # change reference N mismatches to matches (e.g. ['3', 'N', '3'] -> ['7'])\n",
    "        new_mds = []\n",
    "        was_n = False\n",
    "        for md in mds:\n",
    "            if md[0] == 'N':\n",
    "                new_mds[-1] = str(int(new_mds[-1]) + 1)\n",
    "                was_n = True\n",
    "            elif was_n:\n",
    "                new_mds[-1]  = str(int(new_mds[-1]) + int(md))\n",
    "                was_n = False\n",
    "            else:\n",
    "                new_mds.append(md)\n",
    "        mds = new_mds\n",
    "        \n",
    "        # change deletions of lenth >1 into multiple 1bp deletions (e.g. ['^AGC'] -> ['^A', '^G', '^C'])\n",
    "        # new_mds = []\n",
    "        # for md in mds:\n",
    "        #     if md[0] == '^':\n",
    "        #         for i in range(1, len(md)):\n",
    "        #             new_mds.append('^' + md[i])\n",
    "        #     else:\n",
    "        #         new_mds.append(md)\n",
    "        # mds = new_mds\n",
    "        \n",
    "        cur_cig = 0 # keep track of the sum of cigar element lengths\n",
    "        # find the insertions in the cigar, add them to mds as \"*x\", where x is the length of the insertion\n",
    "        for cig in read.cigar:\n",
    "            if cig[0] == 1: # if insertion\n",
    "                cur_md = 0 # keep track of sum of md element lengths\n",
    "                for i, md in enumerate(mds): # find md entry which the insertion falls within\n",
    "                    if md.isnumeric(): # if match\n",
    "                        cur_md += int(md)\n",
    "                        if cur_cig <= cur_md: # if the insertion falls within this md element\n",
    "                            first = int(md) - (cur_md - cur_cig)\n",
    "                            second = int(md) - first\n",
    "                            insert = []\n",
    "                            if first != '0':\n",
    "                                insert.append(str(first))\n",
    "                            insert.append('*' + str(cig[1]))\n",
    "                            if second != '0':\n",
    "                                insert.append(str(second))\n",
    "                            mds = mds[:i] + insert + mds[i+1:]\n",
    "                            break\n",
    "                    elif md[0] == '*': # if insertion\n",
    "                        cur_md += int(md[1:])\n",
    "                    elif md[0] == '^': # if deletion\n",
    "                        cur_md += len(md) - 1\n",
    "                    else: # if mismatch\n",
    "                        cur_md += 1\n",
    "                        \n",
    "            elif cig[0] != 0 and cig[0] != 2: # if not aligned nor deletion\n",
    "                with lock:\n",
    "                    sys.stderr.write('WARNING: treating unexpected cigar char \"{}\" as match in read:\\n{}\\n'.format(cig[0], read))\n",
    "            cur_cig += cig[1]\n",
    "        \n",
    "        # remove spacer zeros (e.g. ['A', '0', 'T'] -> ['A', 'T'])\n",
    "        mds = [md for md in mds if md != '0']\n",
    "                \n",
    "        # example of how mds should now look: ['A', '3', 'C', 'G', '^TA', '9', '*2', '2']\n",
    "        \n",
    "        total_mismatches = 0\n",
    "        for md in mds:\n",
    "            if md.isnumeric():\n",
    "                continue\n",
    "            elif md[0] == '*':\n",
    "                total_mismatches += int(md[1:])\n",
    "            elif md[0] == '^':\n",
    "                total_mismatches += len(md) - 1\n",
    "            else:\n",
    "                total_mismatches += 1\n",
    "        \n",
    "        # get the UMI tag\n",
    "        if args.umi is None:\n",
    "            umi = ''\n",
    "        else:\n",
    "            umi = read.get_tag(args.umi) \n",
    "        \n",
    "        # get the worst variant to fragment edge mismatch rate for each md\n",
    "        edge_mm_rate = np.zeros(len(mds))\n",
    "        dist = 0\n",
    "        past_mismatches = 0\n",
    "        if read.is_reverse: # for reverse reads, only consider the position to the fragment end\n",
    "            for i, md in enumerate(mds):\n",
    "                if md.isnumeric():\n",
    "                    dist += int(md)\n",
    "                    continue\n",
    "\n",
    "                r = (total_mismatches - past_mismatches) / (read.query_length - dist)\n",
    "                if edge_mm_rate[i] < r:\n",
    "                        edge_mm_rate[i:] = r\n",
    "\n",
    "                if md[0] == '^':\n",
    "                    past_mismatches += len(md) - 1\n",
    "                elif md[0] == '*':\n",
    "                    past_mismatches += int(md[1:])\n",
    "                    dist += int(md[1:])\n",
    "                else:\n",
    "                    past_mismatches += 1\n",
    "                    dist += 1\n",
    "        else: # for forward reads, only consider the position to the fragment start\n",
    "            for i in range(len(mds) - 1, -1, -1):\n",
    "                md = mds[i]\n",
    "                if md.isnumeric():\n",
    "                    dist += int(md)\n",
    "                    continue\n",
    "\n",
    "                r = (total_mismatches - past_mismatches) / (read.query_length - dist)\n",
    "                if edge_mm_rate[i] < r:\n",
    "                        edge_mm_rate[:i + 1] = r\n",
    "\n",
    "                if md[0] == '^':\n",
    "                    past_mismatches += len(md) - 1\n",
    "                elif md[0] == '*':\n",
    "                    past_mismatches += int(md[1:])\n",
    "                    dist += int(md[1:])\n",
    "                else:\n",
    "                    past_mismatches += 1\n",
    "                    dist += 1\n",
    "            \n",
    "        \n",
    "        # find and add all variants using md\n",
    "        cur_md = 0 # current position in the read\n",
    "        total_insertion_len = 0\n",
    "        for i, md in enumerate(mds):\n",
    "            if md.isnumeric(): # skip over matches\n",
    "                cur_md += int(md)\n",
    "            else:\n",
    "                if md[0] == '^': # if deletion\n",
    "                    key_pos = read.reference_start + cur_md - total_insertion_len\n",
    "                    key_ref = md[1:]\n",
    "                    key_alt = '*'\n",
    "                    qualities = '' # no base call qualities to add\n",
    "                    # don't increment cur_md, as deletions don't count towards the read length\n",
    "                    total_insertion_len -= len(md) - 1\n",
    "                elif md[0] == '*': # if insertion\n",
    "                    key_pos = read.reference_start + cur_md - total_insertion_len\n",
    "                    key_ref = '*'\n",
    "                    key_alt = read.query_sequence[cur_md:cur_md + int(md[1:])]\n",
    "                    qualities = ''\n",
    "                    for qual in read.query_qualities[cur_md:cur_md+int(md[1:])]:\n",
    "                        qualities += chr(qual + 33)\n",
    "                    cur_md += int(md[1:])\n",
    "                    total_insertion_len += int(md[1:])\n",
    "                else: # if mismatch\n",
    "                    key_pos = read.reference_start + cur_md - total_insertion_len\n",
    "                    key_ref = md\n",
    "                    key_alt = read.query_sequence[cur_md]\n",
    "                    qualities = chr(read.query_qualities[cur_md] + 33)\n",
    "                    cur_md += 1\n",
    "                \n",
    "                key = (key_pos, key_ref, key_alt,\n",
    "                       frag_start, frag_len, \n",
    "                       umi)\n",
    "                    \n",
    "                # add variants to dictionary\n",
    "                if frag_strand == 'f_read1':\n",
    "                    if read.query_name in variants[key]['f_read1_names']: # if we already found this variant in the mate\n",
    "                        variants[key]['f_read1_conc'] += 1\n",
    "                    else:\n",
    "                        variants[key]['f_read1_names'].add(read.query_name)\n",
    "                    variants[key]['f_read1_sup'] += 1\n",
    "                else:\n",
    "                    if read.query_name in variants[key]['r_read1_names']:\n",
    "                        variants[key]['r_read1_conc'] += 1\n",
    "                    else:\n",
    "                        variants[key]['r_read1_names'].add(read.query_name)\n",
    "                    variants[key]['r_read1_sup'] += 1\n",
    "                if read.is_reverse:\n",
    "                    variants[key]['r_sup'] += 1\n",
    "                    variants[key]['end_mismatch_rates'].append(edge_mm_rate[i])\n",
    "                else:\n",
    "                    variants[key]['f_sup'] += 1\n",
    "                    variants[key]['start_mismatch_rates'].append(edge_mm_rate[i])\n",
    "                variants[key]['mq'] += chr(read.mapping_quality + 33)\n",
    "                variants[key]['bq'] += qualities\n",
    "                \n",
    "        \n",
    "        # add coverage of fragment to frag_coverage\n",
    "        frag = (frag_start, frag_len, umi)\n",
    "        if frag not in frag_coverage: # if not in dictionary yet, add it\n",
    "            frag_coverage[frag] = {'f_read1':np.zeros(frag[1], dtype=np.short), \n",
    "                                   'r_read1':np.zeros(frag[1], dtype=np.short), \n",
    "                                   'f':np.zeros(frag[1], dtype=np.short), \n",
    "                                   'r':np.zeros(frag[1], dtype=np.short)}\n",
    "        if read.is_reverse: # FIXME there may be errors here if forward read goes past the end of the reverse read (haven't seen that yet in concordant bowtie2 reads)\n",
    "            frag_coverage[frag]['r'][frag_len - read.reference_length:] += 1\n",
    "            frag_coverage[frag][frag_strand][frag_len - read.reference_length:] += 1\n",
    "        else:\n",
    "            frag_coverage[frag]['f'][:read.reference_length] += 1\n",
    "            frag_coverage[frag][frag_strand][:read.reference_length] += 1\n",
    "        \n",
    "        # periodically flush variants and coverage info out of the dictionaries\n",
    "        if num_read % args.chunk_size == 0:\n",
    "            if draw_pbars:\n",
    "                with lock:\n",
    "                    pbar.update(args.chunk_size)\n",
    "            ready_to_flush = True\n",
    "        \n",
    "        if (ready_to_flush and read.reference_start > cur_pos) or num_read == total_reads:\n",
    "            cur_pos = read.reference_start\n",
    "            \n",
    "            # flush variants\n",
    "            to_flush = []\n",
    "            for var in variants: \n",
    "                if var[0] >= cur_pos and num_read < total_reads: # don't yet flush variants at or past the current read start\n",
    "                    continue\n",
    "                to_flush.append(var)\n",
    "            to_flush.sort() # this should ensure the output is sorted by position-ref-alt\n",
    "            for var in to_flush:\n",
    "                frag = (var[3], var[4], var[5]) # (frag start, frag end, umi)\n",
    "                pos_in_frag = var[0] - var[3] # variant position - frag start\n",
    "                f_read1_cov = frag_coverage[frag]['f_read1'][pos_in_frag]\n",
    "                r_read1_cov = frag_coverage[frag]['r_read1'][pos_in_frag]\n",
    "                f_cov = frag_coverage[frag]['f'][pos_in_frag]\n",
    "                r_cov = frag_coverage[frag]['r'][pos_in_frag]\n",
    "                s_rates = variants[var]['start_mismatch_rates']\n",
    "                e_rates = variants[var]['end_mismatch_rates']\n",
    "                avg_start_mm_rate = sum(s_rates) / len(s_rates) if len(s_rates) > 0 else 0\n",
    "                avg_end_mm_rate = sum(e_rates) / len(e_rates) if len(e_rates) > 0 else 0\n",
    "                max_mr = '%.3f' % max(avg_start_mm_rate, avg_end_mm_rate)\n",
    "                \n",
    "                v = variants[var]\n",
    "                if args.all or (v['f_read1_sup'] > 0 and v['r_read1_sup'] > 0): # only output if support in both strands\n",
    "                    output_vals = [contig] + list(var) + [v['f_read1_sup'], f_read1_cov, v['r_read1_sup'], r_read1_cov, v['f_sup'], f_cov, v['r_sup'], r_cov, \\\n",
    "                                                          v['f_read1_conc'], v['r_read1_conc'], v['mq'], v['bq'], max_mr]\n",
    "                    output_file.write('\\t'.join(map(str, output_vals)) + '\\n')\n",
    "                del variants[var]\n",
    "            \n",
    "            # flush coverage dictionaries\n",
    "            to_flush = []\n",
    "            for frag in frag_coverage:\n",
    "                if frag[0] + frag[1] < cur_pos: # if frag_start + frag_len is before the current read, we won't need it any more\n",
    "                    to_flush.append(frag)\n",
    "                else:\n",
    "                    break # since frag_coverage is sorted by start position, don't bother checking the remaining fragments (though some could be flushed if they're inside a long fragment)\n",
    "            for frag in to_flush:\n",
    "                del frag_coverage[frag]\n",
    "            \n",
    "            ready_to_flush = False\n",
    "    \n",
    "    pbar.close()\n",
    "    output_file.close()\n",
    "    tqdm.write(f'finished calling {contig}\\n', file=sys.stderr)\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    chroms = [x.contig for x in aln.get_index_statistics()]\n",
    "    chroms.sort()\n",
    "        \n",
    "    # start worker processes from pool\n",
    "    sys.stderr.write('Iterating over chromosomes\\n')\n",
    "    tmp_files = []\n",
    "    tmp_base = args.tmp + 'tmp_' + args.output.split('/')[-1]\n",
    "    with multiprocessing.Pool(processes=max(1, args.threads - 1)) as pool:\n",
    "        lock = multiprocessing.Manager().Lock()\n",
    "        processes = []\n",
    "        stats = sorted(aln.get_index_statistics(), key=lambda x: x.contig) # sort contigs by name, this is needed for duplex_caller_output to simultaneously parse multiple variant tsvs\n",
    "        for i, x in enumerate(stats): # for each chromosome, add a process\n",
    "            tmp_files.append('{}_{}.tsv'.format(tmp_base, x.contig))\n",
    "            pool_args = (args.input, x.contig, x.total, aln.get_reference_length(x.contig), tmp_files[-1], i, lock)\n",
    "            processes.append(pool.apply_async(call_variants, pool_args))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    sys.stderr.flush()\n",
    "    time.sleep(2)\n",
    "    sys.stderr.write('Checking for errors during runtime\\n')\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and delete temporary files\n",
    "sys.stderr.write('Merging temporary output files\\n')\n",
    "out_header = '\\t'.join(output_columns)\n",
    "if args.output:\n",
    "    with open(args.output, 'w') as out_file:\n",
    "        out_file.write(out_header + '\\n')\n",
    "        for tmp in tmp_files:\n",
    "            with open(tmp, 'r') as f:\n",
    "                for l in f:\n",
    "                    out_file.write(l)\n",
    "else:\n",
    "    sys.stdout.write(out_header)\n",
    "    for tmp in tmp_files:\n",
    "        with open(tmp, 'r') as f:\n",
    "            for l in f:\n",
    "                sys.stdout.write(l) # FIXME this may give a broken pipe error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Deleting temporary files\\n')\n",
    "for tmp in tmp_files:\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Complete\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
