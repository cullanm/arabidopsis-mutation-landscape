{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pysam\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import time\n",
    "import gtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Groups read pairs by their UMI and template start and end positions. Adds an integer UMI id tag (UG) and corrected UMI sequence (RX)\n",
    "tag. The UG tag is unique within the contig. Corrects for sequencing errors by grouping UMIs within n edit distance (hamming distance) and selecting the UMI with the \n",
    "greatest count as the corrected UMI. Grouping works by selecting the most common UMI for a given template start and end position, \n",
    "grouping all UMIs within n edits from that UMI, then repeating until no UMIs are left. Intended for use with xGen CS adapters. Will discard\n",
    "any reads not part of a properly aligned pair (flag 2). The output should match that of UMI-tools group but with a few differences: |n\n",
    "- a different corrected UMI tag (RX)|n\n",
    "- the ability to group F1R2 and F2R1 reads when UMIs are on each side of the template|n\n",
    "- marking of both r1 and r2 with the UG and RX tag|n\n",
    "- can optionally group reads with slightly different template start and end positions\n",
    "    ''', formatter_class=gtools.EscapeFormatter)\n",
    "parser.add_argument(dest='fin', metavar='INPUT_BAM', type=str,\n",
    "                   help='input BAM file, must be coordinate sorted')\n",
    "parser.add_argument(dest='fout', metavar='OUTPUT_BAM', type=str, \n",
    "                   help='output BAM file, will remain coordinate sorted')\n",
    "parser.add_argument('-s', '--separator', dest='sep', metavar='STR', type=str, default='_',\n",
    "                   help='separator between the read id and the UMI (default: _)')\n",
    "parser.add_argument('-f', '--no_flip', dest='flip', action='store_false',\n",
    "                   help='by default, UMIs of F2R1 reads are flipped from ABCDEF to DEFABC. Setting this flag negates this behavior')\n",
    "parser.add_argument('-a', '--append', dest='append_umi', metavar='STR', type=str, default='',\n",
    "                   help='append an arbitrary string to the end of all RX tags, useful when the output will later be \\\n",
    "                   merged with other files which could have UMI conflicts')\n",
    "parser.add_argument('-e', '--edit_distance', dest='edit_distance', metavar='INT', type=int, default=0,\n",
    "                   help='max edit (hamming) distance to use when clustering UMIs (default: 0)')\n",
    "parser.add_argument('-b', '--bp_distance', dest='bp_distance', metavar='INT', type=int, default=0,\n",
    "                   help='max bp distance the template start and end can differ by when clustering UMIs (default: 0)')\n",
    "parser.add_argument('-t', '--tmp', dest='tmp', metavar='PREFIX', type=str, default='./',\n",
    "                   help='prefix to add to temporary files (default: ./)')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='INT', type=int, default=1,\n",
    "                   help='number of processors to use, can not utilize more than num contigs + 1 (default=1)')\n",
    "parser.add_argument('--buffer_size', dest='buffer_size', metavar='INT', type=int, default=20000,\n",
    "                   help='number of reads kept in memory before flushing to output, adjusting may improve speed (default: 20000)')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    # args = parser.parse_args('tests/test_duplex_caller/duplex_caller_test_case.bam tests/test_duplex_caller/out.tsv -@ 1 --all -t tmp/'.split()) # used for testing\n",
    "    if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "        os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "    args = parser.parse_args('-@ 8 -e 0 --append _0 data/align/chandler_KVKCS001D_0_sort.bam data/align/chandler_KVKCS001D_0_umigroup.bam'.split()) # used for testing\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "if args.fout and '/' in args.fout:\n",
    "    os.makedirs(os.path.dirname(args.fout), exist_ok=True)\n",
    "\n",
    "sys.stderr.write(f'Running group_umis.py with arguments {args}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the input if it hasn't been already\n",
    "try:\n",
    "    aln = pysam.AlignmentFile(args.fin, 'rb')\n",
    "    aln.check_index()\n",
    "except ValueError:\n",
    "    sys.stderr.write('No index found for input file, running samtools index\\n')\n",
    "    proc = subprocess.run(f'samtools index -@ {args.threads} {args.fin}', shell=True, capture_output=True)\n",
    "    if proc.returncode != 0:\n",
    "        sys.stderr.write('ERROR: failed to run samtools index on input\\n')\n",
    "        sys.stderr.write(proc.stderr)\n",
    "        sys.exit(1)\n",
    "    \n",
    "# confirm that file is coordinate sorted and has a UMI        \n",
    "aln = pysam.AlignmentFile(args.fin, 'rb')\n",
    "last = ('', 0)\n",
    "for i, read in enumerate(aln.fetch()):\n",
    "    nex = (read.reference_name, read.reference_start)\n",
    "    if nex < last:\n",
    "        sys.stderr.write('ERROR: SAM/BAM file is not coordinate sorted\\n')\n",
    "        exit()\n",
    "    if len(read.query_name.rsplit(args.sep, 1)) < 2:\n",
    "        sys.stderr.write(f'ERROR: no separator \"{args.sep}\" present in read name\\n')\n",
    "    if not set(read.query_name.rsplit(args.sep, 1)[1]).issubset({'A', 'C', 'G', 'T', 'N'}):\n",
    "        sys.stderr.write(f'WARNING: characters after separator \"{args.sep}\" do not look like a UMI: {read.query_name.rsplit(args.sep, 1)[1]}\\n')\n",
    "    last = nex\n",
    "    if i > 1000:\n",
    "        break\n",
    "aln.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates number of edits needed to convert one UMI to another\n",
    "def hamming_dist(seq1, seq2):\n",
    "    dist = 0\n",
    "    for i in range(len(seq1)):\n",
    "        if seq1[i] != seq2[i]:\n",
    "            dist += 1\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_umis(fin, fout, chrom, total):\n",
    "    aln_in = pysam.AlignmentFile(fin, 'rb')\n",
    "    aln_out = pysam.AlignmentFile(fout, 'wb', template=aln_in)\n",
    "    read_iter = aln_in.fetch(contig=chrom)\n",
    "    \n",
    "    cur_umi_counts = defaultdict(lambda: defaultdict(lambda: 0)) # cur_umi_counts[fragment end][raw umi] = number of forward reads with the umi\n",
    "    cur_reads = [] # list of (fragment end, umi, read) tuples\n",
    "    name_to_umi = dict() # key: read name, value: (umi id, corrected umi)\n",
    "    cur_id = 0\n",
    "    \n",
    "    pbar = tqdm(miniters=50000, position=0, total=total, desc=chrom, maxinterval=9999)\n",
    "    read = next(read_iter, None) # get the first read\n",
    "    pbar.update()\n",
    "    while not read.is_proper_pair: # skip all reads that aren't proper pairs (flag 2)\n",
    "            read = next(read_iter, None)\n",
    "            pbar.update()\n",
    "    cur_pos = read.reference_start if read is not None else None\n",
    "    while read is not None:\n",
    "        # print(read.query_name, read.is_reverse)\n",
    "        # break\n",
    "        umi = read.query_name.rsplit(args.sep, 1)[1]\n",
    "        if args.flip and ((not read.is_reverse and read.is_read2) or (read.is_reverse and read.is_read1)): # flip F2R1 UMIs from ABCDEF to DEFABC\n",
    "            # print('flipping')\n",
    "            umi = umi[len(umi) // 2:len(umi)] + umi[:len(umi) // 2]\n",
    "        tlen = abs(read.template_length)\n",
    "        cur_reads.append((tlen, umi, read))\n",
    "        \n",
    "        if not read.is_reverse:\n",
    "            cur_umi_counts[abs(read.template_length)][umi] += 1\n",
    "            \n",
    "        # if read.query_name == 'lh00134:636:22M5YKLT4:7:2231:8378:14340_TGGTTA':\n",
    "        #     if read.is_reverse:\n",
    "        #         print('found reverse read')\n",
    "        #     else:\n",
    "        #         print('found forward read')\n",
    "        #     # print(read.is_reverse)\n",
    "        #     # print(read.reference_start, read.next_reference_start)\n",
    "        #     print(dict(cur_umi_counts))\n",
    "            \n",
    "        read = next(read_iter, None) # get the next read\n",
    "        pbar.update()\n",
    "        while (read is not None) and (not read.is_proper_pair): # skip all reads that aren't proper pairs (flag 2)\n",
    "            read = next(read_iter, None)\n",
    "            pbar.update()\n",
    "        \n",
    "        # if read.query_name == 'lh00134:636:22M5YKLT4:7:2231:8378:14340_TGGTTA':\n",
    "        #     print(f'proper pair? {read.is_proper_pair}')\n",
    "        #     if read.is_reverse:\n",
    "        #         print('found reverse read')\n",
    "        #     else:\n",
    "        #         print('found forward read')\n",
    "        \n",
    "        # if we advanced to a new start position, flush the reads of the previous one\n",
    "        if read is None or read.reference_start > cur_pos:\n",
    "            # print(dict(cur_umi_counts))\n",
    "            umi_corrector = defaultdict(lambda: dict())\n",
    "            for tlen in cur_umi_counts:\n",
    "                sorted_umis = sorted(list(cur_umi_counts[tlen].items()), key=lambda x: x[1], reverse=True)\n",
    "                # if len(sorted_umis) > 0:\n",
    "                #     print(f'correcting umis for {cur_pos}+{tlen}: {sorted_umis}')\n",
    "                for i in range(len(sorted_umis)):\n",
    "                    if sorted_umis[i][0] not in umi_corrector[tlen]:\n",
    "                        umi_corrector[tlen][sorted_umis[i][0]] = sorted_umis[i][0]\n",
    "                        # print(f'umi {sorted_umis[i][0]} is correct')\n",
    "                    for j in range(i + 1, len(sorted_umis)):\n",
    "                        if sorted_umis[j][0] in umi_corrector[tlen]:\n",
    "                            continue\n",
    "                        if hamming_dist(sorted_umis[i][0], sorted_umis[j][0]) <= args.edit_distance:\n",
    "                            # print(f'correcting {sorted_umis[j][0]} into {sorted_umis[i][0]}')\n",
    "                            umi_corrector[tlen][sorted_umis[j][0]] = sorted_umis[i][0]\n",
    "            # print(f'corrected umis: {dict(umi_corrector)}')\n",
    "            umi_ids = defaultdict(lambda: dict())\n",
    "            for tlen, umi, r in cur_reads:\n",
    "                if r.is_reverse:\n",
    "                    continue\n",
    "                # print(tlen, umi)\n",
    "                cor_umi = umi_corrector[tlen][umi]\n",
    "                if cor_umi not in umi_ids[tlen]:\n",
    "                    umi_ids[tlen][cor_umi] = cur_id\n",
    "                    cur_id += 1\n",
    "                name_to_umi[r.query_name] = (umi_ids[tlen][cor_umi], cor_umi)\n",
    "            \n",
    "            for tlen, umi, r in cur_reads:\n",
    "                if not r.is_reverse:\n",
    "                    cor_umi = umi_corrector[tlen][umi]\n",
    "                    r.set_tag('UG', umi_ids[tlen][cor_umi])\n",
    "                    r.set_tag('RX', cor_umi + args.append_umi)\n",
    "                    # print(f'assigned read {min(r.reference_start, r.next_reference_start)}-{abs(r.tlen)}-{umi} a UG of {umi_ids[tlen][cor_umi]}')\n",
    "                    aln_out.write(r)\n",
    "                else:\n",
    "                    umi_id, cor_umi = name_to_umi[r.query_name]\n",
    "                    del name_to_umi[r.query_name]\n",
    "                    r.set_tag('UG', umi_id)\n",
    "                    r.set_tag('RX', cor_umi + args.append_umi)\n",
    "                    # print(f'assigned read {min(r.reference_start, r.next_reference_start)}-{abs(r.tlen)}-{umi} a UG of {umi_id}')\n",
    "                    aln_out.write(r)\n",
    "                \n",
    "            cur_umi_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "            cur_reads = []\n",
    "            cur_pos = read.reference_start if read is not None else -1\n",
    "            # if cur_pos > 2000:\n",
    "            #     aln_in.close()\n",
    "            #     aln_out.close()\n",
    "            #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD groups umis for templates which share the same start and end position for one chromosome\n",
    "# def group_umis(fin, fout, chrom, total): # total is the expected number of reads for drawing the pbar\n",
    "#     aln_in = pysam.AlignmentFile(fin, 'rb')\n",
    "#     aln_out = pysam.AlignmentFile(fout, 'wb', template=aln_in)\n",
    "#     read_buf = [] # list of pysam read objects in order they were read\n",
    "#     umi_buf = dict() # indexed as umi_buf[(tstart, tlen)][umi] = [read names], dict-dict-list\n",
    "#     true_umi = dict() # key is read name, value is (corrected_umi, umi_id)\n",
    "#     cur_id = 1\n",
    "#     num_read = 0\n",
    "\n",
    "#     read_iter = aln_in.fetch(contig=chrom)\n",
    "#     try:\n",
    "#         read = next(read_iter) # get the first read\n",
    "#     except StopIteration:\n",
    "#         sys.stderr.write(f'WARNING: no reads on contig {chrom}\\n')\n",
    "#         aln_in.close()\n",
    "#         aln_out.close()\n",
    "#         return\n",
    "#     pbar = tqdm(miniters=50000, position=0, total=total, desc=chrom)\n",
    "#     while True:\n",
    "#     #     print(f'processing read {read.query_name}')\n",
    "#         # extract the UMI\n",
    "#         umi = read.query_name.rsplit(args.sep, 1)[1]\n",
    "#         if args.flip and ((read.is_read1 and read.is_reverse) or (not read.is_read1 and not read.is_reverse)): # flip F2R1 UMIs from ABCDEF to DEFABC\n",
    "#             umi = umi[len(umi) // 2:len(umi)] + umi[:len(umi) // 2]\n",
    "#     #         read.query_name = read.query_name[:len(read.query_name) - len(umi)] + umi # FIXME I don't think I need this, but I could add it\n",
    "\n",
    "#         # add umi and read name to umi_buf\n",
    "#         coord = (min(read.reference_start, read.next_reference_start), abs(read.template_length))\n",
    "#         if coord not in umi_buf:\n",
    "#             umi_buf[coord] = dict()\n",
    "#         if umi not in umi_buf[coord]:\n",
    "#             umi_buf[coord][umi] = []\n",
    "#         umi_buf[coord][umi].append(read.query_name)\n",
    "\n",
    "#         read_buf.append(read)\n",
    "\n",
    "#         # get the next properly aligned read, set read to None if at the end of the file\n",
    "#         while True:\n",
    "#             read = next(read_iter, None) \n",
    "#             if read is None or read.flag & 2:\n",
    "#                 break\n",
    "\n",
    "#         # flush reads from the buffer when it gets to large and when the end of the file is reached\n",
    "#         if len(read_buf) > args.buffer_size or read is None:\n",
    "#     #         print(read_buf)\n",
    "#     #         print(umi_buf)\n",
    "#     #         print(true_umi)\n",
    "#             for coord in umi_buf:\n",
    "#                 # skip if we aren't past the template end yet, this is an overestimate. Don't skip if at end of file\n",
    "#                 if read is not None and coord[0] + coord[1] + 3 > read.reference_start:\n",
    "#                     continue\n",
    "#     #             print(umi_buf[frag])\n",
    "#     #             print(f'processing umis at coords {coord}')\n",
    "#                 # sort from umi with highest counts to lowest\n",
    "#                 umis = sorted(list(umi_buf[coord].items()), key=lambda x: len(x[1]), reverse=True) # this is a list of (umi, [read_names]) tuples\n",
    "#     #             print(umis)\n",
    "#                 while len(umis) > 0:\n",
    "#                     mode_umi = umis[0][0]\n",
    "#     #                 print(f'selected {mode_umi} as mode_umi')\n",
    "#                     new_umis = []\n",
    "#                     for i, umi in enumerate(umis):\n",
    "#                         if hamming_dist(mode_umi, umi[0]) <= args.edit_distance:\n",
    "#     #                         print(f'merging {umi[0]} with mode_umi')\n",
    "#                             true_umi.update({x:(mode_umi, cur_id) for x in umi[1]})\n",
    "#                         else:\n",
    "#                             new_umis.append(umi)\n",
    "#                     umis = new_umis\n",
    "#                     cur_id += 1\n",
    "#     #         print(true_umi)\n",
    "\n",
    "#             num_written = 0\n",
    "#             for r in read_buf:\n",
    "#                 if r.query_name in true_umi:\n",
    "#     #                 print(f'writing {r.query_name}')\n",
    "#                     r.set_tag('UG', true_umi[r.query_name][1]) # add tag for UMI group ID\n",
    "#                     r.set_tag('RX', true_umi[r.query_name][0] + args.append_umi) # add tag for corrected UMI\n",
    "#                     aln_out.write(r)\n",
    "#                     num_written += 1\n",
    "#                     if r.is_reverse:\n",
    "#                         del true_umi[r.query_name]\n",
    "#                 else:\n",
    "#                     break\n",
    "#             umi_buf = {x:y for (x, y) in umi_buf.items() if x[0] >= r.reference_start}\n",
    "#             read_buf = read_buf[num_written:]\n",
    "#     #         print(f'wrote {num_written} reads, {len(read_buf)} remain. umi_buf size of {len(umi_buf)}, true_umi size of {len(true_umi)}')\n",
    "#             pbar.update(num_written)\n",
    "#     #         if num_written == 0:\n",
    "#     #             sys.stderr.out('ERROR: exceeded read buffer size, this could be a bug or --buffer_size may need to be larger')\n",
    "#     #             sys.exit(1)\n",
    "\n",
    "#         # break if at the end of the file\n",
    "#         if read is None:\n",
    "#             del umi_buf\n",
    "#             break\n",
    "\n",
    "#     aln_in.close()\n",
    "#     aln_out.close()\n",
    "#     sys.stderr.write(f'Done grouping {chrom}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group UMIs which vary by a few bp in start or end position, OLD and probably really slow\n",
    "def group_neighbors(fin, fout, chrom, total):\n",
    "    aln_in = pysam.AlignmentFile(fin, 'rb')\n",
    "    aln_out = pysam.AlignmentFile(fout, 'wb', template=aln_in)\n",
    "    read_buf = [] # list of pysam read objects in order they were read\n",
    "    cur_pos = -1\n",
    "    group_buf = dict() # indexed as umi_buf[(tstart, tlen)] = {(umi, umi_id)} dict-set\n",
    "    new_id = dict() # used to convert umis being grouped to the new one key:umi_id value:umi_id to group the key into\n",
    "    id_to_umi = dict()\n",
    "    # merged_umis = dict() # inverse of new_umi, finds the UMIs being grouped into a umi key:umi_id value:(true_umi, umi_id being grouped into the key)\n",
    "    group_counts = defaultdict(lambda: 0) # key:umi_id value:number of reads with that id \n",
    "\n",
    "    read_iter = aln_in.fetch(until_eof=True)\n",
    "    try:\n",
    "        read = next(read_iter) # get the first read\n",
    "    except StopIteration:\n",
    "        sys.stderr.write(f'WARNING: no reads on contig {chrom}\\n')\n",
    "        aln_in.close()\n",
    "        aln_out.close()\n",
    "        return\n",
    "    pbar = tqdm(miniters=20000, position=0, total=total, desc=chrom)\n",
    "    while True:\n",
    "    #     print(f'processing read {read.query_name}')\n",
    "        # extract the UMI\n",
    "\n",
    "        # add umi and id to buffer\n",
    "        coord = (min(read.reference_start, read.next_reference_start), abs(read.template_length))\n",
    "        if coord not in group_buf:\n",
    "            group_buf[coord] = set()\n",
    "        group_buf[(min(read.reference_start, read.next_reference_start), abs(read.template_length))].add((read.get_tag('RX'), read.get_tag('UG')))\n",
    "        group_counts[read.get_tag('UG')] += 1\n",
    "        id_to_umi[read.get_tag('UG')] = read.get_tag('RX')\n",
    "\n",
    "        read_buf.append(read)\n",
    "\n",
    "        read = next(read_iter, None) # get the next read, set read to None if at the end of the file\n",
    "\n",
    "        # flush reads from the buffer when it gets to large and when the end of the file is reached\n",
    "        if len(read_buf) > args.buffer_size or read is None:\n",
    "    #         print(read_buf)\n",
    "    #         print(umi_buf)\n",
    "    #         print(true_umi)\n",
    "            for coord in group_buf: # for each (templat start, template len) in the buffer\n",
    "    #             print(f'joining neighbors around {coord}')\n",
    "                if read is not None and coord[0] + coord[1] + 3 + args.bp_distance > read.reference_start: # skip if we could still find more to group\n",
    "                    continue\n",
    "                for central_umi in group_buf[coord]: # for each (umi, umi_id) at the coord position\n",
    "                    for i in range(coord[0] - args.bp_distance, coord[0] + args.bp_distance + 1): # vary template start\n",
    "                        for j in range(coord[1] - args.bp_distance, coord[1] + args.bp_distance + 1): # vary template len\n",
    "                            if i == coord[0] and j == coord[1]: # ignore identical template coord, these should already be grouped\n",
    "                                continue\n",
    "                            if (i, j) not in group_buf: # ignore template positions with no reads\n",
    "                                continue\n",
    "                            for neighbor_umi in group_buf[(i, j)]: # for each (umi, umi_id) at the varied template position\n",
    "                                if hamming_dist(central_umi[0], neighbor_umi[0]) <= args.edit_distance: # if the umis are similar, group\n",
    "    #                                 print(f'grouping {central_umi} {coord} and {neighbor_umi} {(i, j)}')\n",
    "                                    # group the umi with fewer reads into the one with more\n",
    "                                    if group_counts[central_umi[1]] > group_counts[neighbor_umi[1]]:\n",
    "                                        new_id[neighbor_umi[1]] = central_umi[1]\n",
    "                                    elif group_counts[central_umi[1]] < group_counts[neighbor_umi[1]]:\n",
    "                                        new_id[central_umi[1]] = neighbor_umi[1]\n",
    "                                    else:\n",
    "                                        new_id[min(central_umi[1], neighbor_umi[1])] = max(central_umi[1], neighbor_umi[1])\n",
    "\n",
    "\n",
    "            num_written = 0\n",
    "            for r in read_buf:\n",
    "                # stop writing once we find a read that might still need grouping, don't stop if at end of file\n",
    "                if read is not None and r.reference_start + r.tlen + 3 + args.bp_distance > read.reference_start:\n",
    "                    break\n",
    "    #             print(f'writing {r.query_name}')\n",
    "    #             print(new_id)\n",
    "                new = r.get_tag('UG')\n",
    "                while new in new_id:\n",
    "                    new = new_id[new]\n",
    "                r.set_tag('UG', new) # add tag for UMI group ID\n",
    "                r.set_tag('RX', id_to_umi[new]) # add tag for corrected UMI\n",
    "                aln_out.write(r)\n",
    "                num_written += 1\n",
    "            to_remove = [y for (x, y) in group_buf.items() if x[0] + x[1] + args.bp_distance < r.reference_start]\n",
    "            for s in to_remove:\n",
    "                for u in s:\n",
    "                    del id_to_umi[u[1]]\n",
    "                    new_id.pop(u[1], None)\n",
    "            group_buf = {x:y for (x, y) in group_buf.items() if x[0] + x[1] + args.bp_distance >= r.reference_start}\n",
    "            read_buf = read_buf[num_written:]\n",
    "            pbar.update(num_written)\n",
    "#             print(f'wrote {num_written} reads, {len(read_buf)} remain. group_buf:{len(group_buf)}, id_to_umi:{len(id_to_umi)}, new_id:{len(new_id)}')\n",
    "\n",
    "\n",
    "        # break if at the end of the file\n",
    "        if read is None:\n",
    "            break\n",
    "\n",
    "    aln_in.close()\n",
    "    aln_out.close()\n",
    "    sys.stderr.write(f'Done grouping {chrom}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    aln = pysam.AlignmentFile(args.fin, 'rb')\n",
    "    chroms = {x.contig:x.mapped for x in aln.get_index_statistics()}\n",
    "    chroms = {x:chroms[x] for x in sorted(chroms.keys())}\n",
    "    \n",
    "    # if the tmp file directory doesn't exist, make it\n",
    "    if not os.path.exists(args.tmp.rsplit('/', 1)[0]):\n",
    "        os.makedirs(args.tmp.rsplit('/', 1)[0])\n",
    "    \n",
    "    # get file names for temporary files\n",
    "    tmp_files = {chrom:args.tmp + args.fout.replace('/', '_') + '_' + chrom + '.bam' for chrom in chroms}\n",
    "    sys.stderr.write(f'using temporary files {\", \".join(tmp_files.values())}\\n')\n",
    "    \n",
    "    # start worker processes from pool\n",
    "    sys.stderr.write('Grouping umis with the same template start and end\\n')\n",
    "    if args.threads > 1:\n",
    "        with multiprocessing.Pool(processes=args.threads - 1) as pool:\n",
    "            processes = []\n",
    "            for chrom in chroms: # for each chromosome, add a process\n",
    "                processes.append(pool.apply_async(group_umis, (args.fin, tmp_files[chrom], chrom, chroms[chrom])))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        sys.stderr.flush()\n",
    "        time.sleep(2)\n",
    "        sys.stderr.write('Checking for errors during runtime\\n')\n",
    "        # check for errors\n",
    "        for p in processes:\n",
    "            p.get()\n",
    "    else:\n",
    "        for chrom in chroms:\n",
    "            group_umis(args.fin, tmp_files[chrom], chrom, chroms[chrom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and args.bp_distance > 0:\n",
    "    tmp_nei_files = {chrom:args.tmp + args.fout.rsplit('.', 1)[0] + '_' + chrom + '_nei.bam' for chrom in chroms}\n",
    "    sys.stderr.write(f'using another set of temporary files {\", \".join(tmp_nei_files.values())}\\n')\n",
    "    \n",
    "    # start worker processes from pool\n",
    "    sys.stderr.write(f'Grouping umis off by up to {args.bp_distance}bp in template start and/or end\\n')\n",
    "    if args.threads > 1:\n",
    "        with multiprocessing.Pool(processes=args.threads - 1) as pool:\n",
    "            processes = []\n",
    "            for chrom in chroms: # for each chromosome, add a process\n",
    "                processes.append(pool.apply_async(group_neighbors, (tmp_files[chrom], tmp_nei_files[chrom], chrom, chroms[chrom])))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        sys.stderr.flush()\n",
    "        time.sleep(2)\n",
    "        sys.stderr.write('Checking for errors during runtime\\n')\n",
    "        # check for errors\n",
    "        for p in processes:\n",
    "            p.get()\n",
    "    else:\n",
    "        for chrom in chroms:\n",
    "            group_neighbors(tmp_files[chrom], tmp_nei_files[chrom], chrom, chroms[chrom])\n",
    "    \n",
    "    sys.stderr.write(f'removing initial temporary files\\n')\n",
    "    for file in tmp_files.values():\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    tmp_files = tmp_nei_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Concatenating temporary files into final output\\n')\n",
    "proc = subprocess.run(f'samtools cat -@ {args.threads} -o {args.fout} {\" \".join(tmp_files.values())}', shell=True, capture_output=True)\n",
    "if proc.returncode != 0:\n",
    "    sys.stderr.write('ERROR: failed to run samtools cat on temporary files\\n')\n",
    "    sys.stderr.write(proc.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "# sys.stderr.write('Removing any remaining temporary files\\n')\n",
    "# for file in tmp_files.values():\n",
    "#     try:\n",
    "#         os.remove(file)\n",
    "#     except FileNotFoundError:\n",
    "#         pass\n",
    "\n",
    "sys.stderr.write(f'completed group_umis.py on {args.fin}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
