{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gtools\n",
    "\n",
    "import pysam\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import subprocess\n",
    "from multiprocessing import Process\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Subsamples fragments/reads from a NanoSeq library BAM.\n",
    "    ''')\n",
    "parser.add_argument(dest='input', metavar='FILE', type=str,\n",
    "                   help='input NanoSeq BAM file. Must be coordinate sorted and indexed')\n",
    "parser.add_argument('-o', '--output', required=True, dest='output', metavar='FILE', type=str,\n",
    "                   help='subsampled BAM file')\n",
    "parser.add_argument('-s', '--required_strand_coverage', dest='strand_req', metavar='INT', type=int, default=0,\n",
    "                   help='number of reads in each strand required to output the fragment (default: 0)')\n",
    "parser.add_argument('-t', '--required_total_coverage', dest='total_req', metavar='INT', type=int, default=0,\n",
    "                   help='number of total reads in the fragment required to output (default: 0)')\n",
    "parser.add_argument('-c', '--cap', dest='cap_at_req', action='store_true',\n",
    "                   help='caps the number of reads output for a fragment at the minimum required to pass the requirements \\\n",
    "                   reads to be output are selected at random without replacement (default: output all reads for passing fragments)')\n",
    "parser.add_argument('-r', '--random_frac', dest='random_frac', metavar='FLOAT', type=float, default=1.001,\n",
    "                   help='fraction of fragments passing requirements to output (default: 1 (output all passing fragments))')\n",
    "parser.add_argument('-u', '--umi', dest='umi', metavar='TAG', type=str, default=None,\n",
    "                   help='Consider the UMI in the provided BAM tag when grouping fragments as PCR dups (default: no UMI)')\n",
    "parser.add_argument('--tmp', dest='tmp_prefix', metavar='PREFIX', type=str, default='tmp/',\n",
    "                   help='prefix to add to temporary files. Will make any directories which do not exist (default: tmp/)')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='INT', type=int, default=1,\n",
    "                   help='number of threads. Each chromosome can be processed by a separate thread. (default: 1)')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    print('Determined code is running in Jupyter')\n",
    "    args = parser.parse_args('-@ 8 --umi RG -s 4 -t 10 --cap -o tmp/subsampler_test.bam tmp/smol.bam'.split()) # used for testing\n",
    "    if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "        os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "if args.tmp_prefix and '/' in args.tmp_prefix:\n",
    "    os.makedirs(os.path.dirname(args.tmp_prefix), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 200000 # amount of reads to process before each flush\n",
    "read_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsamples one chromosome of a bam file to only output fragments with at least a certain amount of read coverage\n",
    "# file_in and file_out are bam files\n",
    "# f_req and r_req are the required number of read pairs from the original forward and reverse strands\n",
    "# to output only f_req forward and r_req reverse read pairs, set cap_at_req to True\n",
    "# if cap_at_req is True, reads will be selected at random WITHOUT REPLACEMENT for output\n",
    "def subsample_frags(file_in, file_out, chrom):\n",
    "    fragments = defaultdict(lambda: [0, 0, [], []]) # index as fragments[(frag_start, tlen, umi)] = [f_read1_supp, r_read1_supp, [f read names], [r read names]]\n",
    "    aln_in = pysam.AlignmentFile(file_in, 'rb')\n",
    "    aln_out = pysam.AlignmentFile(file_out, 'wb', template=aln_in)\n",
    "    buffer = [] # stores reads in the order read from aln_in, these are eventually written out\n",
    "    num_written = 0 # reads written\n",
    "    num_frags = 0 # fragments written\n",
    "    buffer_log = [] # history of buffer sizes before and after each flush\n",
    "    to_output = dict() # read names to output from buffer, value is number of copies to output\n",
    "    to_discard = set() # read names to discard from buffer\n",
    "    num_read = 0\n",
    "    for x in aln_in.get_index_statistics():\n",
    "        if x.contig == chrom:\n",
    "            total_reads = x.total\n",
    "    \n",
    "    for read in tqdm(aln_in.fetch(contig=chrom), mininterval=10, total=total_reads, desc=chrom):\n",
    "        num_read += 1\n",
    "        \n",
    "        # periodically flush out reads\n",
    "        if num_read % buffer_size == 0 or num_read == total_reads:\n",
    "            buffer_log.append(len(buffer))\n",
    "#             print(len(buffer))\n",
    "            \n",
    "            new_fragments = defaultdict(lambda: [0, 0, [], []]) # will later overwrite fragments\n",
    "            for x in fragments:\n",
    "                \n",
    "                # if the fragment start is past the reader position, don't consider outputting it yet, as more PCR dups may be found\n",
    "                if num_read != total_reads and x[0] >= frag_start:\n",
    "                    new_fragments[x] = fragments[x]\n",
    "                    \n",
    "                # if there are enough reads in the fragment to meet the requirements, and the fragment is ranomly selected to output (-r)\n",
    "                elif fragments[x][0] >= args.strand_req // 2 and fragments[x][1] >= args.strand_req // 2 and fragments[x][0] + fragments[x][1] >= args.total_req // 2 and random.random() < args.random_frac:\n",
    "                    num_frags += 1\n",
    "                    if args.cap_at_req: # if subsampling PCR duplicates\n",
    "                        # choose read names from each original strand without replacement\n",
    "                        \n",
    "                        chosen_names = random.sample(fragments[x][2], k=args.strand_req // 2)\n",
    "                        chosen_names += random.sample(fragments[x][3], k=args.strand_req // 2)\n",
    "                        if args.total_req > args.strand_req * 2:\n",
    "                            remaining_names = [name for name in fragments[x][2] + fragments[x][3] if name not in chosen_names]\n",
    "                            chosen_names += random.sample(remaining_names, k=(args.total_req - 2 * args.strand_req) // 2)\n",
    "                            \n",
    "                        for name in chosen_names: # add names to to_output\n",
    "                            if name in to_output:\n",
    "                                to_output[name] += 1\n",
    "                            else:\n",
    "                                to_output[name] = 1\n",
    "                        for name in fragments[x][2] + fragments[x][3]: # add unchosen names to to_discard\n",
    "                            if name not in chosen_names:\n",
    "                                to_discard.add(name)\n",
    "                    else: # if not subsampling PCR duplicates, add all reads to to_output\n",
    "                        to_output.update({name:1 for name in fragments[x][2]})\n",
    "                        to_output.update({name:1 for name in fragments[x][3]})\n",
    "                else: # if there aren't enough covering fragments, add reads to to_discard\n",
    "                    to_discard.update(fragments[x][2])\n",
    "                    to_discard.update(fragments[x][3])\n",
    "        \n",
    "            new_buffer = [] # will later overwrite buffer\n",
    "            done = False # when to stop outputting reads\n",
    "            for x in buffer: # for each read in buffer\n",
    "                if done: # do nothing if done outputting\n",
    "                    new_buffer.append(x)\n",
    "                elif x.query_name in to_output: # if read should be output\n",
    "                    num_written += 1\n",
    "                    aln_out.write(x)\n",
    "                    num_copies = to_output[x.query_name] # how many copies of the read to write\n",
    "                    basename = x.query_name\n",
    "                    for i in range(1, num_copies): # write extra copies with \":n\" appended to the read name\n",
    "                        x.query_name = basename + f':{i}'\n",
    "                        num_written += 1\n",
    "                        aln_out.write(x)\n",
    "                    if x.is_reverse: # remove from to_output once we output the reverse read\n",
    "                        del to_output[basename]\n",
    "                elif x.query_name in to_discard: # if read should be discarded\n",
    "                    if x.is_reverse: # remove from to_output once we output the reverse read\n",
    "                        to_discard.remove(x.query_name)\n",
    "                else: # if don't know whether the read should be output yet, stop outputting to preserve sort order\n",
    "                    done = True\n",
    "                    new_buffer.append(x)\n",
    "            \n",
    "            fragments = new_fragments\n",
    "            buffer = new_buffer\n",
    "        \n",
    "            buffer_log.append(len(buffer))\n",
    "\n",
    "        \n",
    "        # thow out read pairs that don't align as expected\n",
    "        if read.is_unmapped or read.mate_is_unmapped or not (read.flag & 2):\n",
    "            continue\n",
    "        \n",
    "        buffer.append(read)\n",
    "        \n",
    "        if read.is_reverse: # ignore reverse reads so we don't double count each fragment\n",
    "            continue\n",
    "        \n",
    "        # find the fragment start and end. Used to find PCR duplicates\n",
    "        frag_start = min(read.reference_start, read.next_reference_start)\n",
    "        tlen = abs(read.template_length)\n",
    "        umi = read.get_tag(args.umi)\n",
    "        \n",
    "        read_strand = '-' if read.is_read2 else '+'\n",
    "        \n",
    "        # add to the fragments read pair count and read name list\n",
    "        if read_strand == '+':\n",
    "            fragments[(frag_start, tlen, umi)][0] += 1\n",
    "            fragments[(frag_start, tlen, umi)][2].append(read.query_name)\n",
    "        else:\n",
    "            fragments[(frag_start, tlen, umi)][1] += 1\n",
    "            fragments[(frag_start, tlen, umi)][3].append(read.query_name)\n",
    "        \n",
    "        # FIXME this loop freezes on some chromosomes (e.g. Chr3 52%, Chr4 64%, Chr3 52%)\n",
    "    aln_in.close()\n",
    "    aln_out.close()\n",
    "    sys.stderr.write(f'completed chromosome {chrom}, wrote {num_written} reads and {num_frags} fragments, \\\n",
    "    max buffer size of {max(buffer_log)} reads at {buffer_log.index(max(buffer_log)) / len(buffer_log) * 100:.1f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a file containing all reads from all high coverage fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln = pysam.AlignmentFile(args.input)\n",
    "chroms = aln.references\n",
    "aln.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('subsampling each chromosome\\n')\n",
    "if __name__ == '__main__':\n",
    "    # start worker processes from pool\n",
    "    pool = multiprocessing.Pool(processes=args.threads)\n",
    "    processes = []\n",
    "    for chrom in chroms:\n",
    "        processes.append(pool.apply_async(subsample_frags, (args.input, f'{args.tmp_prefix}{args.output.replace(\"/\", \".\")}_{chrom}.bam', chrom)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('concatenating subsampled chromosome BAMs\\n')\n",
    "\n",
    "to_cat = ' '.join([f'{args.tmp_prefix}{args.output.replace(\"/\", \".\")}_{chrom}.bam' for chrom in chroms])\n",
    "\n",
    "p = subprocess.run(f'samtools cat -@ {args.threads} -o {args.output} {to_cat}', shell=True, capture_output=True)\n",
    "if p.returncode != 0:\n",
    "    sys.stderr.write(p.stderr)\n",
    "    exit()\n",
    "    \n",
    "p = subprocess.run(f'rm {to_cat}', shell=True, capture_output=True)\n",
    "if p.returncode != 0:\n",
    "    sys.stderr.write(p.stderr)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('completed duplex_subsampler\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
