{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import multiprocessing\n",
    "from os.path import exists\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import gtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Calls variants from a coordinate sorted BAM file without any fancy realignment stuff. Output will treat back-to-back SNVs \n",
    "as multiple 1bp variants. Indels of length >1 bp will not be split up. Outputs a tsv with columns:|n\n",
    "    - chrom: chromosome/contig|n\n",
    "    - pos: 0-based position of the variant. For insertions, this is the site in the reference which the insertion occurs before|n\n",
    "    - ref: reference sequence, \"*\" for insertion|n\n",
    "    - alt: alt sequence, \"*\" for deletion|n\n",
    "    - f_sup: number of forward reads supporting the variant|n\n",
    "    - f_cov: number of forward reads covering the variant position|n\n",
    "    - r_sup: number of reverse reads supporting the variant|n\n",
    "    - r_cov: number of reverse reads covering the variant position|n\n",
    "    - mapping quality: average mapping quality of supporting reads. If -a is set, ascii converted (qual + 33) phred scaled mapping qualities of supporting reads. Length equals alt depth|n\n",
    "    - base quality: average base call qualities of supporting bases. If -a is set, ascii converted (qual + 33) phred scaled base call qualities of supporting bases. Length equals alt\n",
    "        depth * length of variant (e.g. a variant *->TA with two supporting reads where the base call qualities for the two reads\n",
    "        are \",,\" and \"<<\" will have a base quality of \",,<<\")\n",
    "    ''', formatter_class=gtools.EscapeFormatter)\n",
    "parser.add_argument(dest='input', metavar='INPUT_FILE', type=str, \n",
    "                   help='BAM file to call from. Must be coordinate sorted and have an index file')\n",
    "parser.add_argument(dest='output', metavar='OUTPUT_FILE', type=str,\n",
    "                   help='output variant file')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='THREADS_INT', type=int, default=1,\n",
    "                   help='chromosomes are divided between threads. Cannot use more threads than num chromosomes + 1 (default: 1)')\n",
    "parser.add_argument('-t', '--tmp', dest='tmp', metavar='TMP_STRING', type=str, default='./',\n",
    "                   help='prefix to append temporary output files (default: ./)')\n",
    "parser.add_argument('-q', '--min_mapq', dest='min_mapq', metavar='MIN_MAPQ', type=int, default=1,\n",
    "                   help='''minimum read mapping quality required for a read to be processed. Reads with lower mapping quality\n",
    "                    will not count towards any columns (default: 1)''')\n",
    "parser.add_argument('-s', '--min_support', dest='min_support', metavar='MIN_SUPPORT', type=int, default=1,\n",
    "                   help='minimum number of supporting reads (forward and reverse) required to output a variant (default: 1)')\n",
    "parser.add_argument('-p', '--positions', dest='positions', action='store_true', \n",
    "                   help='report the position of the variant within all supporting reads as the last column of output (default: do not report positions)')\n",
    "parser.add_argument('-a', '--all_qualities', dest='all_qualities', action='store_true', \n",
    "                   help='report every quality value in the mq and bq column rather than the average')\n",
    "parser.add_argument('-C', '--chunk_size', dest='chunk_size', metavar='INT', type=int, default=20000,\n",
    "                   help='''No effect on output. Number of reads to process before flushing buffered information. Increasing\n",
    "                   may speed up runtime, especially for high-coverage samples. Decreasing may lower memory requirement (default: 20000)''')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    # args = parser.parse_args('-@ 1 -p ../tmp/test.bam ../tmp/test.tsv'.split()) # used for testing\n",
    "    args = parser.parse_args('-@ 1 -p ../../TEST_OxK_ACR_labelsKi3_chr10%3A100539221-100539618.bam ../../TEST_new_output.tsv'.split()) # used for testing\n",
    "    # args = parser.parse_args('-@ 1 -a tests/test_naive_variant_caller/test_case.bam tests/test_naive_variant_caller/output.tsv'.split()) # used for testing\n",
    "    # args = parser.parse_args('../data/align/big_ku80_1_filtered.bam ../tmp/test_naive_variant_caller.tsv -@ 6 --all_qualities -s 2'.split()) # used for testing\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "if not exists(args.input):\n",
    "    sys.stderr.write('ERROR: input file {} does not exist\\n'.format(args.input))\n",
    "    exit()\n",
    "\n",
    "aln = pysam.AlignmentFile(args.input, 'rb') # load BAM for doing checks\n",
    "\n",
    "# assert index can be opened\n",
    "try:\n",
    "    aln.check_index()\n",
    "except:\n",
    "    sys.stderr.write('ERROR: failed to open index file {}\\n'.format(args.input + '.bai'))\n",
    "    exit()\n",
    "\n",
    "if args.output and '/' in args.output:\n",
    "    os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "\n",
    "if args.tmp and '/' in args.tmp:\n",
    "    os.makedirs(os.path.dirname(args.tmp), exist_ok=True)\n",
    "\n",
    "# set threads to num chromosomes + 1 if more were allocated\n",
    "if args.threads > len(aln.get_index_statistics()) + 1:\n",
    "    args.threads = len(aln.get_index_statistics()) + 1\n",
    "    sys.stderr.write('WARNING: specified more threads than number of chromosomes + 1. Reducing threads to {}\\n'.format(len(aln.get_index_statistics()) + 1))\n",
    "    \n",
    "sys.stderr.write('Running naive_variant_caller.py with arguments:\\n' + '\\n'.join([f'{key}={val}' for key, val in vars(args).items()]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that file is coordinate sorted and contains the MD tag\n",
    "last = ('', 0)\n",
    "for i, read in enumerate(aln.fetch()):\n",
    "    nex = (read.reference_name, read.reference_start)\n",
    "    if nex < last:\n",
    "        sys.stderr.write('ERROR: SAM/BAM file is not coordinate sorted\\n')\n",
    "        exit()\n",
    "    last = nex\n",
    "    \n",
    "    try:\n",
    "        read.get_tag('MD')\n",
    "    except KeyError:\n",
    "        sys.stderr.write('ERROR: SAM/BAM file does not contain the \"MD\" tag\\n')\n",
    "        exit()\n",
    "    \n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- code requires coordinate sorted BAM\n",
    "- code requires the SAM to contain the MD tag to function (see https://samtools.github.io/hts-specs/SAMtags.pdf). Tag follows the regex `[0-9]+(([A-Z]|\\^[A-Z]+)[0-9]+)*`, where a number indicates a match with the reference, a letter indicates the reference base at a mismatch, and a deletion is specified as a carat followed by one or more letters indicating the aligned reference sequence. This is how Bowtie2 produces the MD field. Insertions are not represented in the MD tag.\n",
    "- positions are 0 based in varaibles \"vcf\" and \"variants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pbars = True # whether to draw tqdm progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = '\\t'.join('chrom pos ref alt f_sup f_cov r_sup r_cov mq bq read_pos'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls variants for a single contig. reads must be an iterator over a contig from AlignmentFile.fetch()\n",
    "def call_variants(input_filepath, contig, total_reads, contig_len, output_filepath, process_num, lock):\n",
    "    aln = pysam.AlignmentFile(input_filepath, 'rb')\n",
    "\n",
    "    # key is (pos (0 based), ref, alt)\n",
    "    variants = defaultdict(lambda: {'f_sup': 0, # supporting forward reads\n",
    "                                    'f_cov': 0, # coverage of forward reads\n",
    "                                    'r_sup': 0, # supporting reverse reads\n",
    "                                    'r_cov': 0, # coverage of reverse reads\n",
    "                                    'mq': '', # mapping qualities of supporting reads\n",
    "                                    'bq': '', # base qualities of supporting reads\n",
    "                                    'positions': []}) # position of variant in supporting reads\n",
    "    f_coverage = np.zeros(contig_len, dtype=np.intc) # forward strand coverage\n",
    "    r_coverage = np.zeros(contig_len, dtype=np.intc) # reverse strand coverage\n",
    "    num_read = 0\n",
    "    cur_pos = -1\n",
    "    ready_to_flush = False\n",
    "    output_file = open(output_filepath, 'w') # temporary output file\n",
    "    \n",
    "    # initialize progress bar\n",
    "    if draw_pbars:\n",
    "        with lock:\n",
    "            pbar = tqdm(total=total_reads, desc=contig, position=process_num, leave=True, miniters=100000, maxinterval=9999)\n",
    "    \n",
    "    for read in aln.fetch(contig=contig):\n",
    "        num_read += 1\n",
    "        \n",
    "        # skip pairs that don't pass the mapq filter\n",
    "        if read.mapq < args.min_mapq:\n",
    "            continue\n",
    "\n",
    "        # split md tag into matches (numbers), mismatches (letters), and deletions (carat + letter(s)). non-matches are always separated by a number\n",
    "        mds = re.findall('[0-9]+|[A-Z]|\\^[A-Z]+', read.get_tag('MD'))\n",
    "        \n",
    "        # change reference N mismatches to matches (e.g. ['3', 'N', '3'] -> ['7'])\n",
    "        new_mds = []\n",
    "        was_n = False\n",
    "        for md in mds:\n",
    "            if md[0] == 'N':\n",
    "                new_mds[-1] = str(int(new_mds[-1]) + 1)\n",
    "                was_n = True\n",
    "            elif was_n:\n",
    "                new_mds[-1]  = str(int(new_mds[-1]) + int(md))\n",
    "                was_n = False\n",
    "            else:\n",
    "                new_mds.append(md)\n",
    "        mds = new_mds\n",
    "        \n",
    "        print(mds)\n",
    "        \n",
    "        # cigar strings include insertions, soft, and hard clipping, info not present in the MD tag\n",
    "        # get the length of starting softclips and the number of clipping operations at the start\n",
    "        softclip_start_len = 0\n",
    "        clip_cigs = 0\n",
    "        for cig in read.cigartuples:\n",
    "            if cig[0] == 5:\n",
    "                clip_cigs += 1\n",
    "            elif cig[0] == 4:\n",
    "                softclip_start_len += cig[1]\n",
    "                clip_cigs += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        softclip_end_len = 0\n",
    "        for cig in read.cigartuples[-1:-1:-1]:\n",
    "            if cig[0] == 4:\n",
    "                softclip_end_len += cig[1]\n",
    "            elif cig[0] != 5:\n",
    "                break\n",
    "        \n",
    "        cur_cig = 0 # keep track of the sum of cigar element lengths\n",
    "        # find the insertions in the cigar, add them to mds as \"*x\", where x in the length of insertion\n",
    "        for cig in read.cigartuples[clip_cigs:]: # ignore the clipping cigars\n",
    "            if cig[0] == 1: # if insertion (I)\n",
    "                cur_md = 0 # keep track of sum of md element lengths\n",
    "                for i, md in enumerate(mds): # find md entry which the insertion falls within\n",
    "                    if md.isnumeric(): # if match\n",
    "                        cur_md += int(md)\n",
    "                        if cur_cig <= cur_md: # if the insertion falls within this md element\n",
    "                            first = int(md) - (cur_md - cur_cig)\n",
    "                            second = int(md) - first\n",
    "                            insert = []\n",
    "                            if first != '0':\n",
    "                                insert.append(str(first))\n",
    "                            insert.append('*' + str(cig[1]))\n",
    "                            if second != '0':\n",
    "                                insert.append(str(second))\n",
    "                            mds = mds[:i] + insert + mds[i+1:]\n",
    "                            break\n",
    "                    elif md[0] == '*': # if insertion\n",
    "                        cur_md += int(md[1:])\n",
    "                    elif md[0] == '^': # if deletion\n",
    "                        cur_md += len(md) - 1\n",
    "                    else: # if mismatch\n",
    "                        cur_md += 1   \n",
    "            elif cig[0] != 0 and cig[0] != 2 and cig[0] != 4 and cig[0] != 5: # if cigar operation isn't M, D, I, S, nor H\n",
    "                with lock:\n",
    "                    sys.stderr.write('WARNING: treating unexpected cigar operation \"{}\" as match in read:\\n{}\\n'.format(cig[0], read))\n",
    "            cur_cig += cig[1] # note: clipping shouldn't be added here, but it doesn't matter since all remaining clipping is at the end\n",
    "        \n",
    "        # remove spacer zeros (e.g. ['A', '0', 'T'] -> ['A', 'T'])\n",
    "        mds = [md for md in mds if md != '0']\n",
    "                \n",
    "        # example of how mds should now look: ['A', '3', 'C', 'G', '^TA', '9', '*2', '2']\n",
    "        \n",
    "        has_bqs = read.query_qualities is not None # query_qualities can be none if the BQ field is a sigle \"*\"\n",
    "        \n",
    "        # find and add all variants using md\n",
    "        cur_md = 0\n",
    "        total_insertion_len = 0\n",
    "        for md in mds:\n",
    "            if md.isnumeric(): # skip over matches\n",
    "                cur_md += int(md)\n",
    "            else:\n",
    "                read_pos = cur_md\n",
    "                if md[0] == '^': # if deletion\n",
    "                    key = (read.reference_start + cur_md - total_insertion_len, md[1:], '*')\n",
    "                    qualities = ''# no base call qualities to add\n",
    "                    # don't increment cur_md, as deletions don't count towards the read length\n",
    "                    total_insertion_len -= len(md) - 1\n",
    "                elif md[0] == '*': # if insertion\n",
    "                    key = (read.reference_start + cur_md - total_insertion_len, '*', read.query_sequence[softclip_start_len + cur_md:softclip_start_len + cur_md + int(md[1:])])\n",
    "                    qualities = ''\n",
    "                    if has_bqs: \n",
    "                        for qual in read.query_qualities[cur_md:cur_md+int(md[1:])]:\n",
    "                            qualities += chr(qual + 33)\n",
    "                    cur_md += int(md[1:])\n",
    "                    total_insertion_len += int(md[1:])\n",
    "                else: # if mismatch\n",
    "                    key = (read.reference_start + cur_md - total_insertion_len, md, read.query_sequence[softclip_start_len + cur_md])\n",
    "                    qualities = chr(read.query_qualities[cur_md] + 33) if has_bqs else ''\n",
    "                    cur_md += 1\n",
    "                \n",
    "                if read.is_reverse:\n",
    "                    variants[key]['r_sup'] += 1\n",
    "                    variants[key]['positions'].append(read.query_length - read_pos - 1 + softclip_end_len)\n",
    "                else:\n",
    "                    variants[key]['f_sup'] += 1\n",
    "                    variants[key]['positions'].append(read_pos + softclip_start_len)\n",
    "                variants[key]['mq'] += chr(read.mapping_quality + 33)\n",
    "                variants[key]['bq'] += qualities\n",
    "    \n",
    "        # add to read coverage array\n",
    "        if read.is_reverse:\n",
    "            r_coverage[read.reference_start:read.reference_start + read.reference_length] += 1\n",
    "        else:\n",
    "            f_coverage[read.reference_start:read.reference_start + read.reference_length] += 1\n",
    "        \n",
    "        # periodically flush variants and coverage info out of the dictionaries\n",
    "        if num_read % args.chunk_size == 0:\n",
    "            if draw_pbars:\n",
    "                with lock:\n",
    "                    pbar.update(args.chunk_size)\n",
    "            ready_to_flush = True\n",
    "        \n",
    "        # periodically flush variants out of the dictionary\n",
    "        if (ready_to_flush and read.reference_start > cur_pos) or num_read == total_reads:\n",
    "            # variants = defaultdict(variants.default_factory, {key: val for key, val in sorted(variants.items(), key=lambda ele: ele[0])}) # sort by position\n",
    "            cur_pos = read.reference_start\n",
    "            \n",
    "            to_flush = []\n",
    "            for var in variants:\n",
    "                if var[0] >= cur_pos and num_read < total_reads:\n",
    "                    continue\n",
    "                to_flush.append(var)\n",
    "            for var in to_flush:\n",
    "                if variants[var]['f_sup'] + variants[var]['r_sup'] >= args.min_support:\n",
    "                    variants[var]['f_cov'] = f_coverage[var[0]]\n",
    "                    variants[var]['r_cov'] = r_coverage[var[0]]\n",
    "                    v = variants[var]\n",
    "                    \n",
    "                    if not args.all_qualities:\n",
    "                        v['mq'] = sum([ord(x) - 33 for x in v['mq']]) / len(v['mq'])\n",
    "                        if var[2] == '*' or v['bq'] == '':\n",
    "                            v['bq'] = 42\n",
    "                        else:\n",
    "                            v['bq'] = sum([ord(x) - 33 for x in v['bq']]) / len(v['bq'])\n",
    "                    \n",
    "                    out_line = '\\t'.join(map(str, [contig] + list(var) + [v['f_sup'], v['f_cov'], v['r_sup'], v['r_cov'], v['mq'], v['bq']]))\n",
    "                    out_line += '\\t' + ','.join(map(str, v['positions'])) if args.positions else ''\n",
    "                    output_file.write(out_line + '\\n')\n",
    "                del variants[var]\n",
    "            if draw_pbars:\n",
    "                with lock:\n",
    "                    pbar.update(args.chunk_size)\n",
    "            ready_to_flush = False\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    chroms = [x.contig for x in aln.get_index_statistics()]\n",
    "    chroms.sort()\n",
    "    \n",
    "    # start worker processes from pool\n",
    "    sys.stderr.write('Iterating over chromosomes\\n')\n",
    "    tmp_files = []\n",
    "    tmp_base = args.tmp + 'tmp_' + args.output.split('/')[-1]\n",
    "    with multiprocessing.Pool(processes=max(1, args.threads - 1)) as pool:\n",
    "        lock = multiprocessing.Manager().Lock()\n",
    "        processes = []\n",
    "        stats = sorted(aln.get_index_statistics(), key=lambda x: x.contig) # sort contigs by name, this is needed for duplex_caller_output to simultaneously parse multiple variant tsvs\n",
    "        for i, x in enumerate(stats): # for each chromosome, add a process\n",
    "            tmp_files.append('{}_{}.tsv'.format(tmp_base, x.contig))\n",
    "            pool_args = (args.input, x.contig, x.total, aln.get_reference_length(x.contig), tmp_files[-1], i, lock)\n",
    "            processes.append(pool.apply_async(call_variants, pool_args))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    sys.stderr.flush()\n",
    "    time.sleep(2)\n",
    "    sys.stderr.write('Checking for errors during runtime\\n')\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and delete temporary files\n",
    "sys.stderr.write('Merging temporary output files\\n') \n",
    "\n",
    "with open(args.output, 'w') as out_file:\n",
    "    out_file.write(output_cols + '\\n')\n",
    "    for tmp in tmp_files:\n",
    "        with open(tmp, 'r') as f:\n",
    "            for l in f:\n",
    "                out_file.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Deleting temporary files\\n')\n",
    "for tmp in tmp_files:\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Complete\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
