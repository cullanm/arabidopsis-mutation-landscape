{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import gtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='''\n",
    "Calls variants from a coordinate sorted BAM file without any fancy realignment stuff. Ignores reads that didn't map as a concordant pair. Output will treat back-to-back SNVs \n",
    "as multiple 1bp variants. Indels of length >1 bp will appear as a single row. Takes pcr/optical duplicates and read1-read2 overlap into consideration when determining\n",
    "if a read is in support and doesn't double count duplicates. For each set of reads with the same fragment start and end, they only contribute to support if variants are supported by > x% of the covering \n",
    "reads in the set (includes both forward and reverse reads). If a set of reads passes this filter, it contributes 1 to forward/reverse support (or both if in the \n",
    "read1-read2 overlap). This means read pairs with no duplicates still must pass this filter within the read1-read2 overlap. Outputs a tsv with columns:|n\n",
    "    - chrom: chromosome/contig|n\n",
    "    - pos: 0-based position of the variant. For insertions, this is the site in the reference which the insertion occurs before|n\n",
    "    - ref: reference sequence, \"*\" for insertion|n\n",
    "    - alt: alt sequence, \"*\" for deletion|n\n",
    "    - f_sup: number of forward reads supporting the variant|n\n",
    "    - f_cov: number of forward reads covering the variant position|n\n",
    "    - r_sup: number of reverse reads supporting the variant|n\n",
    "    - r_cov: number of reverse reads covering the variant position|n\n",
    "    - mapping quality: ascii converted (qual + 33) phred scaled mapping qualities of supporting reads. Length equals alt depth. Selects the median value for sets of duplicates|n\n",
    "    - base quality: ascii converted (qual + 33) phred scaled base call qualities of bases in supporting reads. Length equals alt\n",
    "        depth * length of variant (e.g. a variant *->TA with two supporting reads where the base call qualities for the two reads\n",
    "        are \",,\" and \"<<\" will have a base quality of \",,<<\"). Selects the median value for sets of duplicates\n",
    "    ''', formatter_class=gtools.EscapeFormatter)\n",
    "parser.add_argument(dest='input', metavar='FILE', type=str, \n",
    "                   help='BAM file to call from. Must be coordinate sorted and have an index file')\n",
    "parser.add_argument(dest='output', metavar='FILE', type=str,\n",
    "                   help='output variant file')\n",
    "parser.add_argument('-@', '--threads', dest='threads', metavar='INT', type=int, default=1,\n",
    "                   help='chromosomes are divided between threads. Cannot use more threads than num chromosomes + 1 (default: 1)')\n",
    "parser.add_argument('-t', '--tmp', dest='tmp', metavar='PREFIX', type=str, default='./',\n",
    "                   help='prefix to append temporary output files (default: ./)')\n",
    "parser.add_argument('-q', '--min_mq', dest='min_mq', metavar='INT', type=int, default=1,\n",
    "                   help='''minimum read mapping quality required for a read to be processed. Reads with lower mapping quality\n",
    "                    will not count towards any columns (default: 1)''')\n",
    "parser.add_argument('-d', '--duplicate_ratio', dest='duplicate_ratio', metavar='FLOAT', type=float, default=0.79,\n",
    "                   help='''Cutoff for whether a group of duplicates supports a variant. If the number of supporting reads from\n",
    "                   all duplicates / the number of covering reads from all duplicates (both forward and reverse) is greater than \n",
    "                   DUPLICATE_RATIO, the group of duplicates will add 1 to the variant's support (1 to forward and reverse \n",
    "                   support if in the read1-read2 overlap) (default: 0.79)''')\n",
    "parser.add_argument('-p', '--duplicate_support', dest='duplicate_sup', metavar='INT', type=int, default=1,\n",
    "                   help='''Minimum number of duplicates required to contibute to variant support and coverage (default: 1)''')\n",
    "parser.add_argument('-m', '--min_support', dest='min_support', metavar='INT', type=int, default=1,\n",
    "                   help='only output variants with f_sup + r_sup > MIN_SUPPORT (default: 1)')\n",
    "parser.add_argument('-s', '--min_strand_support', dest='min_strand_support', metavar='INT', type=int, default=0,\n",
    "                   help='only output variants with f_sup > MIN_STRAND_SUPPORT and r_sup > MIN_STRAND_SUPPORT (default: 0)')\n",
    "parser.add_argument('-u', '--umi', dest='umi', metavar='TAG', type=str, default=None,\n",
    "                   help='Consider the UMI in the provided BAM tag when grouping pre-PCR fragments (default: no UMI)')\n",
    "parser.add_argument('-e', '--end_dist', dest='end_dist', action='store_true',\n",
    "                   help='Report the distance from fragment end of each supporting read as an extra column (default: not reported)')\n",
    "parser.add_argument('-C', '--chunk_size', dest='chunk_size', metavar='INT', type=int, default=50000,\n",
    "                   help='''No effect on output. Number of reads to process before flushing buffered information. Increasing\n",
    "                   may speed up runtime, especially for high-coverage samples. Decreasing lowers memory requirement (default: 10000)''')\n",
    "\n",
    "try: # run this if in a jupyter notebook\n",
    "    get_ipython()\n",
    "    if os.getcwd()[:8] != '/scratch': # switch to the scratch directory where all the data files are\n",
    "        os.chdir(f'/scratch/cam02551/{os.getcwd().split(\"/\")[-2]}')\n",
    "    # args = parser.parse_args('tests/test_dup_informed_caller/dup_informed_caller_test_case.bam tests/test_dup_informed_caller/test.out -@ 1 -C 1 -p 4'.split()) # used for testing\n",
    "    args = parser.parse_args('data/align/chandler_KVKCS001D_0_filtered.bam tmp/dup_inf_testing.tsv --umi RX --duplicate_support 2 -@ 4'.split()) # used for testing\n",
    "except: # run this if in a terminal\n",
    "    args = parser.parse_args()\n",
    "\n",
    "aln = pysam.AlignmentFile(args.input, 'rb') # load BAM for doing checks\n",
    "\n",
    "# assert index can be opened\n",
    "try:\n",
    "    aln.check_index()\n",
    "except:\n",
    "    sys.stderr.write('ERROR: failed to open index file {}\\n'.format(args.input + '.bai'))\n",
    "    exit()\n",
    "\n",
    "if args.output and '/' in args.output:\n",
    "    os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "\n",
    "if args.tmp and '/' in args.tmp:\n",
    "    os.makedirs(os.path.dirname(args.tmp), exist_ok=True)\n",
    "\n",
    "# set threads to num chromosomes + 1 if more were allocated\n",
    "if args.threads > len(aln.get_index_statistics()) + 1:\n",
    "    args.threads = len(aln.get_index_statistics()) + 1\n",
    "    sys.stderr.write('WARNING: specified more threads than number of chromosomes + 1. Reducing threads to {}\\n'.format(len(aln.get_index_statistics()) + 1))\n",
    "\n",
    "sys.stderr.write('Running dup_informed_caller.py with arguments:\\n' + '\\n'.join([f'{key}={val}' for key, val in vars(args).items()]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that file is coordinate sorted and contains the MD tag and specified UMI\n",
    "last = ('', 0)\n",
    "# max_read_length = 0\n",
    "for i, read in enumerate(aln.fetch()):\n",
    "    # check if MD tag is present\n",
    "    if not read.is_unmapped and (read.flag & 2): # if properly mapped\n",
    "        try:\n",
    "            read.get_tag('MD')\n",
    "        except KeyError:\n",
    "            sys.stderr.write('ERROR: BAM file does not contain the \"MD\" tag\\n')\n",
    "            exit()\n",
    "    \n",
    "    # check if current read aligned before previous read (out of order)\n",
    "    nex = (read.reference_name, read.reference_start)\n",
    "    if nex < last:\n",
    "        sys.stderr.write('ERROR: BAM file is not coordinate sorted\\n')\n",
    "        exit()\n",
    "    last = nex\n",
    "    \n",
    "    # max_read_length = max(max_read_length, read.query_length)\n",
    "    if i > 1000:\n",
    "        break\n",
    "# sys.stderr.write(f'inferring a read length of {max_read_length}bp\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- code requires coordinate sorted BAM\n",
    "- code requires the BAM to contain the MD tag to function (see https://samtools.github.io/hts-specs/SAMtags.pdf). Tag follows the regex `[0-9]+(([A-Z]|\\^[A-Z]+)[0-9]+)*`, where a number indicates a match with the reference, a letter indicates the reference base at a mismatch, and a deletion is specified as a carat followed by one or more letters indicating the aligned reference sequence. This is how Bowtie2 produces the MD field. Insertions are not represented in the MD tag.\n",
    "- all position variables and output should be zero-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pbars = True # whether to draw tqdm progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be the column names ofthe final output, the temporary outputs of call_variants must match this order\n",
    "output_columns = 'chrom pos ref alt f_sup f_cov r_sup r_cov mq bq'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls variants for a single contig\n",
    "def call_variants(input_filepath, contig, total_reads, contig_len, output_filepath, process_num, lock):\n",
    "    aln = pysam.AlignmentFile(input_filepath, 'rb')\n",
    "\n",
    "    # key is (pos (0 based), ref, alt, frag_start, frag_length, umi)\n",
    "    variants = defaultdict(lambda: {'f_sup': 0, # supporting reads on + strand\n",
    "                                    'r_sup': 0, # supporting reads on - strand\n",
    "                                    'mq': '', # mapping qualities of supporting reads\n",
    "                                    'bq': ''}) # base qualities of supporting reads\n",
    "    frag_coverage = dict() # key is (frag_start, frag_length, strand, umi); value is array of read coverage over the fragment\n",
    "    uncounted_frags = [] # frags in frag_coverage which haven't yet contributed to chrom_coverage\n",
    "    chrom_coverage = np.zeros((3, contig_len), dtype=np.intc) # forward [0], reverse [1], and frag [2] coverage of contig (duplicates don't count)\n",
    "    num_read = 0\n",
    "    cur_pos = -1\n",
    "    ready_to_flush = False\n",
    "    output_file = open(output_filepath, 'w') # temporary output file\n",
    "    \n",
    "    # initialize progress bar\n",
    "    if draw_pbars:\n",
    "        with lock:\n",
    "            pbar = tqdm(total=total_reads, desc=contig, position=process_num, leave=True, mininterval=10, maxinterval=9999)\n",
    "    \n",
    "    for read in aln.fetch(contig=contig):\n",
    "        num_read += 1\n",
    "        \n",
    "        # skip pairs that don't align as expected or don't pass the mq filter. this might cause incorrect coverage values if two mates have different mqs\n",
    "        if read.is_unmapped or read.mate_is_unmapped or not (read.flag & 2) or read.mapping_quality < args.min_mq:\n",
    "            continue\n",
    "            \n",
    "        # determine the fragment start and length\n",
    "        frag_start = min(read.reference_start, read.next_reference_start)\n",
    "        frag_len = abs(read.template_length)\n",
    "        \n",
    "        # split md tag into matches (numbers), mismatches (letters), and deletions (carat + letter(s)). non-matches are always separated by a number\n",
    "        mds = re.findall('[0-9]+|[A-Z]|\\^[A-Z]+', read.get_tag('MD'))\n",
    "        \n",
    "        # change reference N mismatches to matches (e.g. ['3', 'N', '3'] -> ['7'])\n",
    "        new_mds = []\n",
    "        was_n = False\n",
    "        for md in mds:\n",
    "            if md[0] == 'N':\n",
    "                new_mds[-1] = str(int(new_mds[-1]) + 1)\n",
    "                was_n = True\n",
    "            elif was_n:\n",
    "                new_mds[-1]  = str(int(new_mds[-1]) + int(md))\n",
    "                was_n = False\n",
    "            else:\n",
    "                new_mds.append(md)\n",
    "        mds = new_mds\n",
    "        \n",
    "        # change deletions of lenth >1 into multiple 1bp deletions (e.g. ['^AGC'] -> ['^A', '^G', '^C'])\n",
    "        # new_mds = []\n",
    "        # for md in mds:\n",
    "        #     if md[0] == '^':\n",
    "        #         for i in range(1, len(md)):\n",
    "        #             new_mds.append('^' + md[i])\n",
    "        #     else:\n",
    "        #         new_mds.append(md)\n",
    "        # mds = new_mds\n",
    "        \n",
    "        cur_cig = 0 # keep track of the sum of cigar element lengths\n",
    "        # find the insertions in the cigar, add them to mds as \"*x\", where x is the length of the insertion\n",
    "        for cig in read.cigar:\n",
    "            if cig[0] == 1: # if insertion\n",
    "                cur_md = 0 # keep track of sum of md element lengths\n",
    "                for i, md in enumerate(mds): # find md entry which the insertion falls within\n",
    "                    if md.isnumeric(): # if match\n",
    "                        cur_md += int(md)\n",
    "                        if cur_cig <= cur_md: # if the insertion falls within this md element\n",
    "                            first = int(md) - (cur_md - cur_cig)\n",
    "                            second = int(md) - first\n",
    "                            insert = []\n",
    "                            if first != '0':\n",
    "                                insert.append(str(first))\n",
    "                            insert.append('*' + str(cig[1]))\n",
    "                            if second != '0':\n",
    "                                insert.append(str(second))\n",
    "                            mds = mds[:i] + insert + mds[i+1:]\n",
    "                            break\n",
    "                    elif md[0] == '*': # if insertion\n",
    "                        cur_md += int(md[1:])\n",
    "                    elif md[0] == '^': # if deletion\n",
    "                        cur_md += len(md) - 1\n",
    "                    else: # if mismatch\n",
    "                        cur_md += 1\n",
    "                        \n",
    "            elif cig[0] != 0 and cig[0] != 2: # if not aligned nor deletion\n",
    "                with lock:\n",
    "                    sys.stderr.write('WARNING: treating unexpected cigar char \"{}\" as match in read:\\n{}\\n'.format(cig[0], read))\n",
    "            cur_cig += cig[1]\n",
    "        \n",
    "        # remove spacer zeros (e.g. ['A', '0', 'T'] -> ['A', 'T'])\n",
    "        mds = [md for md in mds if md != '0']\n",
    "                \n",
    "        # example of how mds should now look: ['A', '3', 'C', 'G', '^TA', '9', '*2', '2']\n",
    "        \n",
    "        # get the UMI tag\n",
    "        if args.umi is None:\n",
    "            umi = ''\n",
    "        else:\n",
    "            umi = read.get_tag(args.umi) \n",
    "        \n",
    "        # find and add all variants using md\n",
    "        cur_md = 0 # current position in the read\n",
    "        total_insertion_len = 0 # aligned read is\n",
    "        for md in mds:\n",
    "            if md.isnumeric(): # skip over matches\n",
    "                cur_md += int(md)\n",
    "            else:\n",
    "                if md[0] == '^': # if deletion\n",
    "                    key_pos = read.reference_start + cur_md - total_insertion_len\n",
    "                    key_ref = md[1:]\n",
    "                    key_alt = '*'\n",
    "                    qualities = '' # no base call qualities to add\n",
    "                    # don't increment cur_md, as deletions don't count towards the read length\n",
    "                    total_insertion_len -= len(md) - 1\n",
    "                elif md[0] == '*': # if insertion\n",
    "                    key_pos = read.reference_start + cur_md - total_insertion_len\n",
    "                    key_ref = '*'\n",
    "                    key_alt = read.query_sequence[cur_md:cur_md + int(md[1:])]\n",
    "                    qualities = ''\n",
    "                    for qual in read.query_qualities[cur_md:cur_md+int(md[1:])]:\n",
    "                        qualities += chr(qual + 33)\n",
    "                    cur_md += int(md[1:])\n",
    "                    total_insertion_len += int(md[1:])\n",
    "                else: # if mismatch\n",
    "                    key_pos = read.reference_start + cur_md - total_insertion_len\n",
    "                    key_ref = md\n",
    "                    key_alt = read.query_sequence[cur_md]\n",
    "                    qualities = chr(read.query_qualities[cur_md] + 33)\n",
    "                    cur_md += 1\n",
    "                \n",
    "                key = (key_pos, key_ref, key_alt,\n",
    "                       frag_start, frag_len, umi)\n",
    "                    \n",
    "                # add variants to dictionary\n",
    "                if read.is_reverse:\n",
    "                    variants[key]['r_sup'] += 1\n",
    "                else:\n",
    "                    variants[key]['f_sup'] += 1\n",
    "                variants[key]['mq'] += chr(read.mapping_quality + 33)\n",
    "                variants[key]['bq'] += qualities\n",
    "        \n",
    "        # add coverage of fragment to frag_coverage (and to chrom_coverage if a duplicate wasn't counted yet)\n",
    "        frag = (frag_start, frag_len, umi)\n",
    "        \n",
    "        # if this is the first read seen of a fragment, add it to the frag_coverage dict\n",
    "        if frag not in frag_coverage: \n",
    "            frag_coverage[frag] = np.zeros((2, frag[1]), dtype=np.short)\n",
    "            uncounted_frags.append(frag)\n",
    "        \n",
    "        # add the coverage of the read and it's mate to frag_coverage\n",
    "        if not read.is_reverse: # only run for forward reads, since we find these first and can infer reverse read coverage using read.next_reference_start\n",
    "            frag_coverage[frag][0, :read.reference_length] += 1\n",
    "            frag_coverage[frag][1, read.next_reference_start - read.reference_start:] += 1\n",
    "            \n",
    "        # periodically flush variants and coverage info out of the dictionaries\n",
    "        if num_read % args.chunk_size == 0:\n",
    "            if draw_pbars:\n",
    "                with lock:\n",
    "                    pbar.update(args.chunk_size)\n",
    "            ready_to_flush = True\n",
    "\n",
    "        if (ready_to_flush and read.reference_start > cur_pos) or num_read == total_reads:\n",
    "            # variants = defaultdict(variants.default_factory, {key: val for key, val in sorted(variants.items(), key=lambda ele: ele[0])}) # sort by position, not necessary\n",
    "            cur_pos = read.reference_start\n",
    "            \n",
    "            # compute chromosome level coverage up to cur_pos\n",
    "            last_counted_frag = -1\n",
    "            for i, f in enumerate(uncounted_frags):\n",
    "                # if we reach the current read position, skip, as we may not have found all the reads for a fragment yet\n",
    "                if f[0] >= cur_pos: \n",
    "                    break\n",
    "                total_frag_cov = np.sum(frag_coverage[f], axis=0) # summed forward and reverse coverage over the fragment\n",
    "                high_cov_mask = total_frag_cov >= args.duplicate_sup # positions in the fragment with enough coverage to pass the duplicate_sup requirement, only these positions should contribute to coverage\n",
    "                high_cov_forward = np.logical_and(high_cov_mask, frag_coverage[f][0]) # positions with enough coverage and >0 coverage from forward reads\n",
    "                high_cov_reverse = np.logical_and(high_cov_mask, frag_coverage[f][1]) # same but for reverse reads\n",
    "                high_cov_any = np.logical_or(high_cov_forward, high_cov_reverse)\n",
    "\n",
    "                # add positions with sufficient coverage to chrom_coverage\n",
    "                chrom_coverage[0, f[0]:f[0] + f[1]] += high_cov_forward\n",
    "                chrom_coverage[1, f[0]:f[0] + f[1]] += high_cov_reverse\n",
    "                chrom_coverage[2, f[0]:f[0] + f[1]] += high_cov_any\n",
    "                \n",
    "                last_counted_frag = i\n",
    "            uncounted_frags = uncounted_frags[last_counted_frag + 1:] # remove all counted fragments\n",
    "            \n",
    "            # group variants from multiple fragments\n",
    "            to_flush = dict() # key is (pos, ref, alt) value is [f_sup, r_sup]\n",
    "            to_del = set()\n",
    "            for var in variants: # (pos, ref, alt, frag_start, frag_len)\n",
    "                if var[0] >= cur_pos and num_read < total_reads:\n",
    "                    continue\n",
    "                v = variants[var]\n",
    "                f_cov = frag_coverage[(var[3], var[4], var[5])][0, var[0] - var[3]]\n",
    "                r_cov = frag_coverage[(var[3], var[4], var[5])][1, var[0] - var[3]]\n",
    "                if (v['f_sup'] + v['r_sup']) / (f_cov + r_cov) > args.duplicate_ratio and v['f_sup'] + v['r_sup'] >= args.duplicate_sup: # if it passes the duplicate ratio and support filter\n",
    "                    glob_var = (var[0], var[1], var[2])\n",
    "                    if glob_var not in to_flush:\n",
    "                        to_flush[glob_var] = [0, 0, '', '']\n",
    "                    to_flush[glob_var][0] += 1 if f_cov > 0 else 0\n",
    "                    to_flush[glob_var][1] += 1 if r_cov > 0 else 0\n",
    "                    to_flush[glob_var][2] += sorted(v['mq'])[len(v['mq']) // 2] # take median mq\n",
    "                    \n",
    "                    # take median bq(s)\n",
    "                    if var[1] == '*': # if insertion\n",
    "                        for i in range(len(var[2])): # get the median bq of each position (e.g. for a bq of 123456, output median(1,3,5) median(2,4,6))\n",
    "                            to_flush[glob_var][3] += sorted(v['bq'][i::len(var[2])])[len(v['mq']) // 2]\n",
    "                    elif var[2] == '*': # if deletion\n",
    "                        pass\n",
    "                    else: # if SNV\n",
    "                        to_flush[glob_var][3] += sorted(v['bq'])[len(v['bq']) // 2]\n",
    "                to_del.add(var)\n",
    "\n",
    "            for var in to_del:\n",
    "                del variants[var]\n",
    "\n",
    "            # output the variants\n",
    "            for var in sorted(to_flush): # (pos, ref, alt)\n",
    "                f_cov = chrom_coverage[0, var[0]]\n",
    "                r_cov = chrom_coverage[1, var[0]]\n",
    "                sup = to_flush[var]\n",
    "                \n",
    "                # check if enough support\n",
    "                if (sup[0] >= args.min_strand_support) and (sup[1] >= args.min_strand_support) and (sup[0] + sup[1] >= args.min_support):\n",
    "                    output_vals = [contig] + list(var) + [sup[0], f_cov, sup[1], r_cov, sup[2], sup[3]]\n",
    "                    output_file.write('\\t'.join(map(str, output_vals)) + '\\n')\n",
    "            # flush coverage dictionaries\n",
    "            to_flush = []\n",
    "            for frag in frag_coverage:\n",
    "                if frag[0] + frag[1] < cur_pos: # if frag_start + frag_len is before the current read, we won't need it any more\n",
    "                    to_flush.append(frag)\n",
    "                    if frag in uncounted_frags:\n",
    "                        print('ruh roh')\n",
    "            for frag in to_flush:\n",
    "                del frag_coverage[frag]\n",
    "            ready_to_flush = False\n",
    "    \n",
    "    pbar.close()\n",
    "    output_file.close()\n",
    "    tqdm.write(f'finished calling {contig}\\n', file=sys.stderr)\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # start worker processes from pool\n",
    "    sys.stderr.write('Running a subprocess for each chromosome\\n')\n",
    "    tmp_files = []\n",
    "    tmp_base = args.tmp + 'tmp_' + args.output.split('/')[-1]\n",
    "    with multiprocessing.Pool(processes=max(1, args.threads - 1)) as pool:\n",
    "        lock = multiprocessing.Manager().Lock()\n",
    "        processes = []\n",
    "        stats = sorted(aln.get_index_statistics(), key=lambda x: x.contig) # sort contigs by name, this is needed for duplex_caller_output to simultaneously parse multiple variant tsvs\n",
    "        for i, x in enumerate(stats): # for each chromosome, add a process\n",
    "            tmp_files.append('{}_{}.tsv'.format(tmp_base, x.contig))\n",
    "            pool_args = (args.input, x.contig, x.total, aln.get_reference_length(x.contig), tmp_files[-1], i, lock)\n",
    "            processes.append(pool.apply_async(call_variants, pool_args))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    sys.stderr.flush()\n",
    "    time.sleep(2)\n",
    "    sys.stderr.write('Checking for errors during runtime\\n')\n",
    "    # check for errors\n",
    "    for p in processes:\n",
    "        p.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and delete temporary files\n",
    "sys.stderr.write('Merging temporary output files\\n')\n",
    "out_header = '\\t'.join(output_columns)\n",
    "if args.output:\n",
    "    with open(args.output, 'w') as out_file:\n",
    "        out_file.write(out_header + '\\n')\n",
    "        for tmp in tmp_files:\n",
    "            with open(tmp, 'r') as f:\n",
    "                for l in f:\n",
    "                    out_file.write(l)\n",
    "else:\n",
    "    sys.stdout.write(out_header)\n",
    "    for tmp in tmp_files:\n",
    "        with open(tmp, 'r') as f:\n",
    "            for l in f:\n",
    "                sys.stdout.write(l) # FIXME this may give a broken pipe error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Deleting temporary files\\n')\n",
    "for tmp in tmp_files:\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.write('Complete\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
